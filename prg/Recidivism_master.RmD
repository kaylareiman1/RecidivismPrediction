---
title: "Recidivism Analysis"
subtitle: "Course: Data Mining with Dr. Diane Igoche"
author: "Group 12: Moses Hetfield, Sormeh Yazdi, and Kayla Reiman"
date: "May 2020"
output: html_document
---

# **Team Notes**
****Delete this notes section eventually****  

**data questions**
- What is jouvenile other count?  
- Confirming: 1 row per person
-Does violent mean original crime or later?  
- How did ProPublica decide on a logistic regression using low/high as the outcome?  
`glm(formula = score_factor ~ gender_factor + age_factor + race_factor + priors_count + crime_factor + two_year_recid, family = "binomial", data = df)`    
- Why are the datasets of violent crimes and regular crimes separate in this way?
- Is NB a reasonable model to use in this  scenario? If so, can you help us troubleshoot Sormeh's code? If not, we're inclined to cut it. 
- To confirm, are ROC and Lift charts for both choosing model and choosing between model?
- Decision trees: will you release answers for HW 5?
- c_charge_degree: clean into factor with "other"? 

#### **To do list**       
   + Validation: Figure out how we should be splitting our data into training/validation from the beginning so we don't make [this mistake](https://canvas.cmu.edu/courses/14656/files/4676325?module_item_id=4438423). Then document it at the start of section 2.  
   + Improve KR's descriptions of possible models in 2a.1: Model Selection and validation. You can use the book's section called "4.5 A Comparison of Classification Methods" as a resource. Then decide which models to use.
   + Do the decision tree
   + Do the random forrest
   + Moses: write up all the steps of choosing vars when you have a chance so it's part of the deliverable; learn how to do branch stuff and teach the rest of us at 6pm
   + Kayla: spend more time looking at code
   + Sormeh: figure out how one is supposed to choose variables in the logistic regression and send us what you have before doc appt. 



#### **Tasks for later**   
* **Medium level of effort**
   + Model Selection and validation (2a.1): run our first model!
* **High level of effort**
   + Figure out the best version of a given model (ex. logistic regression) using test/training error and figuring out if we can incorporate sensitivity/specifity in choosing the version of the model.
   + Compare models (ex. logistic regression vs. random forrest)
   + Make pretty pictures about how our models performed (lift charts, ROC curves, sensitivity vs. specificity)
   
#### **List of resources**
[Our github repo](https://github.com/kaylareiman1/RecidivismPrediction)  
[ProPublica main article]( https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)  
[ProPublica methodology]( https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm)  
[ProPublica github]( https://github.com/propublica/compas-analysis)  
[Raw version of COMPAS survey]( https://www.documentcloud.org/documents/2702103-Sample-Risk-Assessment-COMPAS-CORE.html)  
[Columbia article on Risk as proxy for race (2010)]( https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1677654)
[Article on Risk, Race, and Recidivism]( https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2687339)  
[Original publication by COMPAS creator]( http://www.northpointeinc.com/files/publications/Criminal-Justice-Behavior-COMPAS.pdf)  
[Assignment instructions on Canvas]( https://canvas.cmu.edu/courses/14656/modules/items/4432299)  
[Project description on Canvas]( https://canvas.cmu.edu/courses/14656/modules/items/4443203)  


****End of team notes**** 

```{r global_options, include=FALSE}
#This code chunk sets up the HTML output to not show code by default.  
#knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE)
```

```{r, warning = FALSE, message = FALSE}
#Suppressing warnings about libraries
#Import  libraries (currently overdoing it)
library(tidyverse)    
library(ggplot2)      # graphics library
library(gridExtra)    # For displaying graphs side-by-side
library(knitr)        # contains knitting control
library(tree)         # For the tree-fitting 'tree' function
library(randomForest) # For random forests
library(rpart)        # For nicer tree fitting
library(partykit)     # For nicer tree plotting
library(boot)         # For cv.glm
library(leaps)        # needed for regsubsets (though maybe not relevant b/c our outcome vars are binary)
library(plotly)
library(rsample)      # data splitting, just trying to see if works (for naive bayes)
library(dplyr)        # data transformation, just trying to see if works (naive bayes)
library(caret)        # naive bayes package
library(h2o)          # naive bayes package
library(MASS)         # For LDA
#Format numbers so that they are not in scientific notation.
options(scipen = 4)
```

# **Introduction**  
#### **Background**    
In 2016, ProPublica published a [story](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) on how a commonly-used pre-trial risk assessment tool called the COMPAS is racially biased. The journalists showed that in spite of a 2009 validation study showing similar accuracy rates for black and white men (67 percent versus 69 percent), the inacccuracies were in opposite directions for the two groups. This racial bias of the tool is reflected in their their published table replicated below:      
  
| Type of error                              |           White | African American | 
|--------------------------------------------|-----------------|------------------|
| Labeled Higher Risk, But Didn’t Re-Offend  |            23.5%|            44.9% |
| Labeled Lower Risk, Yet Did Re-Offend      |            47.7%|            28.0% |

#### **Additional information on the COMPAS**    
The COMPAS is widely used across states [add more here from ProPublica article]. It gives people a risk score ranging from 1 to 10, where risk scores of 1 to 4 are  “Low”, 5 to 7 are labeled “Medium”, and 8 to 10 are labeled “High.” Although race is not included in its [137 questions]( https://www.documentcloud.org/documents/2702103-Sample-Risk-Assessment-COMPAS-CORE.html), some of the  questions such as how often people moved can be linked to poverty...[add more here from]  

#### **Prompt**    
* Using the available data, construct a Risk Assessment Instrument (RAI) for predicting two-year recidivism.   
   - Evaluate the predictive performance of your model. 
   - What are the most important predictors of recidivism?
   
* Create an RAI for predicting violent recidivism.
   - Evaluate the predictive performance of your model. 
   - What are the most important predictors of violent recidivism? 
   - How do they compare to the important predictors of general recidivism?
  
* Assess whether the RAIs from (A) and (B) are equally predictive across race/ethnicity groups? How about across age and gender_factor groups?

* Compare your RAIs to the COMPAS RAI. 
   - Do your RAIs perform better or worse than COMPAS?   
   - Do your RAIs produce similar classifications to COMPAS? 
   - Can you identify any systematic differences between your classifications and those of COMPAS? 
   
#### **Goal** 
Our goal is to investigate whether it is possible to create a Risk Assessment Instrument (RAI) that is more accurate and less racist than the COMPAS.

#### **Data**
This analysis is run using ProPublica's data. In order to have clean data, it is necessary to remove certain observations. Fortunately, ProPublica staff published their [Jupyter notebook](https://github.com/propublica/compas-analysis/blob/master/Compas%20Analysis.ipynb) with data cleaning steps in R, so our data is cleaned in the same way (ex. dropping people whose charge date was not w/in 30 days ). However, although we drop the same observations (rows) as they do, we have adapted their code so that it does not drop any attributes (columns) because this would be bad practice for data mining.  

```{r, message = FALSE}
###################################
# Read in all ProPublica datasets #
###################################
compas.scores.raw <- read.csv("https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-raw.csv", header=T)
compas.scores <- read.csv("https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores.csv", header=T)
compas.scores.two.years <- read.csv("https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv", header=T)
compas.scores.two.years.violent <- read.csv("https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years-violent.csv", header=T)
cox.parsed <- read.csv("https://raw.githubusercontent.com/propublica/compas-analysis/master/cox-parsed.csv", header=T)
cox.violent.parsed <- read.csv("https://raw.githubusercontent.com/propublica/compas-analysis/master/cox-violent-parsed.csv", header=T)

####################################
# Adapt ProPublica's Data Cleaning #
####################################
#Note: This code is copied directly from ProPublica. Here is why they dropped data:
      # - dropped if charge date not w/in 30 days  
      # - Coded the recidivist flag -- is_recid -- to be -1 if could not find a compas case at all.  
      # - Ordinary traffic offenses removed  
      # - Filtered the underlying data from Broward county to include:  
      #   + people who had either recidivated in two years  
      #   + had at least two years outside of a correctional facility.  
#Unlike ProPublica, we won't be dropping any attributes because that may bias our model.
### All recidivism ###
#Dropping bad data
recid.all <-compas.scores.two.years %>% 
        filter(days_b_screening_arrest <= 30) %>%
        filter(days_b_screening_arrest >= -30) %>%
        filter(is_recid != -1) %>%
        filter(c_charge_degree != "O") %>%
        filter(score_text != 'N/A')
#Recoding variables
recid.all$length_of_stay <- as.numeric(as.Date(recid.all$c_jail_out) - as.Date(recid.all$c_jail_in))
recid.all <- mutate(recid.all, crime_factor = factor(c_charge_degree)) %>%
      mutate(age_factor = as.factor(age_cat)) %>%
      within(age_factor <- relevel(age_factor, ref = 1)) %>%
      mutate(race_factor = factor(race)) %>%
      within(race_factor <- relevel(race_factor, ref = 3)) %>%
      mutate(gender_factor = factor(sex, labels= c("Female","Male"))) %>%
      within(gender_factor <- relevel(gender_factor, ref = 2)) %>%
      mutate(score_factor = factor(score_text != "Low", labels = c("LowScore","HighScore")))
### Violent recidivism ###
#Dropping bad data
recid.vio <- compas.scores.two.years.violent %>% 
        filter(days_b_screening_arrest <= 30) %>%
        filter(days_b_screening_arrest >= -30) %>% 
        filter(is_recid != -1) %>%
        filter(c_charge_degree != "O") %>%
        filter(v_score_text != 'N/A')
#Recoding variables
#KR note: Why is race coded differently? Is there a mistake?
recid.vio <- mutate(recid.vio, crime_factor = factor(c_charge_degree)) %>%
      mutate(age_factor = as.factor(age_cat)) %>%
      within(age_factor <- relevel(age_factor, ref = 1)) %>%
      mutate(race_factor = factor(race,
                                  labels = c("African-American", 
                                             "Asian",
                                             "Caucasian", 
                                             "Hispanic", 
                                             "Native American",
                                             "Other"))) %>%
      within(race_factor <- relevel(race_factor, ref = 3)) %>%
      mutate(gender_factor = factor(sex, labels= c("Female","Male"))) %>%
      within(gender_factor <- relevel(gender_factor, ref = 2)) %>%
      mutate(score_factor = factor(v_score_text != "Low", labels = c("LowScore","HighScore")))
```

# **Section 1: Exploratory Data Analysis**


#### **1a: Overview of datasets**  
[KR note: must check whether 1 person per row or different row driver]

This project uses 2 datasets from [ProPublica's github repository]( https://github.com/propublica/compas-analysis), which are described in the table below:  
  
| Dataset name | original obs | obs after cleaning | # varibles |
|--------------|--------------|--------------------|------------|   
| recid.all    | 7214         | 6172               | 59         |    
| recid.vio    | 4743         | 4020               | 59         |    

Out of the 59 variables in each dataset, we first looked at the variables in the logistic that ProPublica ran to see how well recidivism could predict whether somebody was flagged as low-risk. Their code is pasted below:  
`glm(formula = score_factor ~ gender_factor + age_factor + race_factor + priors_count + crime_factor + two_year_recid, family = "binomial", data = df)`  

An overview of variables that ProPublica used is below: 
   - two_year_recid: this is the variable that ProPublica used to indicate recidivism in each dataset.   
   - score_factor: the binary variable indicating low risk vs. medium/high risk.
   - Other covariates: 
      -gender_factor: male vs. female
      -age_factor: a 3-level categorical variable for age
      -race_factor: from the original race variable. As will be shown below, most categories have very few observations.  
      -crime_factor: felonies vs. misdimeanors
      -priors_count: prior infractions
      
A list of all of the variables in the dataset is shown below:
```{r, warning = FALSE, message = FALSE, echo = TRUE}
#It'll be ideal if we can delete this section eventually (or make it prettier). However, I'm leaving this here so you can get to know the data better without running everything from scratch.
colnames.all <- colnames(recid.all)
colnames.vio <- colnames(recid.vio)
#Print full list of columns
colnames.all
#Print columns that differ
colnames.vio[(colnames.vio != colnames.all)]
colnames.all[(colnames.vio != colnames.all)]
```


Given the multiple posibilities for recidivism outcome variables, the tables below show the checks that were run to determine that two_year_recid is the correct outcome variable to use in both datasets. These checks also helped us determine that the ProPublica's "violent recidivsm" dataset was a subset of the "all recidivism" dataset. Therefore, the rest of our project uses dataset for all recidivism - nicknamed recid.all.  

```{r, message = FALSE}
#Check how the recidivism variables relate
kable(recid.all %>%
  group_by (two_year_recid, is_recid, is_violent_recid, violent_recid) %>%
  summarize(count = n()), caption = "How variables relate in recid.all dataset")
kable(recid.vio %>%
  group_by (two_year_recid, two_year_recid.1, is_recid, is_violent_recid, violent_recid) %>%
  summarize(count = n()), caption = "How variables relate in recid.vio dataset")
```

#### **1b: Demographic Characteristics**
**Note: These graphs use the full dataset (not limited to violent crime)**
The univariate graphs below show that the most categories in the dataset were men, black people, and those between the age of of 20 and 45.
```{r, message = FALSE}
#These three graphs are identical, except with different input variables.
gender_factor.uni.all <- ggplot(recid.all, aes(x=gender_factor))
gender_factor.uni.all + 
  geom_bar(stat = "count", na.rm = F) + 
  ggtitle("Univariate Gender Breakdown") + 
  guides(fill = FALSE) + 
  xlab("Gender") + 
  ylab("Number of People")
race_factor.uni.all <- ggplot(recid.all, aes(x=race_factor))
race_factor.uni.all + 
  geom_bar(stat = "count", na.rm = F) + 
  ggtitle("Univariate Race Breakdown") + 
  guides(fill = FALSE) + 
  xlab("Race") + 
  ylab("Number of People")
age_factor.uni.all <- ggplot(recid.all, aes(x=age_factor))
age_factor.uni.all + 
  geom_bar(stat = "count", na.rm = F) + 
  ggtitle("Univariate Age Breakdown") + 
  guides(fill = FALSE) + 
  xlab("Age") + 
  ylab("Number of People")
```

#### **1c: Recidivism Rates**  
The table below shows how common two-year recidivism of both types was in our dataset.
```{r, message = FALSE}
rate.all <- recid.all %>%
  summarize(100*round(mean(two_year_recid), 2))
rate.vio <- recid.vio %>%
  summarize(100*round(mean(two_year_recid), 2))
```

| Dataset                                |Two-year Rate |
|----------------------------------------|--------------|
| All recidivism (recid.all)             | `r rate.all`%|
| Violent recidivism (recid.vio)         | `r rate.vio`%|

#### **1d: COMPAS Predictions**
The graphs below show participants' 10-point scores, where a score of 10 indicates the highest recidivism potential. Judges were often shown categories of low, medium, and high risk. The ProPublica analysis combined the categories of medium and high in order to create a binary variable called score_factor with two categories.  

```{r, warning = FALSE, message = FALSE}
#Create histograms of decile scores for all crime, color coded by the category. 
ranking.all <- ggplot(recid.all, aes(x=decile_score, fill = score_text))+
  geom_bar(stat = "count", bin = 1) +
  ggtitle("Prediction Metrics\nAll data") + 
  xlab("Prediction") + 
  guides(fill = FALSE) + 
  ylab("Number of people") 
#Note that v_score_text and v_decile_score are different from decile_score and score_text
ranking.vio <- ggplot(recid.vio, aes(x=v_decile_score, fill = v_score_text))+
  geom_bar(stat = "count", bin = 1) +
  ggtitle("Prediction Metrics\nViolence data") + 
  xlab("Prediction")+ 
  ylab(NULL)
grid.arrange(ranking.all, ranking.vio, ncol = 2)
```

#### **1e: Accuracy of Predictions by Demographic Group**
[KR note to team: I actually expected these wouldn't look alike at all. But by the time I finished this, I'd spent so much time adapting code from our good ol' NLSY project that now I want to keep them.]
Initial data exploration showed that the trends in recidivism were similar to predictions. However, a few limitations should be noted:  
   - The higher rate of recidivism among African-American respondents cannot be separated from bias in policing [cite Michelle Alexander].  
   - This includes no interactions between terms demographic characteristics (ex. race and gender)  
   - As ProPublica pointed out, these charts do not differentiate between false positives and false negatives.   
   
Additionally, we already know from the univariate race graphs that the recidivism rates for groups other than White, Hispanic, and African-American people will be distorted by a small sample size. [KR note: perhaps we should drop these obs or create a new race var?]

```{r, warning = FALSE, message = FALSE}
#Note -- just copying and pasting same charts for different characteristics. Could create a function instead. Also, note that I only ran this for recid.all because running it for recid.vio seemed like too many charts. 
### gender_factor ###
# Bivariate w/ decile score 
gender_factor.bi.conf.all <- recid.all %>%
  group_by(gender_factor) %>%
  summarize(Avgdecile_score = mean(decile_score),
            upper = t.test(decile_score)$conf.int[1],
            lower = t.test(decile_score)$conf.int[2])
gender_factor.bi.all.dec <- ggplot(data = gender_factor.bi.conf.all, aes(x = gender_factor, y = Avgdecile_score)) +  geom_bar(stat = "identity") +
  xlab("gender_factor") + 
  ylab("Average prediction") +
  ggtitle("Prediction from COMPAS") + 
  guides(fill = FALSE) +
  geom_errorbar(aes(ymax = upper, ymin = lower), width = 0.1, size = 1) +
  theme(text = element_text(size=12))
# Bivariate w/ recid 
gender_factor.bi.conf.all <- recid.all %>%
  group_by(gender_factor) %>%
  summarize(Avgtwo_year_recid = mean(two_year_recid),
            upper = t.test(two_year_recid)$conf.int[1],
            lower = t.test(two_year_recid)$conf.int[2])
gender_factor.bi.all.recid <- ggplot(data = gender_factor.bi.conf.all, aes(x = gender_factor, y = Avgtwo_year_recid)) +
  geom_bar(stat = "identity") +
  xlab("gender_factor") + 
  ylab("Average Rate") +
  ggtitle("Actual 2-year Recidivism") + 
  guides(fill = FALSE) +
  geom_errorbar(aes(ymax = upper, ymin = lower), width = 0.1, size = 1) +
  theme(text = element_text(size=12))
### RACE ###
# Bivariate w/ decile score 
race_factor.bi.conf.all <- recid.all %>%
  group_by(race_factor) %>%
  summarize(Avgdecile_score = mean(decile_score),
            upper = t.test(decile_score)$conf.int[1],
            lower = t.test(decile_score)$conf.int[2])
race_factor.bi.all.dec <- ggplot(data = race_factor.bi.conf.all, aes(x = race_factor, y = Avgdecile_score)) +  geom_bar(stat = "identity") +
  xlab("Race") + 
  ylab("Average prediction") +
  ggtitle("Prediction from COMPAS") + 
  guides(fill = FALSE) +
  geom_errorbar(aes(ymax = upper, ymin = lower), width = 0.1, size = 1)  +
  theme(text = element_text(size=12), axis.text.x = element_text(angle = 60, vjust = 1, hjust = 1))
# Bivariate w/ recid 
race_factor.bi.conf.all <- recid.all %>%
  group_by(race_factor) %>%
  summarize(Avgtwo_year_recid = mean(two_year_recid),
            upper = t.test(two_year_recid)$conf.int[1],
            lower = t.test(two_year_recid)$conf.int[2])
race_factor.bi.all.recid <- ggplot(data = race_factor.bi.conf.all, aes(x = race_factor, y = Avgtwo_year_recid)) +
  geom_bar(stat = "identity") +
  xlab("Race") + 
  ylab("Average Rate") +
  ggtitle("Actual 2-year Recidivism") + 
  guides(fill = FALSE) +
  geom_errorbar(aes(ymax = upper, ymin = lower), width = 0.1, size = 1)  +
  theme(text = element_text(size=12), axis.text.x = element_text(angle = 60, vjust = 1, hjust = 1))
### AGE ###
# Bivariate w/ decile score 
age_factor.bi.conf.all <- recid.all %>%
  group_by(age_factor) %>%
  summarize(Avgdecile_score = mean(decile_score),
            upper = t.test(decile_score)$conf.int[1],
            lower = t.test(decile_score)$conf.int[2])
age_factor.bi.all.dec <- ggplot(data = age_factor.bi.conf.all, aes(x = age_factor, y = Avgdecile_score)) +  geom_bar(stat = "identity") +
  xlab("Age") + 
  ylab("Average prediction") +
  ggtitle("Prediction from COMPAS") + 
  guides(fill = FALSE) +
  geom_errorbar(aes(ymax = upper, ymin = lower), width = 0.1, size = 1) +
  theme(text = element_text(size=12))
# Bivariate w/ recid 
age_factor.bi.conf.all <- recid.all %>%
  group_by(age_factor) %>%
  summarize(Avgtwo_year_recid = mean(two_year_recid),
            upper = t.test(two_year_recid)$conf.int[1],
            lower = t.test(two_year_recid)$conf.int[2])
age_factor.bi.all.recid <- ggplot(data = age_factor.bi.conf.all, aes(x = age_factor, y = Avgtwo_year_recid)) +
  geom_bar(stat = "identity") +
  xlab("age") + 
  ylab("Average Rate") +
  ggtitle("Actual 2-year Recidivism") + 
  guides(fill = FALSE) +
  geom_errorbar(aes(ymax = upper, ymin = lower), width = 0.1, size = 1) +
  theme(text = element_text(size=12))
#Display the predictions versus actual recidivism rate averages
grid.arrange(gender_factor.bi.all.dec, gender_factor.bi.all.recid, ncol = 2)
grid.arrange(race_factor.bi.all.dec, race_factor.bi.all.recid, ncol = 2)
grid.arrange(age_factor.bi.all.dec, age_factor.bi.all.recid, ncol = 2)
```

# **Section 2: Methodology** 
**Splitting Data**
Prior to starting the data mining process, we randomly split the data into training and validation sets. 

Here we create a column `is.cv` where `train.dat` is a subset of the data whose is.cv value is 0 (70% of the recid.all dataset); `cv.dat` is a subset of the data whose is.cv value is 2 (20% of the recid.all dataset); and `test.dat` is a subset of the data whose is.cv value is 1 (10% of the recid.all dataset). Using the `sample` function in r ensures this division is random, and we have set a seed at 1 so that the random division will give the same results when the code is re-run. 
```{r, message = FALSE}
set.seed(12)
recid.all$is.cv <- sample(c(0,1,2), size = nrow(recid.all), replace=TRUE, prob = c(0.7, 0.1, 0.2))
prop.table(table(recid.all$is.cv))
train.dat <- subset(recid.all, subset = is.cv == 0)
cv.dat <- subset(recid.all, subset = is.cv == 2)
test.dat <- subset(recid.all, subset = is.cv == 1)
```

**Splitting Data**

#### **2a: Data Mining Methods and Performance**
**2a.1: Model Selection and validation**   
**Possible models**  
Given our task of predicting binary outcomes, we decided to pursue the following methods:  
   - Logistic regression:  This classification method applies statistical principles to  predicting binary outcomes.     
   - Linear Discriminant Analysis (LDA): This works when the interaction between predictors is the same across classes.    
   - Quadratic Discriminant Analysis (QDA): This works when there are class-based interactions among the predictors.    
   - Random Forest (RF):  This allows for using if/then logic to create decision trees and then combining them.  
   - Classification Tree: Although not as trustworthy as random forest, the potential for public transparency makes this worth trying.  
  
Note: We chose LDA over Naive Bayes because of expected interactions between our predictors.  

PCA first attempted work by Moses:
```{r}
#PCA
#This will require some analysis from someone who understands it??
recid.all %>% dplyr::select(age, priors_count, length_of_stay, juv_fel_count, juv_misd_count, juv_other_count, decile_score, v_decile_score) %>%
  as.data.frame() %>%
  prcomp(scale = TRUE) %>%
  biplot(scale = 0, cex = c(0.1, 1))
```

**Choosing Variables**  
Our dataset has nine variables that could potentially be used to predict recidivism. These include:  
  
Three demographic variables  
   - `age`, also represented in buckets as `age_factor`  
   - `race_factor`(Caucasian, African-American, Asian, Hispanic, Native American, or Other)  
   - `gender_factor` (only Male and Female listed)  
  
Four variables relating to prior history  
   - `priors_count` (number of prior convictions)  
   - `juv_fel_count` (number of felony convictions as a juvenile)  
   - `juv_misd_count` (number of misdemeanor convictions as a juvenile)  
   - `juv_other_count` (number of other infractions as a juvenile)  
   
And two variables relating to the crime itself  
   - `crime_factor` (Felony or Misdemeanor)  
   - `length_of_stay` (time incarcerated)  
  
  
Based on the relationship between some of these variables and two-year recidivism rates, we also looked at modifications of certain variables.  
For instance, a logarithmic scale better captures the relationship between `priors_count` and recidivism than a linear scale:  
  
```{r}
#FOR DISCUSSION: I think everything below needs to use train.dat. Agree?
ggplot(data = train.dat,
       aes(x = priors_count,
           y = two_year_recid)) +
  stat_summary_bin(geom = "line",
                   fun = "mean",
                   binwidth = 2) +
  scale_y_continuous(labels = scales::percent) +
  labs(x = "Prior Convictions",
       y = "Two-Year Recidivism Rate",
       title = "Priors and Recidivism Lack a Linear Relationship")
```
  
  
```{r}  
ggplot(data = mutate(train.dat, priors_count = log2(priors_count + 1)),
       aes(x = priors_count,
           y = two_year_recid)) +
  stat_summary_bin(geom = "line",
                   fun = "mean",
                   binwidth = 0.2) +
  scale_y_continuous(labels = scales::percent) +
  scale_x_continuous(breaks = c(log2(1), log2(2), log2(3), log2(5), log2(9), log2(17), log2(33)),
                 labels = c(0, 1, 2, 4, 8, 16, 32)) +
  labs(x = "Prior Convictions (note log scale)",
       y = "Two-Year Recidivism Rate",
       title = "Log of Priors Varies Linearly with Recidivism Rates")
```
  
  
  Based on this finding, we chose to add `log2(priors_count + 1)` as a variable. We also added variables indicating whether the defendant had any cases on their juvenile record. We then performed an exhaustive search to select variables to keep for our general linear model. In this case, since our outcome variable `two_year_recid` is binary, we will be applying a logistic regression to classify participants.
```{r}
#Choosing variables
recid.subsets <- regsubsets(as.factor(two_year_recid) ~
                              crime_factor +
                              age +
                              age_factor +
                              race_factor +
                              gender_factor +
                              log2(priors_count + 1) +
                              priors_count +
                              juv_fel_count +
                              (juv_fel_count > 0) +
                              juv_misd_count +
                              (juv_misd_count > 0) +
                              juv_other_count +
                              (juv_other_count > 0) +
                              length_of_stay,
                            data = train.dat,
                            nbest = 1,
                            nvmax = 15,
                            method = "exhaustive")
par(mfrow = c(1, 2))
#AIC Chart
plot(x = summary(recid.subsets)$cp, xlab = "Number of Variables", ylab = "Mallow's CP")
mincp <- which.min(summary(recid.subsets)$cp)
points(mincp, summary(recid.subsets)$cp[mincp], col = "green3", cex = 1, pch = 20)
#BIC Chart
plot(x = summary(recid.subsets)$bic, xlab = "Number of Variables", ylab = "BIC")
minbic <- which.min(summary(recid.subsets)$bic)
points(minbic, summary(recid.subsets)$bic[minbic], col = "green3", cex = 1, pch = 20)
```
  
  
Depending on the metric used, the best-performing model uses somewhere between ``r minbic`` (according to BIC) and ``r mincp`` (according to AIC) variables. After examining the models in this range, we went with the following 7 variables (presented with coefficients):
```{r}
kable(coef(recid.subsets, 7), format = "markdown")
```


**Sormeh Attempts Logistic Regression**

First we run a logistic regression using recid.all on `two_year_recid`:
ATTEMPT 2: Logistic Regression and Prediction: 
Create a logistic regression model: 
```{r, message = FALSE}
# 5189.5
glm.logit <- glm(two_year_recid ~
                   crime_factor +
                   age +
                   age_factor +
                   gender_factor +
                   log2(priors_count + 1) +
                   (juv_other_count > 0) +
                   length_of_stay,
                 data=train.dat,
                 family = binomial)
summary(glm.logit)
kable(coef(summary(glm.logit)), digits= 3)
```

Use the model above to make a prediction for two_year_recid (using the cross-validation data)
I tried this two different ways, but I'm not certain which one is the right way. One is from class, the other is from the r documentation page on the web.
I think eventually we would compare the misclassification rate here with the test data if this is the one we decide on. Otherwise we compare it to the misclassification rate of the other models to pick the best model.
```{r, message = FALSE}
glm.logit.predict = predict(glm.logit, cv.dat, type="response")
##what is the actual difference here????
p2 <- predict(glm.logit, cv.dat, interval="prediction")
paste("glm prediction with type response:")
glm.logit.predict[1:10]
paste("glm prediction with type prediction:")
p2[1:10]
confusion.glm = ifelse(glm.logit.predict > 0.5, "1", "0")
confusions.pred = table(confusion.glm, cv.dat$two_year_recid)
confusions.pred
# Misclassification
paste("Misclassification Rate (with cv.dat):")
1- sum(diag(confusions.pred)) / sum(confusions.pred)
qplot(glm.logit.predict, geom="histogram")
## This does not work, sigh:
#p <- ggplot(data = recid.all, aes(two_year_recid, fill = as.factor(glm.logit.predict$y))) +
#  geom_histogram(binwidth = 1)
#ggplotly(p)
```

The histogram shows us estimated probabilities from our logistic regression.

***Linear Discriminant Analysis (LDA)***
Running an LDA using the above variables, we achieve the following results:
  
```{r}
recid.lda <- lda(two_year_recid ~
                   age +
                   age_factor +
                   gender_factor +
                   length_of_stay +
                   (juv_other_count > 0) +
                   log2(priors_count + 1),
                 data = train.dat)
#Results are currently for the training data
recid.lda.pred <- predict(recid.lda,
                          type = "response")
lda.tab = table(predicted = recid.lda.pred$class,
                actual = train.dat$two_year_recid)
lda.tab
lda.acc <- (lda.tab[1,1] + lda.tab[2,2]) / sum(lda.tab)
lda.sens <- lda.tab[2,2] / sum(lda.tab[,2])
lda.spec <- lda.tab[1,1] / sum(lda.tab[,1])
kable(tibble(Accuracy = scales::percent(lda.acc, accuracy = 0.1),
             Sensitivity = scales::percent(lda.sens, accuracy = 0.1),
             Specificity = scales::percent(lda.spec, accuracy = 0.1)),
      format = "markdown")
#It would be great to have an ROC curve here, but for now I'm going to bed :)
```
  
  
***Quinear Discriminant Analysis (QDA)***  
When testing models using QDA, we found that including `length_of_stay` and `(juv_other_count > 0)` actually made the model perform worse. Even after excluding them, QDA performed worse than LDA:
```{r}
recid.qda <- qda(two_year_recid ~
                   crime_factor +
                   age +
                   (age_factor == "Less than 25") +
                   gender_factor +
                   log2(priors_count + 1),
                 data = train.dat)
recid.qda.pred <- predict(recid.qda,
                          type = "response")
qda.tab = table(predicted = recid.qda.pred$class,
                actual = train.dat$two_year_recid)
qda.tab
qda.acc <- (qda.tab[1,1] + qda.tab[2,2]) / sum(qda.tab)
qda.sens <- qda.tab[2,2] / sum(qda.tab[,2])
qda.spec <- qda.tab[1,1] / sum(qda.tab[,1])
kable(tibble(Accuracy = scales::percent(qda.acc, accuracy = 0.1),
             Sensitivity = scales::percent(qda.sens, accuracy = 0.1),
             Specificity = scales::percent(qda.spec, accuracy = 0.1)),
      format = "markdown")
```
***Sormeh Attempts Random Forest because she's interested in varImpPlot()***
These "Partial Dependence" plots seem to be showing us that if we were to look at how our prediction changes, on average, as we vary a single variable, holding all other factors fixed, the likelihood estimated that someone recidivates after two years of comitting a crime are as follows: **THERE IS SOMETHING WRONG WITH THESE PLOTS. THEY SEEM TO BE SHOWING THE EXACT OPPOSITE OF WHAT WE WOULD INTUITIVELY HAVE EXPECTED**
These are approximate plots, they are not capturing any faithful interaction or capturing anything the model is actually doing. They do provide some intuition to what is going on. **OR AT LEAST THEY SHOULD BE**
- If anyone is able to interpret these plots in the way they should be interpreted, or if anyone knows how to fix what I have done, PLEASE GO RIGHT AHEAD!
```{r, message = FALSE}
#set.seed(123)
#recid.rf <- randomForest(two_year_recid ~ race + age + sex + juv_fel_count + juv_misd_count + juv_other_count + priors_count + is_recid +
#                           crime_factor + age_factor + race_factor + gender_factor, data=recid.all, importance = TRUE, ntree = 250)
## is_recid got wildl importance (~400 and then IncNodePUrity was at 1200, not sure what that means)
#recid.all.train$two_year_recid <- as.character(recid.all.train$two_year_recid)
recid.all.train$two_year_recid <- as.factor(recid.all.train$two_year_recid)
#recid.rf <- randomForest(two_year_recid ~ race + age + sex + juv_fel_count + juv_misd_count + juv_other_count + priors_count +
#                           crime_factor + age_factor + race_factor + gender_factor, data=recid.all.train, importance = TRUE)
recid.rf <- randomForest(two_year_recid ~ race + age + juv_fel_count + juv_misd_count + juv_other_count + priors_count +
                           crime_factor + age_factor + race_factor + gender_factor, data=recid.all.train, importance = TRUE)
plot(recid.rf)
varImpPlot(recid.rf)
partialPlot(recid.rf, recid.all.train, priors_count)
partialPlot(recid.rf, recid.all.train, age)
partialPlot(recid.rf, recid.all.train, age_factor)
partialPlot(recid.rf, recid.all.train, race)
partialPlot(recid.rf, recid.all.train, gender_factor)
partialPlot(recid.rf, recid.all.train, race_factor)
partialPlot(recid.rf, recid.all.train, crime_factor)
partialPlot(recid.rf, recid.all.train, juv_other_count)
partialPlot(recid.rf, recid.all.train, juv_misd_count)
partialPlot(recid.rf, recid.all.train, juv_fel_count)
#partialPlot()
imp.recid <- importance(recid.rf)
imp.recid
## For some reason this is not plotting everything out...
#imp.recid <- importance(recid.rf)
#impvar <- rownames(imp.recid)[order(imp.recid[, 1], decreasing=TRUE)]
#op <- par(mfrow=c(2,3))
#for (i in seq_along(impvar)){
#  partialPlot(recid.rf, recid.all.train, impvar[i], xlab=impvar[i],
#              main=paste("Partial Dependence on", impvar[i]),
#              ylim=c(30,70))
#}
#par(op)
#imp <- importance(ozone.rf)
#impvar <- rownames(imp)[order(imp[, 1], decreasing=TRUE)]
#op <- par(mfrow=c(2, 3))
#for (i in seq_along(impvar)) {
#    partialPlot(ozone.rf, airquality, impvar[i], xlab=impvar[i],
#                main=paste("Partial Dependence on", impvar[i]),
#                ylim=c(30, 70))
```
We then decided to pursue [which models should we use?]
```{r, message = FALSE}
#Run these models on training data
#Run cross-validation on these models
#Select the best version of each model based on a low CV
  #Update after meeting: could we actually decide based on high specificity/sensitivity, or is that not legit?
#Output line graphs showing how training and CV error levels work (I think this is a crucial part of the prompt)
#Update after meeting: perhaps first run w/o including race, but then in the discussion section, re-run the final models including race and see if they become less racist?
```
To decide between models, we looked at three measures of error:
   -Accuracy: This is the percent of predictions that were correct. It is a limited measure of the tool's usefulness because it counts false positives and false negatives in the same way, when their human impact is much different.  
    -Specifity: This shows the true negative rate. A higher number shows that we are not mistakenly classifying people as high-risk.  general, we believe this is the most important measure because it keeps innocent people from receiving harsh punishments.  
   -Sensitivity: This shows the true positive rate. A higher number shows that we successfully predicted people would commit crimes. For violent crime, a high sensitivity rate is important.  
  
**Statistics for the best version of each data mining method**    
[fill this in once we've run the models -- this is just a template]  
| Model for any recidivism                   |  Accuracy      | Sensitivity      | Specificity |  
|--------------------------------------------|----------------|------------------|-------------|  
| Logistic regression                        |                |                  |             |  
| Linear Discriminant Analysis (LDA)         |                |                  |             |  
| Quadratic Discriminant Analysis (QDA)      |                |                  |             |  
| Random Forest (RF)                         |                |                  |             |  
|--------------------------------------------|----------------|------------------|-------------|  
| Model for violent recidivism               |  Accuracy      | Sensitivity      | Specificity |  
|--------------------------------------------|----------------|------------------|-------------|  
| Logistic regression                        |                |                  |             |  
| Linear Discriminant Analysis (LDA)         |                |                  |             |  
| Quadratic Discriminant Analysis (QDA)      |                |                  |             |  
| Random Forest (RF)                         |                |                  |             |  
[Let's add some graphs here that show tradoffs between different measures that we care about (like sensitivity vs. specificity. Caulkins would be proud)]
**2c: Demographic analysis of the best version of each model:**  
After choosing the best version of each model, we assessed the potential for prejudice. Our goal was to replicate Pro-Publica's chart for each of these metrics included in the introduction. 
[If possible, maybe create some charts comparing models on accuracy vs. prejudice tradeoffs]
**2c1: Race**  
**2c2: Gender**  
**2c3: Age**  
#### **2b: Final model(s)**
Our final model is....
This has an accuracy rate of [], specifity of []. We chose it because...
```{r, message = FALSE}
#Add pretty figures about our final model(s)
```
# **Section 3: Key Findings and Takeaways**
#### **3a: Comparing our tool to the COMPAS tool** 
**3a.1: Does our RAIs perform better or worse than COMPAS?**      
**3a.2: Do our RAIs produce similar classifications to COMPAS?**   
**3a.3: Are there systematic differences between our classifications and those of COMPAS?**   
#### **3b: Reflections on risk assessment tools**   
- We know that arrest data is biased based on where police patrol, so even a model that's interally sound is still based on biased data.[maybe cite Patrick Ball]
- Pittsburgh has a pre-trial risk assessment tool
- Models can be racist without using race as an input...[maybe add more about our research]
- Black box vs. accessibility  
Perhaps cite some articles here, like the two that ProPublica cited, or something more recent
[Columbia article on Risk as proxy for race (2010)]( https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1677654)
[Article on Risk, Race, and Recidivism](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2687339)
# **References**  
[ProPublica main article](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)  
[ProPublica methodology](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm)  
[ProPublica github](https://github.com/propublica/compas-analysis)   
[Raw version of COMPAS survey](https://www.documentcloud.org/documents/2702103-Sample-Risk-Assessment-COMPAS-CORE.html)
