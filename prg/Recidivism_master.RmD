---
title: "Recidivism Analysis"
subtitle: "Course: Data Mining with Dr. Diane Igoche"
author: "Group 12: Moses Hetfield, Sormeh Yazdi, and Kayla Reiman"
date: "May 2020"
output: 
  html_document:
    theme: paper
    highlight: tango
    toc: true
    toc_depth: 5
---

```{r global_options, include=FALSE}
#This code chunk sets up the HTML output to not show code by default.  
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE)
```


```{r, message = FALSE}
#Suppressing warnings about libraries
#Import  libraries (currently overdoing it)
library(tidyverse)    
library(ggplot2)      # graphics library
library(gridExtra)    # For displaying graphs side-by-side
library(knitr)        # contains knitting control
library(tree)         # For the tree-fitting 'tree' function
library(randomForest) # For random forests
library(rpart)        # For nicer tree fitting
library(partykit)     # For nicer tree plotting
library(boot)         # For cv.glm
library(leaps)        # needed for regsubsets (though maybe not relevant b/c our outcome vars are binary)
library(plotly)
library(rsample)      # data splitting, just trying to see if works (for naive bayes)
library(dplyr)        # data transformation, just trying to see if works (naive bayes)
library(caret)        # naive bayes package
library(MASS)         # For LDA
library(ROCR)
library(pROC)         #roc function
#Format numbers so that they are not in scientific notation.
options(scipen = 4)
```

```{r}
#COLOR SCHEME FOR ROC CURVES:
roccurve.logit.col <- "#4D9DE0"
roc_lda.col <- "#E15554"
roc_qda.col <- "#E1BC29"
roccurve.trpart.col <- "#7768AE"
roccurve.rpart.col <- "#3BB273"
```

# **Introduction**  
#### **Background**    
In 2016, ProPublica published a [story](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) on how a commonly-used pre-trial risk assessment tool called the COMPAS is racially biased. The journalists showed that in spite of a [2009 validation study](http://www.northpointeinc.com/files/publications/Criminal-Justice-Behavior-COMPAS.pdf) showing similar accuracy rates for black and white men (67 percent versus 69 percent), the inaccuracies were in opposite directions for the two groups. This racial bias of the tool is reflected in their  published table replicated below:      
  
| Type of error                              |           White | African American | 
|--------------------------------------------|-----------------|------------------|
| Labeled Higher Risk, But Didn’t Re-Offend  |            23.5%|            44.9% |
| Labeled Lower Risk, Yet Did Re-Offend      |            47.7%|            28.0% |

#### **Additional information on the COMPAS**    
ProPublica authors explained that the COMPAS is widely used across states. It gives people a risk score ranging from 1 to 10, where risk scores of 1 to 4 are  “Low”, 5 to 7 are labeled “Medium”, and 8 to 10 are labeled “High.” Although race is not included in its [137 questions]( https://www.documentcloud.org/documents/2702103-Sample-Risk-Assessment-COMPAS-CORE.html), some of the  questions such as how often people moved arguably function as proxies for poverty.  
   
#### **Goal** 
Our goal is to investigate whether it is possible to create a Risk Assessment Instrument (RAI) that is more accurate and less racist than the COMPAS.

```{r, message = FALSE, cache = TRUE}
###################################
# Read in all ProPublica datasets #
###################################
compas.scores.raw <- read.csv("https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-raw.csv", header=T)
compas.scores <- read.csv("https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores.csv", header=T)
compas.scores.two.years <- read.csv("https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv", header=T)
compas.scores.two.years.violent <- read.csv("https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years-violent.csv", header=T)
cox.parsed <- read.csv("https://raw.githubusercontent.com/propublica/compas-analysis/master/cox-parsed.csv", header=T)
cox.violent.parsed <- read.csv("https://raw.githubusercontent.com/propublica/compas-analysis/master/cox-violent-parsed.csv", header=T)
####################################
# Adapt ProPublica's Data Cleaning #
####################################
#Note: This code is copied directly from ProPublica. Here is why they dropped data:
      # - dropped if charge date not w/in 30 days  
      # - Coded the recidivist flag -- is_recid -- to be -1 if could not find a compas case at all.  
      # - Ordinary traffic offenses removed  
      # - Filtered the underlying data from Broward county to include:  
      #   + people who had either recidivated in two years  
      #   + had at least two years outside of a correctional facility.  
#Unlike ProPublica, we won't be dropping any attributes because that may bias our model.
######################
### All recidivism ###
######################
#Dropping bad data
recid.all <-compas.scores.two.years %>% 
        filter(days_b_screening_arrest <= 30) %>%
        filter(days_b_screening_arrest >= -30) %>%
        filter(is_recid != -1) %>%
        filter(c_charge_degree != "O") %>%
        filter(score_text != 'N/A')
#Recoding variables
recid.all$length_of_stay <- as.numeric(as.Date(recid.all$c_jail_out) - as.Date(recid.all$c_jail_in))
recid.all <- mutate(recid.all, crime_factor = factor(c_charge_degree)) %>%
      mutate(age_factor = as.factor(age_cat)) %>%
      within(age_factor <- relevel(age_factor, ref = 1)) %>%
      mutate(race_factor = factor(race)) %>%
      within(race_factor <- relevel(race_factor, ref = 3)) %>%
      mutate(gender_factor = factor(sex, labels= c("Female","Male"))) %>%
      within(gender_factor <- relevel(gender_factor, ref = 2)) %>%
      mutate(score_factor = factor(score_text != "Low", labels = c("LowScore","HighScore")))
##########################
### Violent recidivism ###
##########################
#Dropping bad data
recid.vio <- compas.scores.two.years.violent %>% 
        filter(days_b_screening_arrest <= 30) %>%
        filter(days_b_screening_arrest >= -30) %>% 
        filter(is_recid != -1) %>%
        filter(c_charge_degree != "O") %>%
        filter(v_score_text != 'N/A')
#Recoding variables
recid.vio <- mutate(recid.vio, crime_factor = factor(c_charge_degree)) %>%
      mutate(age_factor = as.factor(age_cat)) %>%
      within(age_factor <- relevel(age_factor, ref = 1)) %>%
      mutate(race_factor = factor(race,
                                  labels = c("African-American", 
                                             "Asian",
                                             "Caucasian", 
                                             "Hispanic", 
                                             "Native American",
                                             "Other"))) %>%
      within(race_factor <- relevel(race_factor, ref = 3)) %>%
      mutate(gender_factor = factor(sex, labels= c("Female","Male"))) %>%
      within(gender_factor <- relevel(gender_factor, ref = 2)) %>%
      mutate(score_factor = factor(v_score_text != "Low", labels = c("LowScore","HighScore")))
```

# **Section 1: Exploratory Data Analysis**

#### **1a: Overview of ProPublica's Datasets**  
This analysis is run using data from [ProPublica's github repository]( https://github.com/propublica/compas-analysis). ProPublica staff published their [Jupyter notebook](https://github.com/propublica/compas-analysis/blob/master/Compas%20Analysis.ipynb) with data cleaning steps in R, so our data is cleaned in much the same way. However, while we drop the same observations (rows) as they do, we have adapted their code so as not to drop any attributes (columns) because this would be bad data mining practice. Additionally, our analysis of violent recidivism uses a larger dataset than ProPublica's. Their staff chose to drop non-violent recidivators, but we leave in these observations. More information is included in the appendix.  

**Choosing Variables**  
Our dataset has nine variables that could potentially be used to predict recidivism. These include:  
  
Three demographic variables  
   - `age`, also represented in buckets as `age_factor`  
   - `race_factor`(Caucasian, African-American, Asian, Hispanic, Native American, or Other)  
   - `gender_factor` (only Male and Female listed)  
  
Four variables relating to prior history  
   - `priors_count` (number of prior convictions)  
   - `juv_fel_count` (number of felony convictions as a juvenile)  
   - `juv_misd_count` (number of misdemeanor convictions as a juvenile)  
   - `juv_other_count` (number of other infractions as a juvenile)  
   
And two variables relating to the crime itself  
   - `crime_factor` (Felony or Misdemeanor)  
   - `length_of_stay` (time incarcerated)  
  
  
Based on the relationship between some of these variables and two-year recidivism rates (represented by `two_year_recid`), we also looked at modifications of certain variables.  
For instance, a logarithmic scale better captures the relationship between `priors_count` and recidivism than a linear scale:  
  
```{r}
ggplot(data = recid.all,
       aes(x = priors_count,
           y = two_year_recid)) +
  stat_summary_bin(geom = "line",
                   fun = "mean",
                   binwidth = 2) +
  scale_y_continuous(labels = scales::percent) +
  labs(x = "Prior Convictions",
       y = "Two-Year Recidivism Rate",
       title = "Priors and Recidivism Lack a Linear Relationship")
```
  
  
```{r}  
ggplot(data = mutate(recid.all, priors_count = log2(priors_count + 1)),
       aes(x = priors_count,
           y = two_year_recid)) +
  stat_summary_bin(geom = "line",
                   fun = "mean",
                   binwidth = 0.2) +
  scale_y_continuous(labels = scales::percent) +
  scale_x_continuous(breaks = c(log2(1), log2(2), log2(3), log2(5), log2(9), log2(17), log2(33)),
                 labels = c(0, 1, 2, 4, 8, 16, 32)) +
  labs(x = "Prior Convictions (note log scale)",
       y = "Two-Year Recidivism Rate",
       title = "Log of Priors Varies Linearly with Recidivism Rates")
```
  
Based on this finding, we chose to add `log2(priors_count + 1)` as a variable called `priors_log`. We also added variables indicating whether the defendant had any cases on their juvenile record.

```{r}
#Create new vars following Moses's work looking at what should go into the model
recid.new <- mutate(recid.all, 
             any_recid_2yr        = recode_factor(two_year_recid, 
                                                  `0` = "No",                             
                                                  `1` = "Yes"),
             vio_recid_2yr       = as.factor(case_when(two_year_recid != 1 | is_violent_recid != 1 ~ "No",
                                                   TRUE ~"Yes")),
              priors_log        = log2(priors_count + 1),
              juv_fel_bi        =  as.factor(case_when(juv_fel_count   == 0 ~ "No",
                                                      juv_fel_count    >  0 ~ "Yes")),
              juv_misd_bi       =  as.factor(case_when(juv_misd_count  == 0 ~ "No",
                                                      juv_misd_count   >  0 ~ "Yes")),   
              juv_other_bi      =  as.factor(case_when(juv_other_count == 0 ~ "No",
                                                      juv_other_count  >  0 ~ "Yes")),
              v_score_factor = factor(v_score_text != "Low", labels = c("LowScore","HighScore"))
                    
                    )
#add numeric var for violent recidivism to use in regression tree
recid.new$num.recidvio <- (as.numeric(recid.new$vio_recid_2yr) - 1) #different from is_violent_recid
recid.new$num.recidany <- (as.numeric(recid.new$any_recid_2yr) - 1) #same as two_year_recid
recid.new$compas.low <- (as.numeric(recid.new$score_factor)) #different from is_violent_recid
recid.new$v.compas.low <- (as.numeric(recid.new$v_score_factor)) #same as two_year_recid
#Using our professional judgement, we will keep only relevant variables in the dataset. Note that certain demographic variables are duplicates based on ProPublica's steps, and all r_ and vr_ variables would be unknown at the time of arrest.
recid.new <- dplyr::select(recid.new, #dataset 
                          id, #Personal ID
                          any_recid_2yr, vio_recid_2yr, num.recidvio, num.recidany, # Recidivism variables: new -- factor and numeric versions  
                          two_year_recid, is_violent_recid, #Recidivism variables: original -- numeric
                          juv_fel_bi, juv_fel_count, juv_misd_bi, juv_misd_count, juv_other_bi, juv_other_count, #juvenile variables
                          priors_count, priors_log, # other personal history
                          age, age_factor, race_factor, gender_factor, #demographic variables
                          length_of_stay, crime_factor, #crime vars
                          v_decile_score, v_score_text, decile_score, score_text, score_factor, v_score_factor, compas.low, v.compas.low #COMPASS predictions (not to be included in our)
                          
                        )
```

After cleaning, our dataset consisted of `r nrow(recid.new)` observations. 

#### **1b: Demographic Characteristics**
The univariate graphs below show that individuals in the dataset were most commonly male, black, and between the ages of 20 and 45.
```{r, message = FALSE}
#These three graphs are identical, except with different input variables.
##########
# GENDER #
##########
gender_factor.uni.all <- ggplot(recid.new, aes(x=gender_factor, fill = I("darkblue")))
gender_factor.uni.all + 
  geom_bar(stat = "count", na.rm = F) + 
  ggtitle("Univariate Gender Breakdown") + 
  guides(fill = FALSE) + 
  xlab("Gender") + 
  ylab("Number of People")
##########
#  RACE  #
##########
race_factor.uni.all <- ggplot(recid.new, aes(x=race_factor, fill = I("darkblue")))
race_factor.uni.all + 
  geom_bar(stat = "count", na.rm = F) + 
  ggtitle("Univariate Race Breakdown") + 
  guides(fill = FALSE) + 
  xlab("Race") + 
  ylab("Number of People")
##########
#  AGE   #
##########
age_factor.uni.all <- ggplot(recid.new,
                             aes(x=factor(age_factor,
                                          levels = c("Less than 25",
                                                     "25 - 45",
                                                     "Greater than 45")),
                                 fill = I("darkblue")))
age_factor.uni.all + 
  geom_bar(stat = "count", na.rm = F) + 
  ggtitle("Univariate Age Breakdown") + 
  guides(fill = FALSE) + 
  xlab("Age") + 
  ylab("Number of People")
```

#### **1c: Recidivism Rates**  
The table below shows how common two-year recidivism of both types was in our dataset.
```{r, message = FALSE}
rate.all <- recid.new %>%
  summarize(100*round(mean(any_recid_2yr == "Yes"), 2))
rate.vio <- recid.new %>%
  summarize(100*round(mean(vio_recid_2yr == "Yes"), 2))
```

| Type of recidivism                     |Two-year Rate |
|----------------------------------------|--------------|
| All                                    | `r rate.all`%|
| Violent                                | `r rate.vio`%|

#### **1d: COMPAS Distributions**
The graphs below show participants' 10-point scores, where a score of 10 indicates the highest recidivism potential. Judges were often shown categories of low, medium, and high risk. The ProPublica analysis combined the categories of medium and high in order to create a binary variable called `score_factor` with two categories.  

```{r, warning = FALSE, message = FALSE}
#Create histograms of decile scores for all crime, color coded by the category. 
ranking.all <- ggplot(recid.new, aes(x=decile_score, fill = score_text))+
  geom_bar(stat = "count", bin = 1) +
  ggtitle("Prediction Metrics\nAll data") + 
  xlab("Prediction") + 
  guides(fill = FALSE) + 
  ylab("Number of people") 
#Note that v_score_text and v_decile_score are different from decile_score and score_text
ranking.vio <- ggplot(recid.vio, aes(x=v_decile_score, fill = v_score_text))+
  geom_bar(stat = "count", bin = 1) +
  ggtitle("Prediction Metrics\nViolence data") + 
  xlab("Prediction")+ 
  ylab(NULL)
grid.arrange(ranking.all, ranking.vio, ncol = 2)
```

#### **1e: Accuracy of COMPAS Predictions**  

**Overall summary of COMPAS predictions**
The confusion matrix below shows the prediction abilities of the COMPAS, when people were divided into the categories of "low" and "medium/high" for total recidivism. Additional analysis of the COMPAS's predictions will be included in the comparison of our model against theirs. However, this initial analysis shows 2,345 true negatives (correctly identifying people as low risk), 1,733 true positives (correctly identifying people as high risk), and just over 1,000 each of false negatives and false positives. 
```{r, warning = FALSE, message = FALSE}
#Create table
confusions.compas.any = table(predicted = recid.new$score_factor, actual = recid.new$any_recid_2yr)
dimnames(confusions.compas.any)[[1]] = c("No (Predicted)","Yes (Predicted)")
kable(confusions.compas.any, caption = "**Confusion matrix for the COMPAS: any recidivism**")
```

This confusion matrix shows an accuracy rate of ``r scales::percent(sum(diag(confusions.compas.any)) / sum(confusions.compas.any), accuracy = 0.1)``. Although just slightly lower than the published result from the full analysis using decile scores, this confusion matrix reflects ProPublica's methodology of comparing "low" versus "medium/high".  

**Demographic summary of COMPAS predictions**
Data visualization by demographic group (below) showed that the trends in recidivism were similar to predictions. However, a few limitations should be noted:  
   - The higher rate of recidivism among African-American respondents cannot be separated from bias in policing. African-Americans are [arrested at higher rates](https://www.sentencingproject.org/publications/un-report-on-racial-disparities/) for committing the same crimes as whites, so recididivism among white defendants is likely underreported relative to African-Americans. Additional information on this phenomenon is available in Michelle Alexander's book, [The New Jim Crow](https://newjimcrow.com/).  
   - This includes no interactions between terms demographic characteristics (ex. race and gender)  
   - As ProPublica pointed out, these charts do not differentiate between false positives and false negatives.   
   
Additionally, we already know from the univariate race graphs that the recidivism rates for groups other than White, Hispanic, and African-American people will be distorted by a small sample size. 

```{r, message = FALSE}
#Note -- just copying and pasting same charts for different characteristics. Could create a function instead. Also, note that I only ran this for as.numeric(any_recid_2yr) because 2) running it for violent seemed like too many charts.
##########
# GENDER #
##########
# Bivariate w/ decile score 
gender_factor.bi.conf.all <- recid.new %>%
  group_by(gender_factor) %>%
  summarize(Avgdecile_score = mean(decile_score),
            upper = t.test(decile_score)$conf.int[1],
            lower = t.test(decile_score)$conf.int[2])
gender_factor.bi.all.dec <- ggplot(data = gender_factor.bi.conf.all, aes(x = gender_factor, y = Avgdecile_score, fill = I("darkorange"))) +  geom_bar(stat = "identity") +
  xlab("gender_factor") + 
  ylab("Average prediction") +
  ggtitle("Prediction from COMPAS") + 
  guides(fill = FALSE) +
  geom_errorbar(aes(ymax = upper, ymin = lower), width = 0.1, size = 1) +
  theme(text = element_text(size=12))
# Bivariate w/ recid 
gender_factor.bi.conf.all <- recid.new %>%
  group_by(gender_factor) %>%
  summarize(Avgtwo_year_recid = mean(two_year_recid),
            upper = t.test(two_year_recid)$conf.int[1],
            lower = t.test(two_year_recid)$conf.int[2])
gender_factor.bi.all.recid <- ggplot(data = gender_factor.bi.conf.all, aes(x = gender_factor, y = Avgtwo_year_recid, fill = I("darkblue"))) +
  geom_bar(stat = "identity") +
  xlab("gender_factor") + 
  ylab("Average Rate") +
  ggtitle("Actual 2-year Recidivism") + 
  guides(fill = FALSE) +
  geom_errorbar(aes(ymax = upper, ymin = lower), width = 0.1, size = 1) +
  theme(text = element_text(size=12))
##########
# RACE #
##########
# Bivariate w/ decile score 
race_factor.bi.conf.all <- recid.new %>%
  group_by(race_factor) %>%
  summarize(Avgdecile_score = mean(decile_score),
            upper = t.test(decile_score)$conf.int[1],
            lower = t.test(decile_score)$conf.int[2])
race_factor.bi.all.dec <- ggplot(data = race_factor.bi.conf.all, aes(x = race_factor, y = Avgdecile_score, fill = I("darkorange"))) +  
  geom_bar(stat = "identity") +
  xlab("Race") + 
  ylab("Average prediction") +
  ggtitle("Prediction from COMPAS") + 
  guides(fill = FALSE) +
  geom_errorbar(aes(ymax = upper, ymin = lower), width = 0.1, size = 1)  +
  theme(text = element_text(size=12), axis.text.x = element_text(angle = 60, vjust = 1, hjust = 1))
# Bivariate w/ recid 
race_factor.bi.conf.all <- recid.new %>%
  group_by(race_factor) %>%
  summarize(Avgtwo_year_recid = mean(two_year_recid),
            upper = t.test(two_year_recid)$conf.int[1],
            lower = t.test(two_year_recid)$conf.int[2])
race_factor.bi.all.recid <- ggplot(data = race_factor.bi.conf.all, aes(x = race_factor, y = Avgtwo_year_recid, fill = I("darkblue"))) +
  geom_bar(stat = "identity") +
  xlab("Race") + 
  ylab("Average Rate") +
  ggtitle("Actual 2-year Recidivism") + 
  guides(fill = FALSE) +
  geom_errorbar(aes(ymax = upper, ymin = lower), width = 0.1, size = 1)  +
  theme(text = element_text(size=12), axis.text.x = element_text(angle = 60, vjust = 1, hjust = 1))
##########
# AGE  #
##########
# Bivariate w/ decile score 
age_factor.bi.conf.all <- recid.new %>%
  group_by(age_factor) %>%
  summarize(Avgdecile_score = mean(decile_score),
            upper = t.test(decile_score)$conf.int[1],
            lower = t.test(decile_score)$conf.int[2])
age_factor.bi.all.dec <- ggplot(data = age_factor.bi.conf.all,
                                aes(x = factor(age_factor,
                                               levels = c("Less than 25",
                                                          "25 - 45",
                                                          "Greater than 45")),
                                    y = Avgdecile_score,
                                    fill = I("darkorange"))) +
  geom_bar(stat = "identity") +
  xlab("Age") + 
  ylab("Average prediction") +
  ggtitle("Prediction from COMPAS") + 
  guides(fill = FALSE) +
  geom_errorbar(aes(ymax = upper, ymin = lower), width = 0.1, size = 1) +
  theme(text = element_text(size=12))
# Bivariate w/ recid 
age_factor.bi.conf.all <- recid.new %>%
  group_by(age_factor) %>%
  summarize(Avgtwo_year_recid = mean(two_year_recid),
            upper = t.test(two_year_recid)$conf.int[1],
            lower = t.test(two_year_recid)$conf.int[2])
age_factor.bi.all.recid <- ggplot(data = age_factor.bi.conf.all,
                                  aes(x = factor(age_factor,
                                                 levels = c("Less than 25",
                                                            "25 - 45",
                                                            "Greater than 45")),
                                      y = Avgtwo_year_recid,
                                      fill = I("darkblue"))) +
  geom_bar(stat = "identity") +
  xlab("Age") + 
  ylab("Average Rate") +
  ggtitle("Actual 2-year Recidivism") + 
  guides(fill = FALSE) +
  geom_errorbar(aes(ymax = upper, ymin = lower), width = 0.1, size = 1) +
  theme(text = element_text(size=12))
#Display the predictions versus actual recidivism rate averages
grid.arrange(gender_factor.bi.all.dec, gender_factor.bi.all.recid, ncol = 2)
grid.arrange(race_factor.bi.all.dec, race_factor.bi.all.recid, ncol = 2)
grid.arrange(age_factor.bi.all.dec, age_factor.bi.all.recid, ncol = 2)
```

# **Section 2: Methodology and Models**


**Splitting Data**
Prior to starting the data mining process, we randomly split the data into training, validation, and test sets. 

Here we create a column `rand.split` where `train.dat` is a subset of the data whose rand.split value is 0 (70% of the recid.new dataset); `val.dat` is a subset of the data whose rand.split value is 2 (20% of the recid.new dataset); and `test.dat` is a subset of the data whose rand.split value is 1 (10% of the recid.new dataset). Using the `sample` function in r ensures this division is random, and we have set a seed of 12 so that the random division will give the same results when the code is re-run.
```{r, message = FALSE}
#Set seed at 12 b/c we're group 12
set.seed(12)
#Use sample function w/ probabilities
recid.new$rand.split <- sample(c(0,1,2), size = nrow(recid.new), replace=TRUE, prob = c(0.7, 0.1, 0.2))
#Create binary flags that can be useful for following example code from textbook
recid.new <- mutate(recid.new, 
                    is.train = ifelse(rand.split == 0, 1, 0),
                    is.val   = ifelse(rand.split == 2, 1, 0),
                    is.test  = ifelse(rand.split == 1, 1, 0))
#Create new datasets
train.dat <- subset(recid.new, subset = (is.train == 1))
val.dat <- subset(recid.new, subset = (is.val == 1))
test.dat <- subset(recid.new, subset = (is.test == 1))
```

An overview of datasets is below:   

| Dataset         | Purpose                      | Rows                   |  
|-----------------|------------------------------|------------------------| 
| recid.new       | Full clean dataset - used above |  `r nrow(recid.new)`  |    
| train.dat       | Training models              |  `r nrow(train.dat)`    |  
| val.dat         | Validating models            |  `r nrow(val.dat)`      |  
| test.dat        | New data for the final model |  `r nrow(test.dat)`     |  



**PCA: An unsupervised method to learn more about how our variables relate**  

- If we want to see if PCA will work well on this data, we look first to see if there are many correlated variables.
- We use a pairs plot to see correlation: 
```{r}
#Modified to use training data, change if this was wrong -MH
panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- abs(cor(x, y))
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    if(missing(cex.cor)) cex.cor <- 0.4/strwidth(txt)
    text(0.5, 0.5, txt, cex = pmax(1, cex.cor * r))
}
pairs(train.dat[,c("crime_factor", "age_factor", "race_factor", "gender_factor", "priors_count", "juv_fel_count", "juv_misd_count", "juv_other_count", "length_of_stay")], lower.panel = panel.cor)
```
- Based on this correlation chart we see that none of the factors seem to be highly correlated with one another. Pairs with relatively high correlations include: 
- `priors_count` and `length_of_stay` (0.11)
- `length_of_stay` and `crime_factor` (0.11)
- `age_factor` and `juv_other_count` (0.14)
- `priors_count` and `crime_factor` (0.15)
- `priors_count` and `age_factor` (0.18)
- `priors_count` and `juv_fel_count` (0.19)
- `juv_misd_count` and `juv_other_count` (0.26)
- `priors_count` and `juv_misd_count` (0.27)


Despite these low correlations, we still attempt PCA to further examine the relationships between our variables:

- We first normalize the data, standardizing each variable to have mean of zero and standard deviation of 1 to prevent certain variables from dominating.  
- The PCA plot allows us to see if some of these variables are correlated enough to be merged in some way  
```{r}
recid.new.sub <- recid.new %>% dplyr::select(age, priors_log, length_of_stay, juv_fel_count, juv_misd_count, juv_other_count) %>% as.data.frame()
recid.new.sub <- na.omit(recid.new.sub)
recid.new.scaled <- scale(recid.new.sub) # This normalizes the data
recid.pca <- princomp(recid.new.scaled) # Perform PCA
recid.pca$loadings = -recid.pca$loadings
recid.pca$scores = -recid.pca$scores
biplot(recid.pca,
       scale = 0,
       cex = c(0.8, 1),
       col=c("darkorange", "darkblue")) # construct biplot
## Add grid lines
abline(v=0, lty=2, col="grey50")
abline(h=0, lty=2, col="grey50")
```

We started with six-dimensional data and brought it down to a two-dimensional view. Our biplot shows two things: each numbered point represents an individual and the coordinates are the derived z1 and z2 values from running PCA. Each point has a `juv_other_count` value, an `age` value, a `juv_fel_count` value and so on, which determine the z1 and z2 values by which it is plotted. The top axis shows the factor loadings. It shows the magnitude each variable has of component 1 and 2.  
  
While z1 and z2 are abstractions that defy easy interpretation, they allow us to sort related variables into groups. `Priors_log` appears to be closely related to `juv_fel_count` and `length_of_stay`. Including all three of these in a model may not add much value compared with including just one.  
`Age` is quite independent from the other variables, meaning that it has the potential to add value to a model even if the others are all already present. `Priors_log` and `juv_other_count` are orthogonal, making them a promising combination for our models.  


## **2a: Data Mining Methods and Performance**  
**Possible models**  
 
Given our task of predicting binary outcomes, this is fundamentally a classification problem. We decided to pursue the following methods:  
   1) **Logistic regression**:  This classification method assesses the probability that a person belongs to a given category -- in this case the probability an individual recidivates within 2 years. Logistic regressions are commonly used when the dependent variable is binary.   
   2) **Linear Discriminant Analysis (LDA)**: Although LDA generally replaces logistic regression in cases where the outcome has more than two classes, it is also a good idea to compare logistic results to LDA results for binary outcomes. LDA is more stable than logistic regression when the classes are well-separated or the sample size is small. One constraint of the LDA is that the interactions between predictors are assumed to be the same across classes.  
   3) **Quadratic Discriminant Analysis (QDA)**: This is like LDA but more flexible. It works when there are class-based interactions among the predictors, so we will try this out in addition to LDA.    
   4)  **Classification and Regression Trees**: Decision trees are helpful when there are non-linear relationships between variables. Additionally, trees can be interpreted graphically, a major advantage for models requiring public transparency. Trees can approximate human decision-making processes, which makes their interpretation more intuitive to the general public.  However, the disadvantage is that trees are unstable. That is, a few new observations can completely change which variable is being used for the first split. Trees can be poor predictors because they tend to have high variance.  
   5) **Random Forest (RF)**: This model improves the stability of the decision tree by creating many distinct trees, each of which is given a different combination of variables and observations to use. If our data are well-modeled by decision trees, the random forest should give a lower error rate than a single decision tree. However, they lack the easy interpretability of individual decision trees.

Source: [ISLR](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=2ahUKEwjx8tmdyafpAhVxg3IEHWqjDIsQFjAAegQIAhAB&url=https%3A%2F%2Ffaculty.marshall.usc.edu%2Fgareth-james%2FISL%2FISLR%2520Seventh%2520Printing.pdf&usg=AOvVaw3IIbJOiIiKLgG0eFhvQBp9)

### **Running models for any recidivism (including non-violent)**  

#### **2a.1: Logistic regression**  


**Variable Selection**  
Our first step is determining which variables are most predictive. We run this process on the training dataset with the `regsubsets` function using the exhaustive method. Because this is a logistic instead of a linear regression, we use AIC/Mallow's CP and BIC instead of R-squared.  

```{r}
#Model that includes all of our potential variables:
fullmodel <- any_recid_2yr ~ crime_factor +
                            age +
                            age_factor +
                            race_factor +
                            gender_factor +
                            priors_log +
                            priors_count +
                            juv_fel_count +
                            juv_fel_bi +
                            juv_misd_count +
                            juv_misd_bi +
                            juv_other_count +
                            juv_other_bi +
                            length_of_stay
#Choosing variables - use fullmodel as the input
recid.subsets <- regsubsets(fullmodel,
                            data = train.dat,
                            nbest = 1,
                            nvmax = 15,
                            method = "exhaustive")
par(mfrow = c(1, 2))
#AIC Chart
plot(x = summary(recid.subsets)$cp, xlab = "Number of Variables", ylab = "Mallow's CP")
mincp <- which.min(summary(recid.subsets)$cp)
points(mincp, summary(recid.subsets)$cp[mincp], col = "darkorange", cex = 1, pch = 20)
#BIC Chart
plot(x = summary(recid.subsets)$bic, xlab = "Number of Variables", ylab = "BIC")
minbic <- which.min(summary(recid.subsets)$bic)
points(minbic, summary(recid.subsets)$bic[minbic], col = "darkorange", cex = 1, pch = 20)
```
  
Depending on the metric used, the best-performing model uses somewhere between ``r minbic`` (according to BIC) and ``r mincp`` (according to AIC) variables. As the decrease in adjusted AIC is quite small between 8 variables and 11 variables, it makes sense to choose 6-8 variables. After examining the models in this range, we went with the following 7 variables, presented with their coefficients below.   

```{r}
kable(coef(recid.subsets, 7), format = "markdown", digits = 3, col.names = c("Value"))
```

**Model outcome**  
Based on the findings above, we trained a logistic regression model on the training dataset. Results are below. As expected, every p-value is significant except for `age_factorGreater than 45`, which was included because it is part of the same categorical variable as `age_factorLess than 25`.

The logistic regression produces probabilities that an individual will recidivate in the two year time span. This allows us to tune a threshold and observe different model accuracies with different thresholds. The histogram below shows the broad range of outcomes from our prediction model.

```{r, echo = FALSE}
#Copied manually from above
glm.logit <- glm(any_recid_2yr ~
                   crime_factor +
                   age +
                   age_factor + #including the whole var, even though only the < 25 group was flagged above
                   gender_factor +
                   priors_log +
                   juv_other_bi +
                   length_of_stay,
                 data=train.dat,
                 family = binomial)
#summary(glm.logit)
kable(coef(summary(glm.logit)), digits= 4)
train.pred.logit <- predict(glm.logit, type = "response")
#confusion.glm = ifelse(train.pred.logit > 0.5, "1", "0")
#confusions.pred = table(confusion.glm, train.dat$any_recid_2yr)
#confusions.pred
# Misclassification
#paste("Misclassification Rate (with train.dat):")
#1- sum(diag(confusions.pred)) / sum(confusions.pred)
ggplot(data = as.data.frame(train.pred.logit),
       aes(x = train.pred.logit)) +
  geom_histogram(aes(y = (..count..)/sum(..count..)),
                 binwidth = 0.25/8,
                 boundary = 0,
                 fill = "darkblue") +
  labs(x = "Predicted Probability of Recidivism",
       y = "Proportion of Individuals",
       title = "Distribution of Recidivism Predictions (Logistic Regression)")
```

We then evaluated our classifier's accuracy, along with both specificity and sensitivity. An overview of these measures is below:  
   - **Accuracy**: This is the percentage of predictions that were correct. It is a limited measure of the tool's usefulness because it counts false positives and false negatives in the same way, when their human impact is much different. That said, it is the simplest measure of a classifier's performance. The accuracy rate is 1 - the misclassification rate.    
    - **Specificity**: This is the percentage of non-recidivators who are correctly classified by the model. High specificity is important because it reduces the risk of incarcerating people who pose no risk to society. We consider specificity the most important metric for our all-recidivism (as opposed to violent recidivism) model because we believe false positives (which lead to wrongful incarceration) are more costly than false negatives (which lead to wrongful release).  
   - **Sensitivity**: This is the percentage of recidivators who are correctly classified by the model. A highly sensitive model would correctly predict most individuals who will recidivate within two years. We consider sensitivity a more important metric for our violent recidivism model than for the full recidivism model.  

Accuracy, specificity, and sensitivity can be calculated using the training confusion matrix below, though we will explore these measures further with a confusion matrix using $validation data$ in the next section.  
``` {r}
#Predicted values
train.pred.logit <- predict(glm.logit, type = "response")
#Confusion matrix
confusion.glm = ifelse(train.pred.logit > 0.5, "1", "0")
confusions.pred.train = table(confusion.glm, train.dat$any_recid_2yr)
dimnames(confusions.pred.train)[[1]] = c("No (Predicted)","Yes (Predicted)")
kable(confusions.pred.train, caption = "**Logistic regression: confusion matrix using training data**")
```
  
When applied to the training data, this model has an accuracy rate of ``r scales::percent(sum(diag(confusions.pred.train)) / sum(confusions.pred.train), accuracy = 0.1)``.  

**Model Validation**  
Training data tends to underestimate the true error rate when the same model is applied to new data. All models below use our validation dataset.

The following ROC chart shows that our logistic regression model will successfully classify a randomly selected recidivator as a higher risk than a randomly selected non-recidivator 74% of the time. This is a reasonable classifier. As we change the threshold, the relative values of Specificity and Sensitivity change accordingly. While the chart below recommends a threshold of 0.462, we may prefer a higher threshold in order to maximize specificity.  
Please refer to this ROC chart as you progress through the logistic regression section.

```{r, message = FALSE}
glm.logit.predict.val  = predict(glm.logit, val.dat, type="response")
## ROC plot
roccurve.logit <- roc(response = val.dat$any_recid_2yr,
                      predictor = glm.logit.predict.val,
                      percent = TRUE)
#plot.roc(roccurve.logit, color = "red", lwd = 3, main= "Logistic Regression - ROC Chart", print.auc = TRUE, print.thres = TRUE)
plot(roccurve.logit,
     main= "Logistic Regression - ROC Chart",
     col = roccurve.logit.col,
     lwd = 3,
     asp = 0,
     print.auc = TRUE,
     print.thres = TRUE,
     print.thres.pattern = "%.3f (%.1f%%, %.1f%%)")
```

```{r, message = FALSE}
logit.threshold <- 0.462
```

The following model is tested with a **``r scales::percent(logit.threshold)``** threshold:  
 

```{r, message = FALSE}
glm.logit.predict.val = predict(glm.logit, val.dat, type="response")
confusion.glm = ifelse(glm.logit.predict.val > logit.threshold, "1", "0")
confusions.pred.val = table(confusion.glm, val.dat$any_recid_2yr)
dimnames(confusions.pred.val)[[1]] = c("No (Predicted)","Yes (Predicted)")
kable(confusions.pred.val,
      caption = paste("**Logistic regression: confusion matrix using validation data:**",
                      scales::percent(logit.threshold), "threshold"))
#kable(round(1- sum(diag(confusions.pred.val)) / sum(confusions.pred.val), 2), caption = "Misclassification Rate (with val.dat):")
#paste("Accuracy:")
logit.acc <- sum(diag(confusions.pred.val)) / sum(confusions.pred.val)
#NOTE: These are not formatted correctly for the caret::sensitivity() and specificity() functions. Those functions require Yeses to come before Nos. I switched these to calculate manually.
#paste("Sensitivity Rate:")
logit.sens <- confusions.pred.val["Yes (Predicted)", "Yes"] / sum(confusions.pred.val[,"Yes"])
#paste("Specificity Rate:")
logit.spec <- confusions.pred.val["No (Predicted)", "No"] / sum(confusions.pred.val[,"No"])
kable(tibble(Accuracy = scales::percent(logit.acc, accuracy = 0.1),
             Sensitivity = scales::percent(logit.sens, accuracy = 0.1),
             Specificity = scales::percent(logit.spec, accuracy = 0.1)),
      format = "markdown")
#Changing threshold for next iteration
logit.threshold <- 0.6
```
In line with our goal of increasing specificity, we now increase the threshold to **``r scales::percent(logit.threshold)``**, producing the following results:
```{r, message = FALSE}
glm.logit.predict.val = predict(glm.logit, val.dat, type="response")
confusion.glm = ifelse(glm.logit.predict.val > logit.threshold, "1", "0")
confusions.pred.val = table(confusion.glm, val.dat$any_recid_2yr)
dimnames(confusions.pred.val)[[1]] = c("No (Predicted)","Yes (Predicted)")
kable(confusions.pred.val,
      caption = paste("**Logistic regression: confusion matrix using validation data:**",
                      scales::percent(logit.threshold), "threshold"))
#kable(round(1- sum(diag(confusions.pred.val)) / sum(confusions.pred.val), 2), caption = "Misclassification Rate (with val.dat):")
#paste("Accuracy:")
logit.acc <- sum(diag(confusions.pred.val)) / sum(confusions.pred.val)
#NOTE: These are not formatted correctly for the caret::sensitivity() and specificity() functions. Those functions require Yeses to come before Nos. I switched these to calculate manually.
#paste("Sensitivity Rate:")
logit.sens <- confusions.pred.val["Yes (Predicted)", "Yes"] / sum(confusions.pred.val[,"Yes"])
#paste("Specificity Rate:")
logit.spec <- confusions.pred.val["No (Predicted)", "No"] / sum(confusions.pred.val[,"No"])
kable(tibble(Accuracy = scales::percent(logit.acc, accuracy = 0.1),
             Sensitivity = scales::percent(logit.sens, accuracy = 0.1),
             Specificity = scales::percent(logit.spec, accuracy = 0.1)),
      format = "markdown")  
#Changing threshold for next iteration
logit.threshold <- 0.7
```
While the accuracy decreases, the specificity rate `r if_else(logit.threshold > 0.5, "increases", "decreases")`.

The following are the results from a **``r scales::percent(logit.threshold)``** threshold:

```{r, message = FALSE}
glm.logit.predict.val = predict(glm.logit, val.dat, type="response")
confusion.glm = ifelse(glm.logit.predict.val > logit.threshold, "1", "0")
confusions.pred.val = table(predicted = confusion.glm, actual = val.dat$any_recid_2yr)
dimnames(confusions.pred.val)[[1]] = c("No (Predicted)","Yes (Predicted)")
kable(confusions.pred.val,
      caption = paste("**Logistic regression: confusion matrix using validation data:**",
                      scales::percent(logit.threshold), "threshold"))
#kable(round(1- sum(diag(confusions.pred.val)) / sum(confusions.pred.val), 2), caption = "Misclassification Rate (with val.dat):")
#paste("Accuracy:")
logit.acc.3 <- sum(diag(confusions.pred.val)) / sum(confusions.pred.val)
#NOTE: These are not formatted correctly for the caret::sensitivity() and specificity() functions. Those functions require Yeses to come before Nos. I switched these to calculate manually.
#paste("Sensitivity Rate:")
logit.sens.3 <- confusions.pred.val["Yes (Predicted)", "Yes"] / sum(confusions.pred.val[,"Yes"])
#paste("Specificity Rate:")
logit.spec.3 <- confusions.pred.val["No (Predicted)", "No"] / sum(confusions.pred.val[,"No"])
kable(tibble(Accuracy = scales::percent(logit.acc.3, accuracy = 0.1),
             Sensitivity = scales::percent(logit.sens.3, accuracy = 0.1),
             Specificity = scales::percent(logit.spec.3, accuracy = 0.1)),
      format = "markdown")  
```

Sensitivity decreases dramatically to `r scales::percent(logit.sens.3, accuracy = 0.1)`, while Specificity increases slightly to `r scales::percent(logit.spec.3, accuracy = 0.1)`. 

Of these thresholds, we recommend the `60%` cut point because it has only a slight decrease in accuracy relative to the optimal `46.2%` threshold, but substantially higher specificity, meaning that fewer people will be wrongfully incarcerated.  

#### **2a.2 Linear Discriminant Analysis (LDA)**   

**Model Outcome**
Running an LDA on our training data, we achieve the following results:
```{r}
recid.lda <- lda(any_recid_2yr ~
                   crime_factor +
                   age +
                   age_factor + #including the whole var, even though only the < 25 group was flagged above
                   gender_factor +
                   priors_log +
                   juv_other_bi +
                   length_of_stay,
                 data = train.dat)
#Results are currently for the training data
recid.lda.pred.train <- predict(recid.lda,
                          type = "response")
lda.tab = table(predicted = recid.lda.pred.train$class,
                actual = train.dat$any_recid_2yr)
dimnames(lda.tab)[[1]] = c("No (Predicted)","Yes (Predicted)")
kable(lda.tab, caption = "LDA: confusion matrix using training data")
lda.acc <- (lda.tab[1,1] + lda.tab[2,2]) / sum(lda.tab)
lda.sens <- lda.tab[2,2] / sum(lda.tab[,2])
lda.spec <- lda.tab[1,1] / sum(lda.tab[,1])
kable(tibble(Accuracy = scales::percent(lda.acc, accuracy = 0.1),
             Sensitivity = scales::percent(lda.sens, accuracy = 0.1),
             Specificity = scales::percent(lda.spec, accuracy = 0.1)))
```
  
**Model Validation**  
As with logistic regression, we then re-ran the model using our validation data. The results from this analysis are below.  

```{r}
#Validation Time!!
lda.pred.val <- predict(object = recid.lda, newdata = val.dat, type = "response")
lda.cv.tab = table(predicted = lda.pred.val$class,
                   actual = val.dat$any_recid_2yr)
dimnames(lda.cv.tab)[[1]] = c("No (Predicted)","Yes (Predicted)")
kable(lda.cv.tab, caption = "LDA: confusion matrix using validation data")
lda.cv.acc <- (lda.cv.tab[1,1] + lda.cv.tab[2,2]) / sum(lda.cv.tab)
lda.cv.sens <- lda.cv.tab[2,2] / sum(lda.cv.tab[,2])
lda.cv.spec <- lda.cv.tab[1,1] / sum(lda.cv.tab[,1])
kable(tibble(Accuracy = scales::percent(lda.cv.acc, accuracy = 0.1),
             Sensitivity = scales::percent(lda.cv.sens, accuracy = 0.1),
             Specificity = scales::percent(lda.cv.spec, accuracy = 0.1)))
## Drop an ROC like it's hot
roc_lda <- roc(response = val.dat$any_recid_2yr,
               predictor = lda.pred.val$posterior[,"Yes"],
               percent = TRUE)
```

The ROC chart shows that the AUC score is ``r round(auc(roc_lda),1)``%, a value well above 50% meaning that the LDA model is a reasonable measure of separability. Furthermore, it shows us that the optimal threshold for the LDA model is at 51.9%.

```{r}
plot(roc_lda,
     main="LDA - ROC Chart",
     col = roc_lda.col,
     lwd = 3,
     asp = 0,
     print.auc = TRUE,
     print.thres = TRUE,
     print.thres.pattern = "%.3f (%.1f%%, %.1f%%)")
```



#### **2a.3 Quadratic Discriminant Analysis (QDA)**   

**Model Outcomes**  
We then checked to see if QDA could outperform LDA -- which might be the case if covariation between input variables were different across classes. Validation results are below.  
```{r}
recid.qda <- qda(any_recid_2yr ~
                   crime_factor +
                   age +
                   age_factor + #including the whole var, even though only the < 25 group was flagged above
                   gender_factor +
                   priors_log +
                   juv_other_bi +
                   length_of_stay,
                 data = train.dat)

#Training data (calculated but not displayed)
recid.qda.pred.train <- predict(recid.qda,
                          type = "response")
qda.tab = table(predicted = recid.qda.pred.train$class,
                actual = train.dat$two_year_recid)
dimnames(qda.tab)[[1]] = c("No (Predicted)","Yes (Predicted)")
dimnames(qda.tab)[[2]] = c("No", "Yes")
kable(qda.tab, caption = "QDA: confusion matrix using training data")
qda.acc <- (qda.tab[1,1] + qda.tab[2,2]) / sum(qda.tab)
qda.sens <- qda.tab[2,2] / sum(qda.tab[,2])
qda.spec <- qda.tab[1,1] / sum(qda.tab[,1])
kable(tibble(Accuracy = scales::percent(qda.acc, accuracy = 0.1),
             Sensitivity = scales::percent(qda.sens, accuracy = 0.1),
             Specificity = scales::percent(qda.spec, accuracy = 0.1)))
```
  
**Model Validation**  
```{r}
#Validation
qda.cv.pred <- predict(object = recid.qda,
                       newdata = val.dat,
                       type = "response")
qda.cv.tab = table(predicted = qda.cv.pred$class,
                   actual = val.dat$two_year_recid)
dimnames(qda.cv.tab)[[1]] = c("No (Predicted)","Yes (Predicted)")
dimnames(qda.cv.tab)[[2]] = c("No", "Yes")
kable(qda.cv.tab, caption = "QDA: confusion matrix using validation data")
qda.cv.acc <- (qda.cv.tab[1,1] + qda.cv.tab[2,2]) / sum(qda.cv.tab)
qda.cv.sens <- qda.cv.tab[2,2] / sum(qda.cv.tab[,2])
qda.cv.spec <- qda.cv.tab[1,1] / sum(qda.cv.tab[,1])
kable(tibble(Accuracy = scales::percent(qda.cv.acc, accuracy = 0.1),
             Sensitivity = scales::percent(qda.cv.sens, accuracy = 0.1),
             Specificity = scales::percent(qda.cv.spec, accuracy = 0.1)))
```

The QDA model correctly categorizes only 62.6% of individuals based on the default threshold of 50%. Although improvements can be made by using a lower threshold (the ROC curve suggests that 21.8% is optimal), the AUC of the QDA model is lower than that of other models.  

```{r}
## Drop that ROC like it's hot
roc_qda <- roc(response = val.dat$any_recid_2yr,
               predictor = qda.cv.pred$posterior[,"Yes"],
               percent = TRUE)
plot(roc_qda,
     col= roc_qda.col,
     lwd = 3,
     asp = 0,
     main="QDA - ROC Chart",
     print.auc = TRUE,
     print.thres = TRUE,
     print.thres.pattern = "%.3f (%.1f%%, %.1f%%)")
#auc(roc_qda)
```
  

#### **2a.4 Classification And Regression Tree**   

Here, we are trying both classification trees (since this is a classification problem) and regression trees, in which we treat the recidivism outcome variable as a numeric 0/1 response. The use of regression trees helps us create ROC curves and better understand the data.  

**Decision Tree: Variable Selection**  
The recursive prediction methods used in decision trees will automatically select variables for us. Therefore, we will feed it the full model with all possible input variables except for `priors_log`, `juv_fel_bi`, `juv_other_bi`, `juv_misd_bi`, and `age_factor` because they are redundant for decision trees. This also applies to random forests, for which redundant variables can cause serious problems. 

**Model: Classification Tree**  
```{r}
#Run original CLASSIFICATION tree
tree.recid <- tree(formula = update(fullmodel, ~ . - priors_log - juv_fel_bi - juv_other_bi - juv_misd_bi - age_factor),
                   data = train.dat)
#Create summary object
summary.tree.recid <- summary(tree.recid)
```

Using the training dataset (with ``r nrow(train.dat)`` rows), we find an accuracy rate of ``r scales::percent(1 - summary.tree.recid$misclass[1]/summary.tree.recid$misclass[2], accuracy = 0.1)``. The initial tree predicting whether a person will recidivate within two years is shown below.  

```{r}
plot(tree.recid)
text(tree.recid, pretty = 0)
```

**Validation: Classification tree**  
However, this is only the first step in the process. We must next re-run the model using our validation data with (with ``r nrow(val.dat)`` rows).

```{r}
tree.pred=predict(tree.recid, val.dat, type="class")
tree.pred.tab <- table(predicted = tree.pred, actual = val.dat$any_recid_2yr)
dimnames(tree.pred.tab)[[1]] = c("No (Predicted)","Yes (Predicted)")
kable(tree.pred.tab, caption = "Classification Tree: confusion matrix using validation data")
tree.pred.errors = table(tree.pred,val.dat$any_recid_2yr)[2,1] +
  table(tree.pred,val.dat$any_recid_2yr)[1,2]
```

Based on the confusion matrix produced above with validation data, we see ``r tree.pred.errors`` errors out of ``r nrow(val.dat)`` observations, for an accuracy rate of ``r scales::percent(1 - tree.pred.errors/nrow(val.dat), accuracy = 0.1)``. 

Next we see if there are any improvements that can be made by pruning it using cross-validation (with the function `cv.tree`). This process will help us avoid over-fitting.

```{r}
#Create CV version of the model, pruning by misclassification rate.
cv.recid=cv.tree(tree.recid, FUN=prune.misclass)
#plot the error rate as a function of both size and k.
par(mfrow=c(1,2))
plot(cv.recid$size,cv.recid$dev,type="b")
cv.recid.df <- data.frame(cv.recid$size, cv.recid$dev)
#Figure out smallest deviance
cv.recid.min.dev <- min(cv.recid.df$cv.recid.dev)
#Select the best model based on this deviance
cv.recid.best <- min(cv.recid.df$cv.recid.size[cv.recid.df$cv.recid.dev== cv.recid.min.dev])
#Create the pruned version - based on figuring out the best size above. Note that the book did this manually but we are automating so that we can re-use this code for both all and violent recidivism.
prune.recid=prune.misclass(tree.recid,best=cv.recid.best)
plot(prune.recid)
text(prune.recid,pretty=0)
#Again, predict and use the validation data
tree.pred.prune=predict(prune.recid, val.dat, type="class")
## ROC plot -- cannot create this b/c all yes/no variables -- nothing quant.
# roccurve.tree <- roc(response = val.dat$any_recid_2yr, predictor = tree.pred.prune)
# plot.roc(roccurve.logit, print.auc = TRUE, print.thres = TRUE)
```

Since we want to minimize deviations and deviations are the same for trees of size 3 and 4, we pruned the tree to size 3 for simplicity.  

The pruned tree results in the same confusion matrix as the unpruned tree.  

While ``r scales::percent(1 - tree.pred.errors/nrow(val.dat), accuracy = 0.1)`` accuracy is comparable to that of the COMPAS model, and the model is easily comprehensible, it is also extremely simplistic and fails to account for any nuance in particular cases. All defendants under age 23 are assumed to recidivate, regardless of (e.g.) their crime or prior records. All defendants with three or more priors are assumed to recidivate as well (reminiscent of the oft-criticized Three Strikes laws widespread in the United States). A 23-year old with two prior convictions for any crimes, however, is assumed unlikely to recidivate.  

Because of these issues, we should be cautious about using this model. More importantly, we should be cautious of any seemingly more sophisticated model that fails to produce better results than such a basic classification tree.

```{r}
#paste("Accuracy:")
tree.acc <- sum(diag(tree.pred.tab)) / sum(tree.pred.tab)
#paste("Sensitivity Rate:")
tree.sens <- tree.pred.tab["Yes (Predicted)", "Yes"] / sum(tree.pred.tab[,"Yes"])
#paste("Specificity Rate:")
tree.spec <- tree.pred.tab["No (Predicted)", "No"] / sum(tree.pred.tab[,"No"])
kable(tibble(Accuracy = scales::percent(tree.acc, accuracy = 0.1),
             Sensitivity = scales::percent(tree.sens, accuracy = 0.1),
             Specificity = scales::percent(tree.spec, accuracy = 0.1)),
      format = "markdown")
```

**Model: Regression trees**  
The following figures demonstrate modeling with regression trees. Although the data is not continuous, the factor variable is binary and therefore can be approached numerically. A model can fail to meet certain assumptions while still providing a high level of prediction value in classification. So while we do not expect a perfect model from the regression tree, there is still value to examining its results. In addition, the regression tree allows us to look at tradeoffs in sensitivity and specificity via an ROC curve - which was not possible with a classification tree. 

 
Simple regression tree output using the `tree` function is below. 
```{r}
#Run REGRESSION tree
reg.tree.recid <- tree(formula = num.recidany ~
                         crime_factor +
                         age +
                         age_factor +
                         race_factor +
                         gender_factor +
                         #priors_log +
                         priors_count +
                         juv_fel_count +
                         juv_misd_count +
                         juv_other_count +
                         length_of_stay,
                       data = train.dat)
summary.reg.tree.recid <- summary(reg.tree.recid)
plot(reg.tree.recid)
text(reg.tree.recid, pretty = 0)
```

We also tried the regression tree using the `rpart` function, which allowed for a more informative picture of the same results. 
```{r}
#Create model using rpart function and training dataset
tree.recid.rpart.train <- rpart(formula = update(fullmodel, ~ . - priors_log - juv_fel_bi - juv_other_bi - juv_misd_bi - age_factor),
                                data = train.dat)
#Output pretty picture
tree.recid.rpart.train.pretty <- as.party(tree.recid.rpart.train)
plot(tree.recid.rpart.train.pretty, pretty = 1)
```

Now we run `cv.tree` to prune the regression tree. However, we find that the smallest deviance is for the largest tree, so no pruning is needed.
``` {r}
#Prune the regression tree using cv.tree
tree.reg.pred=predict(reg.tree.recid,val.dat)
cv.recid.reg=cv.tree(reg.tree.recid, FUN = prune.tree)
#cv.recid.reg
#Commented out pruning code b/c no pruning needed.
# plot(cv.recid.reg$size,cv.recid.reg$dev,type="b")
# plot(cv.recid.reg$k,cv.recid.reg$dev,type="b")
# cv.recid.reg.df <- data.frame(cv.recid.reg$size, cv.recid.reg$dev)
# #Figure out smallest deviance
# cv.recid.reg.min.dev <- min(cv.recid.reg.df$cv.recid.reg.dev)
# #Select the best model based on this deviance
# cv.recid.reg.best <- min(cv.recid.reg.df$cv.recid.reg.size[cv.recid.reg.df$cv.recid.dev == cv.recid.reg.min.dev])
# #Create the pruned version - based on figuring out the best size above. Note that the book did this manually but we are automating so that we can re-use this code for both all and violent recidivism.
# prune.recid.reg <- prune.misclass(reg.tree.recid,best=cv.recid.reg.best)
# plot(prune.recid.reg)
# text(prune.recid.reg,pretty=0)
# #Again, predict and use the validation data
# prune.recid.reg <- predict(prune.recid, val.dat, type="class")
```

**Validation**  
With our regression tree and validation data, we are now able to look at ROC curves for different cut points.
```{r}
tree.pred.rpart.val   <- predict(tree.recid.rpart.train, val.dat)
#Create ROC curve
roccurve.trpart<- roc(response = val.dat$two_year_recid,
                      predictor = tree.pred.rpart.val[,2],
                      levels = c(0, 1),
                      direction = "<",
                      percent = TRUE)
#plot.roc(roccurve.trpart, print.auc = TRUE, print.thres = TRUE)
plot(roccurve.trpart,
     col= roccurve.trpart.col,
     lwd = 3,
     asp = 0,
     main="Tree - ROC Chart",
     print.auc = TRUE,
     print.thres = "local maximas",
     print.thres.pattern = "%.3f (%.1f%%, %.1f%%)")
tree.vals <- coords(roccurve.trpart, "best", ret=c("threshold", "specificity", "sensitivity", "accuracy"))
#tree.vals$threshold
#tree.vals$specificity
#tree.vals$sensitivity
```

As expected, the AUC for the regression tree is the lowest among our models. The regression tree allows for more nuance than the classification tree, with six threshold options (including all-or-nothing options).  
For instance, a threshold of 0.62 results in a tree that expects individuals to recidivate if and only if:  
 - They have at least 3 priors and are under 32 years old  
 - They have at least 7 priors and are at least 32 years old  
Using those criteria, only 41.6% of recidivators would be correctly identified, but 85.4% of non-recidivators would be correctly identified as low threats.  


#### **2a.5 Random Forest**   


Building on the decision tree work above, we now assess the data using random forests. Although these models are less user-friendly than single trees, they reduce variance and are far more flexible than individual trees. The idea behind random forests is the software randomly selects different predictors for each split, and this process happens multiple times. The trees are decorrelated from each other because the software tries different splitting variables at different times.



```  {r}
recid.rf =randomForest(formula = update(fullmodel, ~ . - priors_log - juv_fel_bi - juv_other_bi - juv_misd_bi - age_factor),
                       data    = train.dat,
                       mtry    = 4 #this shouild be sqrt(p) since we're doing classification. In our case, p = 14.
                       ,importance=TRUE #keeping this from book
                       )
``` 

The confusion matrix for our random forest created using training data is below:
```{r}
kable(recid.rf$confusion, caption = "Random Forest: confusion matrix using training data")
rf.error = (recid.rf$confusion[2,1] + recid.rf$confusion[1,2]) /nrow(train.dat)
```
  
Surprisingly, this matrix shows that the overall accuracy rate for this random forest is ``r scales::percent(1 - rf.error, accuracy = 0.1)``, which is worse than our individual classification tree.

To increase transparency, the plots below show the importance of each variable in the random forest model. Looking at the plot on the right, a small Gini coefficient indicates increased node purity. We see that `age` decreases the Gini coefficient the most, followed by `priors_count`, `length_of_stay`, and `race_factor`.  The least meaningful variables are the juvenile records. We chose to include race in our model because models that exclude race are not necessarily race-neutral, as seen with the COMPAS model. We will assess discrimination later in this report.

```{r}
varImpPlot(recid.rf, main="Variable Importance by Factor")
```


We also plotted an ROC curve for the random forest, as shown below.
```{r, warnings = FALSE}
rf.predict <- as.data.frame(predict(recid.rf, val.dat, type = "prob"))
#Adapting code from other parts of the program
roccurve.rpart<- roc(response = val.dat$two_year_recid,
                     predictor = rf.predict$Yes,
                     percent = TRUE)
plot.roc(roccurve.rpart,
         print.auc = TRUE,
         print.thres = TRUE,
         asp = 0,
         print.thres.pattern = "%.3f\n(%.1f%%, %.1f%%)",
         col = roccurve.rpart.col)
rf.vals <- coords(roccurve.rpart, "best", ret=c("threshold", "specificity", "sensitivity", "accuracy"))
#rf.vals$threshold
#rf.vals$specificity
#rf.vals$sensitivity
```
  
The Random Forest model does seem to be a reasonable classifier, although it performs slightly worse than logistic regression and LDA models as judged by area under the curve.

### **Running models for only violent recidivism**    
We now re-run all models above for violent recidivism. Some different approaches will be necessary, given that violent recidivism is less common than total recidivism (``r rate.all``% as opposed to ``r rate.vio``%).    
  
#### **2a.1V: Logistic Regression:**  

**Variable Selection**  
The charts below helped us choose which variables to use:  
```{r}
#Model that includes all of our potential variables:
fullmodelvio <- vio_recid_2yr ~ crime_factor +
                            age +
                            age_factor +
                            race_factor +
                            gender_factor +
                            priors_log +
                            priors_count +
                            juv_fel_count +
                            juv_fel_bi +
                            juv_misd_count +
                            juv_misd_bi +
                            juv_other_count +
                            juv_other_bi +
                            length_of_stay
#Choosing variables - use fullmodelvio as the input
recid.vio.subsets <- regsubsets(fullmodelvio,
                            data = train.dat,
                            nbest = 1,
                            nvmax = 15,
                            method = "exhaustive")
par(mfrow = c(1, 2))
#AIC Chart
plot(x = summary(recid.vio.subsets)$cp, xlab = "Number of Variables", ylab = "Mallow's CP")
mincp <- which.min(summary(recid.vio.subsets)$cp)
points(mincp, summary(recid.vio.subsets)$cp[mincp], col = "green3", cex = 1, pch = 20)
#BIC Chart
plot(x = summary(recid.vio.subsets)$bic, xlab = "Number of Variables", ylab = "BIC")
minbic <- which.min(summary(recid.vio.subsets)$bic)
points(minbic, summary(recid.vio.subsets)$bic[minbic], col = "green3", cex = 1, pch = 20)
```
  
Depending on the metric used, the best-performing model uses somewhere between ``r minbic`` (according to BIC) and ``r mincp`` (according to CP) variables. We chose to use the 7 variables that minimize Mallow's CP. After examining the models in this range, we went with the following 7 variables, presented with their coefficients below. Unlike the full recidivism dataset, race was included in this model - though only for Hispanic people. We left the full race variable in the dataset for the original logistic regression below. Our choice to leave race in the model is informed by the understanding that we are predicting whether people will be caught recidivating, rather than true recidivism, given racial bias in the policing system.  

```{r}
kable(coef(recid.vio.subsets, 7), format = "markdown", digits = 3, col.names = c("Value"))
```

***Model outcome***  
```{r, echo = FALSE}
#Copied manually from above
glm.viologit <- glm(vio_recid_2yr ~
                   crime_factor +
                   age +
                   race_factor + 
                   gender_factor +
                   priors_log +
                   juv_other_bi +
                   length_of_stay,
                 data=train.dat,
                 family = binomial)
#summary(glm.viologit)
kable(coef(summary(glm.viologit)), digits= 4)
train.pred.vio <- predict(glm.viologit, type = "response")
ggplot(data = as.data.frame(train.pred.vio),
       aes(x = train.pred.vio)) +
  geom_histogram(aes(y = (..count..)/sum(..count..)),
                 binwidth = 0.25/8, boundary = 0) +
  labs(x = "Predicted Probability of Violent Recidivism",
       y = "Proportion of Individuals",
       title = "Distribution of Violent Recidivism Predictions (Logistic Regression)")
```

Almost all predicted probabilities are below 0.5. However, as a default, the model uses a cut point of 0.5. The accuracy rate and confusion matrix are below, using this cut point. Almost nobody was predicted to commit violent crime in our model, and the one person predicted to commit a violent crime never committed it. The 90% accuracy rate thus reflects only the fact that about 90% of our total population was not caught for violent recidivism.  

``` {r}
#Predicted values
train.pred.vio <- predict(glm.viologit, type = "response")
#Confusion matrix
confusion.glm = ifelse(train.pred.vio > 0.5, "1", "0")
confusions.pred.train = table(confusion.glm, train.dat$vio_recid_2yr)
dimnames(confusions.pred.train)[[1]] = c("No (Predicted)","Yes (Predicted)")
kable(confusions.pred.train, caption = "**Logistic regression: confusion matrix using training data**")
kable(scales::percent(sum(diag(confusions.pred.train)) / sum(confusions.pred.train),
                      accuracy = 0.1),
      col.names = "Accuracy (using training data):")
```

However, the ROC curve below using validation data DOES show some value in the model. This means that our default threshold of 0.5 was not optimal. In fact, the ROC curve shows that the optimal threshold is 0.103.

```{r, message = FALSE}
logit.threshold <- 0.103
#Predicted values
glm.viologit.predict.val  = predict(glm.viologit, val.dat, type="response")
## ROC plot
roccurve.vio <- roc(response = val.dat$vio_recid_2yr,
                    predictor = glm.viologit.predict.val,
                    percent = TRUE)
plot.roc(roccurve.vio,
         print.auc = TRUE,
         print.thres = TRUE,
         print.thres.pattern = "%.3f\n(%.1f%%, %.1f%%)",
         col = roccurve.logit.col,
         asp = 0)
```

  
The final relevant rates for our logistic model are shown below: 
``` {r}
confusion.vioglm = ifelse(glm.viologit.predict.val > logit.threshold, "1", "0")
confusions.pred.val = table(confusion.vioglm, val.dat$vio_recid_2yr)
dimnames(confusions.pred.val)[[1]] = c("No (Predicted)","Yes (Predicted)")
kable(confusions.pred.val,
      caption = paste("**Logistic regression - confusion matrix using validation data:**",
                      scales::percent(logit.threshold), "threshold"))
#kable(round(1- sum(diag(confusions.pred.val)) / sum(confusions.pred.val), 2), caption = "Misclassification Rate (with val.dat):")
#paste("Accuracy:")
logit.acc.vio <- sum(diag(confusions.pred.val)) / sum(confusions.pred.val)
#NOTE: These are not formatted correctly for the caret::sensitivity() and specificity() functions. Those functions require Yeses to come before Nos. I switched these to calculate manually.
#paste("Sensitivity Rate:")
logit.sens.vio <- confusions.pred.val["Yes (Predicted)", "Yes"] / sum(confusions.pred.val[,"Yes"])
#paste("Specificity Rate:")
logit.spec.vio <- confusions.pred.val["No (Predicted)", "No"] / sum(confusions.pred.val[,"No"])
kable(tibble(Accuracy = scales::percent(logit.acc.vio, accuracy = 0.1),
             Sensitivity = scales::percent(logit.sens.vio, accuracy = 0.1),
             Specificity = scales::percent(logit.spec.vio, accuracy = 0.1)),
      format = "markdown")
```

#### **2a.2V Linear Discriminant Analysis (LDA)**  

**Model Outcome**
Based on experience with the logistic regression, we realized that 50% is not an appropriate threshold for violent recidivism models because the proportion of violent recidivators is so low. Therefore, we decided to try a threshold of ``r scales::percent(mean(train.dat$vio_recid_2yr == "Yes"), accuracy = 0.1)`` (the violent recidivism rate in our training data), and our LDA achieved the following results:  

```{r}
recid.vio.lda <- lda(vio_recid_2yr ~
                   crime_factor +
                   age +
                   race_factor + 
                   gender_factor +
                   priors_log +
                   juv_other_bi +
                   length_of_stay,
                 data = train.dat)
#Results are currently for the training data
recid.vio.lda.pred.train <- predict(recid.vio.lda,
                          type = "response")
rate.vio2 <- mean(train.dat$vio_recid_2yr == "Yes") #Violent recidivism rate in training data
lda.tab = table(predicted = (recid.vio.lda.pred.train$posterior[,"Yes"] > rate.vio2),
                actual = train.dat$vio_recid_2yr)
dimnames(lda.tab)[[1]] = c("No (Predicted)","Yes (Predicted)")
kable(lda.tab,
      caption = paste("**LDA: confusion matrix using training data:**",
                               scales::percent(rate.vio2, 0.1),
                               "threshold"))
lda.acc <- (lda.tab[1,1] + lda.tab[2,2]) / sum(lda.tab)
lda.sens <- lda.tab[2,2] / sum(lda.tab[,2])
lda.spec <- lda.tab[1,1] / sum(lda.tab[,1])
kable(tibble(Accuracy = scales::percent(lda.acc, accuracy = 0.1),
             Sensitivity = scales::percent(lda.sens, accuracy = 0.1),
             Specificity = scales::percent(lda.spec, accuracy = 0.1)),
      format = "markdown")
```
  
**Model Validation**  
Just like for logistic regression, we then re-ran the model using our validation data and a threshold of ``r scales::percent(rate.vio2, accuracy = 0.1)``. The results are below.
```{r}
lda.pred.val <- predict(object = recid.vio.lda, val.dat, type = "response")
lda.cv.tab = table(predicted = (lda.pred.val$posterior[,"Yes"] > rate.vio2),
                   actual = val.dat$vio_recid_2yr)
dimnames(lda.cv.tab)[[1]] = c("No (Predicted)","Yes (Predicted)")
kable(lda.cv.tab,
      caption = paste("**LDA: confusion matrix using validation data:**",
                      scales::percent(rate.vio2, 0.1),
                      "threshold"))
lda.cv.acc.vio <- (lda.cv.tab[1,1] + lda.cv.tab[2,2]) / sum(lda.cv.tab)
lda.cv.sens.vio <- lda.cv.tab[2,2] / sum(lda.cv.tab[,2])
lda.cv.spec.vio <- lda.cv.tab[1,1] / sum(lda.cv.tab[,1])
kable(tibble(Accuracy = scales::percent(lda.cv.acc.vio, accuracy = 0.1),
             Sensitivity = scales::percent(lda.cv.sens.vio, accuracy = 0.1),
             Specificity = scales::percent(lda.cv.spec.vio, accuracy = 0.1)))
#ROC Curve
roc_lda.vio <- roc(response = val.dat$vio_recid_2yr,
               predictor = lda.pred.val$posterior[,"Yes"],
               percent = TRUE)
plot(roc_lda.vio,
     main="LDA - ROC Chart (Violent Recidivism)",
     col = roc_lda.col,
     lwd = 3,
     asp = 0,
     print.auc = TRUE,
     print.thres = TRUE,
     print.thres.pattern = "%.3f (%.1f%%, %.1f%%)")
```


#### **2a.3V Quadratic Discriminant Analysis (QDA)**  

**Variable Selection and Model Outcomes**  
We then checked to see if QDA could outperform LDA -- which might be the case if covariation between input variables were different across classes. Below are the accuracy, sensitivity, and specificity for a ``r scales::percent(rate.vio2, accuracy = 0.1)`` threshold: 
```{r}
recid.vio.qda <- qda(vio_recid_2yr ~
                   crime_factor +
                   age +
                   race_factor + 
                   gender_factor +
                   priors_log +
                   juv_other_bi +
                   length_of_stay,
                 data = train.dat)
recid.vio.qda.pred.train <- predict(recid.vio.qda,
                          type = "response")
qda.tab = table(predicted = (recid.vio.qda.pred.train$posterior[,"Yes"] > rate.vio2),
                actual = train.dat$two_year_recid)
dimnames(qda.tab)[[1]] = c("No (Predicted)","Yes (Predicted)")
kable(qda.tab, caption = paste("**QDA: confusion matrix using training data:**",
                               scales::percent(rate.vio2, 0.1),
                               "threshold"))
qda.acc <- (qda.tab[1,1] + qda.tab[2,2]) / sum(qda.tab)
qda.sens <- qda.tab[2,2] / sum(qda.tab[,2])
qda.spec <- qda.tab[1,1] / sum(qda.tab[,1])
kable(tibble(Accuracy = scales::percent(qda.acc, accuracy = 0.1),
             Sensitivity = scales::percent(qda.sens, accuracy = 0.1),
             Specificity = scales::percent(qda.spec, accuracy = 0.1)))
```

**Validation of QDA**
```{r}
qda.cv.pred <- predict(object = recid.vio.qda,
                       newdata = val.dat,
                       type = "response")
qda.cv.tab = table(predicted = (qda.cv.pred$posterior[,"Yes"] > rate.vio2),
                   actual = val.dat$two_year_recid)
dimnames(qda.cv.tab)[[1]] = c("No (Predicted)","Yes (Predicted)")
dimnames(qda.cv.tab)[[2]] = c("No", "Yes")
kable(qda.cv.tab, caption = paste("**QDA: confusion matrix using validation data:**",
                                  scales::percent(rate.vio2, 0.1),
                                  "threshold"))
qda.cv.acc.vio <- (qda.cv.tab[1,1] + qda.cv.tab[2,2]) / sum(qda.cv.tab)
qda.cv.sens.vio <- qda.cv.tab[2,2] / sum(qda.cv.tab[,2])
qda.cv.spec.vio <- qda.cv.tab[1,1] / sum(qda.cv.tab[,1])
kable(tibble(Accuracy = scales::percent(qda.cv.acc.vio, accuracy = 0.1),
             Sensitivity = scales::percent(qda.cv.sens.vio, accuracy = 0.1),
             Specificity = scales::percent(qda.cv.spec.vio, accuracy = 0.1)))
#ROC
roc_qda.vio <- roc(response = val.dat$any_recid_2yr,
                   predictor = qda.cv.pred$posterior[,"Yes"],
                   percent = TRUE)
plot(roc_qda.vio,
     main="QDA - ROC Chart (Violent Recidivism)",
     col = roc_qda.col,
     lwd = 3,
     asp = 0,
     print.auc = TRUE,
     print.thres = TRUE,
     print.thres.pattern = "%.3f (%.1f%%, %.1f%%)")
```


#### **2a.4V Classification And Regression Tree**  

The output shows a classification tree that predicts that nobody will violently recidivate. It is a bit odd that the tree still splits on priors, but the regression tree sheds some light on that. Both branches of the classification tree lead to "no", but the ROC curve shows us that this is because the model used an inappropriate 50% cutoff.  

```{r}
#CLASSIFICATION TREE -- direct copy/paste
#Run tree
tree.viorecid <- tree(formula = update(fullmodelvio, ~ . - priors_log),
                      data = train.dat)
par(mfrow = c(1,2))
#Plot
plot(tree.viorecid)
text(tree.viorecid, pretty = 0)
# Regression Tree - Moses version
#Run REGRESSION tree
reg.tree.recidvio <- tree(formula = (as.numeric(vio_recid_2yr) - 1) ~
                         crime_factor +
                         age +
                         age_factor +
                         race_factor +
                         gender_factor +
                         priors_count +
                         juv_fel_count +
                         juv_misd_count +
                         juv_other_count +
                         length_of_stay,
                       data = train.dat)
plot(reg.tree.recidvio)
text(reg.tree.recidvio, pretty = 0)
#Create new var for ROC curve -- for all datasets -- also added this to the master program up to.
train.dat$response.recidvio <- (as.numeric(train.dat$vio_recid_2yr) - 1)
val.dat$response.recidvio <- (as.numeric(val.dat$vio_recid_2yr) - 1)
test.dat$response.recidvio <- (as.numeric(test.dat$vio_recid_2yr) - 1)
#Predict and use the training data
tree.pred.rpart.train   <- predict(reg.tree.recidvio, val.dat)
#Create ROC curve
roccurve.rpart.vio <- roc(response = val.dat$response.recidvio,
                     predictor = tree.pred.rpart.train,
                     percent = TRUE)
#Plot ROC -- still for training
par(mfrow = c(1,1))
plot.roc(roccurve.rpart.vio,
         print.auc = TRUE,
         print.thres = TRUE,
         print.thres.pattern = "%.3f\n(%.1f%%,\n%.1f%%)",
         asp = 0,
         col = roccurve.trpart.col)
tree.vals.vio <- coords(roccurve.rpart.vio, "best", ret = c("threshold", "specificity", "sensitivity", "accuracy"))
```

Even with the best threshold, sensitivity and specificity are quite low. This is clearly not a particularly useful model.  

#### **2a.5V Random Forest**  

```  {r}
recid.vio.rf =randomForest(formula = update(fullmodelvio, ~ . -
                                              priors_log -
                                              juv_fel_bi -
                                              juv_misd_bi -
                                              juv_other_bi -
                                              age_factor),
                           data    = train.dat,
                           mtry    = 4 #this should be sqrt(p) since we're doing classification. In our case, p = 14.
                           ,importance=TRUE #keeping this from book
)
``` 

The confusion matrix for our random forest created using training data is below:
```{r}
kable(recid.vio.rf$confusion[,1:2], caption = "Random Forest: confusion matrix using training data")
rf.vio.error = (recid.vio.rf$confusion[2,1] + recid.vio.rf$confusion[1,2]) /nrow(train.dat)  
```
  
This matrix shows that the overall error rate for this random forest is ``r scales::percent(rf.vio.error, accuracy = 0.1)``.  

```{r}
varImpPlot(recid.vio.rf, main="Variable Importance by Factor")
```


We also plotted an ROC curve for the random forest, as shown below.
```{r, warnings = FALSE}
rf.vio.predict <- as.data.frame(predict(recid.vio.rf, val.dat, type = "prob"))
#Adapting code from other parts of the program
##Update this: two_year_recid news a violent equivalent
roccurve.vio.rpart<- roc(response = val.dat$two_year_recid,
                         predictor = rf.vio.predict$Yes,
                         percent = TRUE)
plot.roc(roccurve.vio.rpart,
         print.auc = TRUE,
         print.thres = TRUE,
         print.thres.pattern = "%.3f\n(%.1f%%, %.1f%%)",
         col = roccurve.rpart.col,
         asp = 0)
rf.vals.vio <- coords(roccurve.vio.rpart, "best", ret = c("threshold", "specificity", "sensitivity", "accuracy"))
```

## **2b Model Decisions**  
```{r}
##Stats for All Recidivism
confusions.compas.any = table(predicted = recid.new$score_factor, actual = recid.new$any_recid_2yr)
dimnames(confusions.compas.any)[[1]] = c("No (Predicted)","Yes (Predicted)")
# kable(confusions.compas.any, caption = "Confusion matrix for the COMPAS: any recidivism")
compas.acc.any <- sum(diag(confusions.compas.any)) / sum(confusions.compas.any)
compas.sens.any <- confusions.compas.any["Yes (Predicted)", "Yes"] / sum(confusions.compas.any[,"Yes"])
compas.spec.any <- confusions.compas.any["No (Predicted)", "No"] / sum(confusions.compas.any[,"No"])
##Stats for Violent Recidivism
confusions.compas.vio = table(predicted = recid.new$v_score_factor, actual = recid.new$vio_recid_2yr)
dimnames(confusions.compas.vio)[[1]] = c("No (Predicted)","Yes (Predicted)")
# kable(confusions.compas.vio, caption = "Confusion matrix for the COMPAS: vio recidivism")
compas.acc.vio <- sum(diag(confusions.compas.vio)) / sum(confusions.compas.vio)
compas.sens.vio <- confusions.compas.vio["Yes (Predicted)", "Yes"] / sum(confusions.compas.vio[,"Yes"])
compas.spec.vio <- confusions.compas.vio["No (Predicted)", "No"] / sum(confusions.compas.vio[,"No"])
```
  
**Summary Statistics for the best version of each data mining method**    

This table (and the table below for violent recidivism) uses the thresholds we discussed in our earlier analysis.

| Model for any recidivism                   |  Accuracy      | Sensitivity      | Specificity |  
|--------------------------------------------|----------------|------------------|-------------|  
| **COMPAS Model **                          |**`r scales::percent(compas.acc.any, 0.1)`** |**`r scales::percent(compas.sens.any, 0.1)`** |**`r scales::percent(compas.spec.any, 0.1)`** |
| <span style = "color: #4D9DE0;">***Logistic regression*** </span>|<span style = "color: #4D9DE0;">*`r scales::percent(logit.acc, 0.1)`*</span>|<span style = "color: #4D9DE0;">*`r scales::percent(logit.sens, 0.1)`* </span>|<span style = "color: #4D9DE0;">*`r scales::percent(logit.spec)`* </span>|  
| <span style = "color: #E15554;">**Linear Discriminant Analysis (LDA)**</span>|<span style = "color: #E15554;">`r scales::percent(lda.cv.acc, 0.1)`  </span>|<span style = "color: #E15554;"> `r scales::percent(lda.cv.sens, 0.1)` </span> |<span style = "color: #E15554;">`r scales::percent(lda.cv.spec, 0.1)` </span>|  
| <span style = "color: #E1BC29">**Quadratic Discriminant Analysis (QDA)** </span>|<span style = "color: #E1BC29">`r scales::percent(qda.cv.acc, 0.1)` </span> |<span style = "color: #E1BC29"> `r scales::percent(qda.cv.sens, 0.1)` </span> |<span style = "color: #E1BC29">`r scales::percent(qda.cv.spec, 0.1)`</span>|  
| <span style = "color: #7768AE">**Decision Tree** </span> |<span style = "color: #7768AE">`r round(tree.vals$accuracy, 1)`% </span>|<span style = "color: #7768AE"> `r round(tree.vals$sensitivity, 1)`% </span>|<span style = "color: #7768AE"> `r round(tree.vals$specificity, 1)`% </span>|  
| <span style = "color: #3BB273;">**Random Forest (RF)** </span> |<span style = "color: #3BB273;">`r round(rf.vals$accuracy, 1)`% </span>|<span style = "color: #3BB273;"> `r round(rf.vals$sensitivity, 1)`% </span>|<span style = "color: #3BB273;"> `r round(rf.vals$specificity, 1)`% </span>|  
  
Another helpful illustration is a side-by-side comparison of the models' ROC curves. LDA and logistic regression have very similar performance, and both models dominate the others, meaning that they outperform them at every threshold.  

```{r}
### Comparing ROC charts
#Logistic ROC
plot(roccurve.logit,
     grid = TRUE,
     col = roccurve.logit.col,
     lty = 11,
     asp = 0,
     main = "Comparing ROC Curves (All Recidivism)",
     print.auc = TRUE,
     print.auc.pattern = "AUC:\n%.1f")
# print.thres.pattern = "%.3f\n(%.1f%%, %.1f%%)"
#LDA ROC
plot(roc_lda, col = roc_lda.col, lwd = 3, print.auc = TRUE, print.auc.pattern = "\n\n%.1f",  add = TRUE)
#QDA ROC
plot (roc_qda, col = roc_qda.col, lty = 10, print.auc = TRUE, print.auc.pattern = "\n\n\n%.1f", add = TRUE)
#Tree ROC
plot (roccurve.trpart, col = roccurve.trpart.col, lty = 3, print.auc = TRUE, print.auc.pattern = "\n\n\n\n%.1f", add = TRUE)
#Random Forest ROC
plot (roccurve.rpart, col = roccurve.rpart.col, lty = 2, print.auc = TRUE, print.auc.pattern = "\n\n\n\n\n%.1f", add = TRUE)
legend(x = 42,
       y = 50,
       legend = c("Logistic Regression", "LDA", "QDA", "Regression Tree", "Random Forest"),lty = c(1,1), 
       lwd = c(2,2),
       col = c(roccurve.logit.col,
               roc_lda.col,
               roc_qda.col,
               roccurve.trpart.col,
               roccurve.rpart.col))
      
```

| Model for violent recidivism               |  Accuracy      | Sensitivity      | Specificity |  
|--------------------------------------------|----------------|------------------|-------------|  
| **COMPAS Model **                          |**`r scales::percent(compas.acc.vio, 0.1)`** |**`r scales::percent(compas.sens.vio, 0.1)`** |**`r scales::percent(compas.spec.vio, 0.1)`** |
|<span style = "color: #4D9DE0;"> ***Logistic regression***</span>|<span style = "color: #4D9DE0;">*`r scales::percent(logit.acc.vio, 0.1)`*</span> |<span style = "color: #4D9DE0;">*`r scales::percent(logit.sens.vio, 0.1)`*</span> |<span style = "color: #4D9DE0;">*`r scales::percent(logit.spec.vio, 0.1)`*</span>|  
| <span style = "color: #E15554;">**Linear Discriminant Analysis (LDA)** </span> |<span style = "color: #E15554;">`r scales::percent(lda.cv.acc.vio, 0.1)` </span>|<span style = "color: #E15554;">`r scales::percent(lda.cv.sens.vio, 0.1)` </span>|<span style = "color: #E15554;">`r scales::percent(lda.cv.spec.vio, 0.1)`</span>|  
| <span style = "color: #E1BC29;">**Quadratic Discriminant Analysis (QDA)** </span> |<span style = "color: #E1BC29">`r scales::percent(qda.cv.acc.vio, 0.1)`</span> |<span style = "color: #E1BC29">`r scales::percent(qda.cv.sens.vio, 0.1)`</span> |<span style = "color: #E1BC29">`r scales::percent(qda.cv.spec.vio, 0.1)`</span>|  
| <span style = "color: #7768AE;">**Decision Tree** </span>  |<span style = "color: #7768AE;">`r round(tree.vals.vio$accuracy, 1)`%</span> |<span style = "color: #7768AE;">`r round(tree.vals.vio$sensitivity, 1)`% </span>|<span style = "color: #7768AE;">`r round(tree.vals.vio$specificity, 1)`%</span> |  
| <span style = "color: #3BB273;">**Random Forest (RF)**  </span> |<span style = "color: #3BB273;">`r round(rf.vals.vio$accuracy, 1)`%</span> |<span style = "color: #3BB273;"> `r round(rf.vals.vio$sensitivity, 1)`%</span> |<span style = "color: #3BB273;"> `r round(rf.vals.vio$specificity, 1)`%</span> |  
  
For violent recidivism, QDA performs best for thresholds that lead to low sensitivity and high specificity. However, sensitivity is important for preventing violent crime, so we chose logistic regression again, which performs better on the right half of the graph.

```{r}
### Comparing Violent ROC charts
#Logistic ROC
plot(roccurve.vio,
     grid = TRUE,
     col = roccurve.logit.col,
     lty = 11,
     asp = 0,
     main = "Comparing ROC Curves (Violent Recidivism)",
     print.auc = TRUE,
     print.auc.pattern = "AUC:\n%.1f")
# print.thres.pattern = "%.3f\n(%.1f%%, %.1f%%)"
#LDA ROC
plot(roc_lda.vio, col = roc_lda.col, lwd = 3, print.auc = TRUE, print.auc.pattern = "\n\n%.1f",  add = TRUE)
#QDA ROC
plot (roc_qda.vio, col = roc_qda.col, lty = 10, print.auc = TRUE, print.auc.pattern = "\n\n\n%.1f", add = TRUE)
#Tree ROC
plot (roccurve.rpart.vio, col = roccurve.trpart.col, lty = 3, print.auc = TRUE, print.auc.pattern = "\n\n\n\n%.1f", add = TRUE)
#Random Forest ROC
plot (roccurve.vio.rpart, col = roccurve.rpart.col, lty = 2, print.auc = TRUE, print.auc.pattern = "\n\n\n\n\n%.1f", add = TRUE)
legend(x = 42,
       y = 50,
       legend = c("Logistic Regression", "LDA", "QDA", "Regression Tree", "Random Forest"),lty = c(1,1), 
       lwd = c(2,2),
       col = c(roccurve.logit.col,
               roc_lda.col,
               roc_qda.col,
               roccurve.trpart.col,
               roccurve.rpart.col))
      
```


## **2c: Final model(s)**
Our final model for all recidivism is a logistic regression with a threshold of 0.6. We chose to use a logistic regression because (along with LDA) it had the largest AUC and the best combinations of sensitivity and specificity. We chose 0.6 as our threshold because it offered very high specificity without sacrificing much accuracy.
Our final model for violent recidivism is also a logistic regression, because again it was (together with LDA and QDA) a top-performing model. We chose the optimal threshold of 0.103 because it offered strong sensitivity as well as specificity, both of which are important for predicting violent recidivism.

Then we ran it with our test data (10% of the original dataset randomly selected before any of data mining began) and found the following results.

```{r, message = FALSE}
final.predict.test.any = predict(glm.logit, test.dat, type="response") 
final.predict.test.any = ifelse(final.predict.test.any > 0.6, "1", "0") #threshold of 0.6
confusions.pred.test.any = table(final.predict.test.any, test.dat$any_recid_2yr)
##Confusion Matrix
dimnames(confusions.pred.test.any)[[1]] = c("No (Predicted)","Yes (Predicted)")
kable(confusions.pred.test.any, caption = "**Final Results with Test Data (Any Recidivism)**")
final.acc.vio <- sum(diag(confusions.pred.test.any)) / sum(confusions.pred.test.any)
final.sens.vio <- confusions.pred.test.any["Yes (Predicted)", "Yes"] / sum(confusions.pred.test.any[,"Yes"])
final.spec.vio <- confusions.pred.test.any["No (Predicted)", "No"] / sum(confusions.pred.test.any[,"No"])
kable(tibble(Accuracy = scales::percent(final.acc.vio, 0.1),
             Sensitivity = scales::percent(final.sens.vio, 0.1),
             Specificity = scales::percent(final.spec.vio, 0.1)))
```

And for violent recidivism...
```{r}
##Violent
final.predict.test.vio = predict(glm.viologit, test.dat, type="response") 
final.predict.test.vio = ifelse(final.predict.test.vio > 0.103, "1", "0") #threshold of 0.103
confusions.pred.test.vio = table(final.predict.test.vio, test.dat$vio_recid_2yr)
##Confusion Matrix
dimnames(confusions.pred.test.vio)[[1]] = c("No (Predicted)","Yes (Predicted)")
kable(confusions.pred.test.vio, caption = "**Final Results with Test Data (Violent Recidivism)**")
final.acc.vio <- sum(diag(confusions.pred.test.vio)) / sum(confusions.pred.test.vio)
final.sens.vio <- confusions.pred.test.vio["Yes (Predicted)", "Yes"] / sum(confusions.pred.test.vio[,"Yes"])
final.spec.vio <- confusions.pred.test.vio["No (Predicted)", "No"] / sum(confusions.pred.test.vio[,"No"])
kable(tibble(Accuracy = scales::percent(final.acc.vio, 0.1),
             Sensitivity = scales::percent(final.sens.vio, 0.1),
             Specificity = scales::percent(final.spec.vio, 0.1)))
```

There was almost no drop in accuracy when we switched to using test data, suggesting that over-fitting did not occur.

**Comparison of predictors for all recidivism vs. violent recidivism**
While the predictors chosen by our Random Forests and regsubsets analyses were similar for all recidivism and violent recidivism, our final logistic model for violent recidivism included race based on the regsubsets analysis. 

## **2d: Demographic analysis of our model:**  
After choosing the best version of each model, we assessed the potential for prejudice. Our goal was to replicate Pro-Publica's chart for each of these metrics included in the introduction. Notably, all of our demographic analysis below uses the $test$ data, which has a fairly low sample size.

### **2d1: Race**  

**Original tool**  

Per our introduction, ProPublica's key finding about the COMPAS was that false-positive and false-negative rates were different across demographic groups. A reproduction of their table is below.  

| COMPAS finding: any recidivism              |           White | African American |  
|--------------------------------------------|-----------------|------------------|
| Labeled Higher Risk, But Didn’t Re-Offend  (False Positive)| 23.5%|       44.9% |
| Labeled Lower Risk, Yet Did Re-Offend      (False Negative)| 47.7%|       28.0% |

We also created a matching table for the COMPAS's false positives/negatives on violent recidivism by race.  
```{r}
#Subset full dataset by race
white.dat <- recid.new[recid.new$race_factor == "Caucasian",]
AfAm.dat <- recid.new[recid.new$race_factor == "African-American",]
#Check PP chart for any recidivism
# confusions.compas.any.white = table(predicted = white.dat$score_factor, actual = white.dat$any_recid_2yr)
# dimnames(confusions.compas.any.white)[[1]] = c("No (Predicted)","Yes (Predicted)")
# FP.compas.any.white <- confusions.compas.any.white["Yes (Predicted)", "No"] / sum(confusions.compas.any.white[,"No"])
# FN.compas.any.white <-  confusions.compas.any.white["No (Predicted)", "Yes"] / sum(confusions.compas.any.white[,"Yes"])
# 
# confusions.compas.any.AfAm = table(predicted = AfAm.dat$score_factor, actual = AfAm.dat$any_recid_2yr)
# dimnames(confusions.compas.any.AfAm)[[1]] = c("No (Predicted)","Yes (Predicted)")
# FP.compas.any.AfAm <- confusions.compas.any.AfAm["Yes (Predicted)", "No"] / sum(confusions.compas.any.AfAm[,"No"])
# FN.compas.any.AfAm <-  confusions.compas.any.AfAm["No (Predicted)", "Yes"] / sum(confusions.compas.any.AfAm[,"Yes"])
#Create PP chart for violent data, since they didn't create it themselves.
confusions.compas.vio.white = table(predicted = white.dat$v_score_factor, actual = white.dat$vio_recid_2yr)
dimnames(confusions.compas.vio.white)[[1]] = c("No (Predicted)","Yes (Predicted)")
FP.compas.vio.white <- confusions.compas.vio.white["Yes (Predicted)", "No"] / sum(confusions.compas.vio.white[,"No"])
FN.compas.vio.white <-  confusions.compas.vio.white["No (Predicted)", "Yes"] / sum(confusions.compas.vio.white[,"Yes"])
confusions.compas.vio.AfAm = table(predicted = AfAm.dat$v_score_factor, actual = AfAm.dat$vio_recid_2yr)
dimnames(confusions.compas.vio.AfAm)[[1]] = c("No (Predicted)","Yes (Predicted)")
FP.compas.vio.AfAm <- confusions.compas.vio.AfAm["Yes (Predicted)", "No"] / sum(confusions.compas.vio.AfAm[,"No"])
FN.compas.vio.AfAm <-  confusions.compas.vio.AfAm["No (Predicted)", "Yes"] / sum(confusions.compas.vio.AfAm[,"Yes"])
```

| Compas Finding: Violent Recidivism               |           White | African American | 
|--------------------------------------------|-----------------|------------------|
| Labeled Higher Risk, But Didn’t Re-Offend  (False Positive)      |`r scales::percent(FP.compas.vio.white, 0.1)`  | `r scales::percent(FP.compas.vio.AfAm, 0.1)` |
| Labeled Lower Risk, Yet Did Re-Offend      (False Negative)      |`r scales::percent(FN.compas.vio.white, 0.1)`  | `r scales::percent(FN.compas.vio.AfAm, 0.1)` |

**Our tool**  

We see that, while we were able to roughly halve the False Positive rate for both Caucasian and African American groups, our model is still biased against the African American group. Both their False Postive rate and their False Negative rate are higher and lower, respectively, than that of the Caucasian group. This means that our model is still more likely to incorrectly predict that African American individuals will recidivate than that white ones will. 

``` {r}
#Create 0/1 flags for white and African-American
test.dat$AfAm = ifelse(test.dat$race_factor == "African-American", 1, 0)
test.dat$white = ifelse(test.dat$race_factor == "Caucasian", 1, 0)
white.test.dat <- test.dat[test.dat$white==1,]
AfAm.test.dat <- test.dat[test.dat$AfAm==1,]
##WHITE PEOPLE VERSION OF FINAL TEST - any 
final.predict.test.any.white = predict(glm.logit, white.test.dat, type="response") 
final.predict.test.any.white = ifelse(final.predict.test.any.white > 0.6, "1", "0") #threshold of 0.6
confusions.pred.test.any.white = table(final.predict.test.any.white, white.test.dat$any_recid_2yr)
dimnames(confusions.pred.test.any.white)[[1]] = c("No (Predicted)","Yes (Predicted)")
FP.any.white <- confusions.pred.test.any.white["Yes (Predicted)", "No"] / sum(confusions.pred.test.any.white[,"No"])
FN.any.white <-  confusions.pred.test.any.white["No (Predicted)", "Yes"] / sum(confusions.pred.test.any.white[,"Yes"])
  
##Black PEOPLE VERSION OF FINAL TEST - any 
final.predict.test.any.AfAm = predict(glm.logit, AfAm.test.dat, type="response") 
final.predict.test.any.AfAm = ifelse(final.predict.test.any.AfAm > 0.6, "1", "0") #threshold of 0.6
confusions.pred.test.any.AfAm = table(final.predict.test.any.AfAm, AfAm.test.dat$any_recid_2yr)
dimnames(confusions.pred.test.any.AfAm)[[1]] = c("No (Predicted)","Yes (Predicted)")
FP.any.AfAm <- confusions.pred.test.any.AfAm["Yes (Predicted)", "No"] / sum(confusions.pred.test.any.AfAm[,"No"])
FN.any.AfAm <-  confusions.pred.test.any.AfAm["No (Predicted)", "Yes"] / sum(confusions.pred.test.any.AfAm[,"Yes"])
# ##WHITE PEOPLE VERSION OF FINAL TEST - vio 
# final.predict.test.vio.white = predict(glm.logit, white.test.dat, type="response") 
# final.predict.test.vio.white = ifelse(final.predict.test.vio.white > 0.6, "1", "0") #threshold of 0.6
# confusions.pred.test.vio.white = table(final.predict.test.vio.white, white.test.dat$vio_recid_2yr)
# dimnames(confusions.pred.test.vio.white)[[1]] = c("No (Predicted)","Yes (Predicted)")
# FP.vio.white <- confusions.pred.test.vio.white["Yes (Predicted)", "No"] / sum(confusions.pred.test.vio.white[,"No"])
# FN.vio.white <-  confusions.pred.test.vio.white["No (Predicted)", "Yes"] / sum(confusions.pred.test.vio.white[,"Yes"])
#   
# ##Black PEOPLE VERSION OF FINAL TEST - vio 
# final.predict.test.vio.AfAm = predict(glm.logit, AfAm.test.dat, type="response") 
# final.predict.test.vio.AfAm = ifelse(final.predict.test.vio.AfAm > 0.6, "1", "0") #threshold of 0.6
# confusions.pred.test.vio.AfAm = table(final.predict.test.vio.AfAm, AfAm.test.dat$vio_recid_2yr)
# dimnames(confusions.pred.test.vio.AfAm)[[1]] = c("No (Predicted)","Yes (Predicted)")
# FP.vio.AfAm <- confusions.pred.test.vio.AfAm["Yes (Predicted)", "No"] / sum(confusions.pred.test.vio.AfAm[,"No"])
# FN.vio.AfAm <-  confusions.pred.test.vio.AfAm["No (Predicted)", "Yes"] / sum(confusions.pred.test.vio.AfAm[,"Yes"])
#   
```


| Our RAI: Any Recidivism                                          |           White | African American | 
|------------------------------------------------------------------|-----------------|------------------|
| Labeled Higher Risk, But Didn’t Re-Offend  (False Positive)      |`r scales::percent(FP.any.white, 0.1)`  | `r scales::percent(FP.any.AfAm, 0.1)` |
| Labeled Lower Risk, Yet Did Re-Offend      (False Negative)      |`r scales::percent(FN.any.white, 0.1)`  | `r scales::percent(FN.any.AfAm, 0.1)` |


``` {r}
##WHITE PEOPLE VERSION OF FINAL TEST - vio
final.predict.test.vio.white = predict(glm.logit, white.test.dat, type="response")
final.predict.test.vio.white = ifelse(final.predict.test.vio.white > 0.6, "1", "0") #threshold of 0.6
confusions.pred.test.vio.white = table(final.predict.test.vio.white, white.test.dat$vio_recid_2yr)
dimnames(confusions.pred.test.vio.white)[[1]] = c("No (Predicted)","Yes (Predicted)")
FP.vio.white <- confusions.pred.test.vio.white["Yes (Predicted)", "No"] / sum(confusions.pred.test.vio.white[,"No"])
FN.vio.white <-  confusions.pred.test.vio.white["No (Predicted)", "Yes"] / sum(confusions.pred.test.vio.white[,"Yes"])
##Black PEOPLE VERSION OF FINAL TEST - vio
final.predict.test.vio.AfAm = predict(glm.logit, AfAm.test.dat, type="response")
final.predict.test.vio.AfAm = ifelse(final.predict.test.vio.AfAm > 0.6, "1", "0") #threshold of 0.6
confusions.pred.test.vio.AfAm = table(final.predict.test.vio.AfAm, AfAm.test.dat$vio_recid_2yr)
dimnames(confusions.pred.test.vio.AfAm)[[1]] = c("No (Predicted)","Yes (Predicted)")
FP.vio.AfAm <- confusions.pred.test.vio.AfAm["Yes (Predicted)", "No"] / sum(confusions.pred.test.vio.AfAm[,"No"])
FN.vio.AfAm <-  confusions.pred.test.vio.AfAm["No (Predicted)", "Yes"] / sum(confusions.pred.test.vio.AfAm[,"Yes"])
#   
```

| Our RAI: Violent Recidivism                                          |           White | African American | 
|------------------------------------------------------------------|-----------------|------------------|
| Labeled Higher Risk, But Didn’t Re-Offend  (False Positive)      |`r scales::percent(FP.vio.white, 0.1)`  | `r scales::percent(FP.vio.AfAm, 0.1)` |
| Labeled Lower Risk, Yet Did Re-Offend      (False Negative)      |`r scales::percent(FN.vio.white, 0.1)`  | `r scales::percent(FN.vio.AfAm, 0.1)` |


### **2d2: Gender**  

``` {r}
female.dat <- recid.new[test.dat$gender_factor == "Female",]
male.dat <- recid.new[test.dat$gender_factor == "Male",]
female.test.dat <- test.dat[test.dat$gender_factor == "Female",]
male.test.dat <- test.dat[test.dat$gender_factor == "Male",]

#COMPAS for gender
confusions.compas.any.female = table(predicted = female.dat$score_factor, actual = female.dat$any_recid_2yr)
dimnames(confusions.compas.any.female)[[1]] = c("No (Predicted)","Yes (Predicted)")
FP.compas.any.female <- confusions.compas.any.female["Yes (Predicted)", "No"] / sum(confusions.compas.any.female[,"No"])
FN.compas.any.female <-  confusions.compas.any.female["No (Predicted)", "Yes"] / sum(confusions.compas.any.female[,"Yes"])
confusions.compas.any.male = table(predicted = male.dat$score_factor, actual = male.dat$any_recid_2yr)
dimnames(confusions.compas.any.male)[[1]] = c("No (Predicted)","Yes (Predicted)")
FP.compas.any.male <- confusions.compas.any.male["Yes (Predicted)", "No"] / sum(confusions.compas.any.male[,"No"])
FN.compas.any.male <-  confusions.compas.any.male["No (Predicted)", "Yes"] / sum(confusions.compas.any.male[,"Yes"])
#COMPAS FOR GENDER - VIO
confusions.compas.vio.female = table(predicted = female.dat$v_score_factor, actual = female.dat$vio_recid_2yr)
dimnames(confusions.compas.vio.female)[[1]] = c("No (Predicted)","Yes (Predicted)")
FP.compas.vio.female <- confusions.compas.vio.female["Yes (Predicted)", "No"] / sum(confusions.compas.vio.female[,"No"])
FN.compas.vio.female <-  confusions.compas.vio.female["No (Predicted)", "Yes"] / sum(confusions.compas.vio.female[,"Yes"])
confusions.compas.vio.male = table(predicted = male.dat$v_score_factor, actual = male.dat$vio_recid_2yr)
dimnames(confusions.compas.vio.male)[[1]] = c("No (Predicted)","Yes (Predicted)")
FP.compas.vio.male <- confusions.compas.vio.male["Yes (Predicted)", "No"] / sum(confusions.compas.vio.male[,"No"])
FN.compas.vio.male <-  confusions.compas.vio.male["No (Predicted)", "Yes"] / sum(confusions.compas.vio.male[,"Yes"])
##Our RAI using test data
##female PEOPLE VERSION OF FINAL TEST - any 
final.predict.test.any.female = predict(glm.logit, female.test.dat, type="response") 
final.predict.test.any.female = ifelse(final.predict.test.any.female > 0.6, "1", "0") #threshold of 0.6
confusions.pred.test.any.female = table(final.predict.test.any.female, female.test.dat$any_recid_2yr)
dimnames(confusions.pred.test.any.female)[[1]] = c("No (Predicted)","Yes (Predicted)")
FP.any.female <- confusions.pred.test.any.female["Yes (Predicted)", "No"] / sum(confusions.pred.test.any.female[,"No"])
FN.any.female <-  confusions.pred.test.any.female["No (Predicted)", "Yes"] / sum(confusions.pred.test.any.female[,"Yes"])
  
##Black PEOPLE VERSION OF FINAL TEST - any 
final.predict.test.any.male = predict(glm.logit, male.test.dat, type="response") 
final.predict.test.any.male = ifelse(final.predict.test.any.male > 0.6, "1", "0") #threshold of 0.6
confusions.pred.test.any.male = table(final.predict.test.any.male, male.test.dat$any_recid_2yr)
dimnames(confusions.pred.test.any.male)[[1]] = c("No (Predicted)","Yes (Predicted)")
FP.any.male <- confusions.pred.test.any.male["Yes (Predicted)", "No"] / sum(confusions.pred.test.any.male[,"No"])
FN.any.male <-  confusions.pred.test.any.male["No (Predicted)", "Yes"] / sum(confusions.pred.test.any.male[,"Yes"])
```

**Original tool**  

ProPublica didn't analyze the COMPAS for differences by gender, so we added this analysis. It turns out that the COMPAS was remarkably fair across gender lines.  Ours was not.


| COMPAS Finding: Any Recidivism               |           Female | Male | 
|--------------------------------------------|-----------------|------------------|
| Labeled Higher Risk, But Didn’t Re-Offend  (False Positive)      |`r scales::percent(FP.compas.any.female, 0.1)`  | `r scales::percent(FP.compas.any.male, 0.1)` |
| Labeled Lower Risk, Yet Did Re-Offend      (False Negative)      |`r scales::percent(FN.compas.any.female, 0.1)`  | `r scales::percent(FN.compas.any.male, 0.1)` |

| COMPAS Finding: Violent Recidivism               |           Female | Male | 
|--------------------------------------------|-----------------|------------------|
| Labeled Higher Risk, But Didn’t Re-Offend  (False Positive)      |`r scales::percent(FP.compas.vio.female, 0.1)`  | `r scales::percent(FP.compas.vio.male, 0.1)` |
| Labeled Lower Risk, Yet Did Re-Offend      (False Negative)      |`r scales::percent(FN.compas.vio.female, 0.1)`  | `r scales::percent(FN.compas.vio.male, 0.1)` |


**Our tool**  

From the Gender table below we see that our model strongly favors giving female individuals a lower recidivism risk rating. The False Negative rate of ``r scales::percent(FN.any.female, 0.1)`` is especially telling, indicating that our model correctly identifies only ``r round(100*(1-FN.any.female))`` of every hundred recidivating women.   


| Our RAI: Any Recidivism                                          |           Female | Male | 
|------------------------------------------------------------------|-----------------|------------------|
| Labeled Higher Risk, But Didn’t Re-Offend  (False Positive)      |`r scales::percent(FP.any.female, 0.1)`  | `r scales::percent(FP.any.male, 0.1)` |
| Labeled Lower Risk, Yet Did Re-Offend      (False Negative)      |`r scales::percent(FN.any.female, 0.1)`  | `r scales::percent(FN.any.male, 0.1)` |



``` {r}
female.test.dat <- test.dat[test.dat$gender_factor == "Female",]
male.test.dat <- test.dat[test.dat$gender_factor == "Male",]
##female PEOPLE VERSION OF FINAL TEST - vio 
final.predict.test.vio.female = predict(glm.logit, female.test.dat, type="response") 
final.predict.test.vio.female = ifelse(final.predict.test.vio.female > 0.6, "1", "0") #threshold of 0.6
confusions.pred.test.vio.female = table(final.predict.test.vio.female, female.test.dat$vio_recid_2yr)
dimnames(confusions.pred.test.vio.female)[[1]] = c("No (Predicted)","Yes (Predicted)")
FP.vio.female <- confusions.pred.test.vio.female["Yes (Predicted)", "No"] / sum(confusions.pred.test.vio.female[,"No"])
FN.vio.female <-  confusions.pred.test.vio.female["No (Predicted)", "Yes"] / sum(confusions.pred.test.vio.female[,"Yes"])
  
##Black PEOPLE VERSION OF FINAL TEST - vio 
final.predict.test.vio.male = predict(glm.logit, male.test.dat, type="response") 
final.predict.test.vio.male = ifelse(final.predict.test.vio.male > 0.6, "1", "0") #threshold of 0.6
confusions.pred.test.vio.male = table(final.predict.test.vio.male, male.test.dat$vio_recid_2yr)
dimnames(confusions.pred.test.vio.male)[[1]] = c("No (Predicted)","Yes (Predicted)")
FP.vio.male <- confusions.pred.test.vio.male["Yes (Predicted)", "No"] / sum(confusions.pred.test.vio.male[,"No"])
FN.vio.male <-  confusions.pred.test.vio.male["No (Predicted)", "Yes"] / sum(confusions.pred.test.vio.male[,"Yes"])
```

| Our RAI: Violent Recidivism                                          |           Female | Male | 
|------------------------------------------------------------------|-----------------|------------------|
| Labeled Higher Risk, But Didn’t Re-Offend  (False Positive)      |`r scales::percent(FP.vio.female, 0.1)`  | `r scales::percent(FP.vio.male, 0.1)` |
| Labeled Lower Risk, Yet Did Re-Offend      (False Negative)      |`r scales::percent(FN.vio.female, 0.1)`  | `r scales::percent(FN.vio.male, 0.1)` |

### **2d3: Age**  

**Original tool**  

We also test the COMPAS's bias on the basis of age, as shown in the following tables. Similar to race, the COMPAS was likely to misclassify people in different directions based on age. Specifically, young people were much more likely to be labeled high-risk without re-offending (false positive), while older people were more likely to be labeled low-risk while re-offending (false negatives).

``` {r}
#SUBSET BY AGE
age1.dat <- recid.new[recid.new$age_factor == "Less than 25",]
age2.dat <- recid.new[recid.new$age_factor == "25 - 45",]
age3.dat <- recid.new[recid.new$age_factor == "Greater than 45",]
#COMPAS for age1
confusions.compas.any.age1= table(predicted = age1.dat$score_factor, actual = age1.dat$any_recid_2yr)
dimnames(confusions.compas.any.age1)[[1]] = c("No (Predicted)","Yes (Predicted)")
FP.compas.any.age1 <- confusions.compas.any.age1["Yes (Predicted)", "No"] / sum(confusions.compas.any.age1[,"No"])
FN.compas.any.age1 <-  confusions.compas.any.age1["No (Predicted)", "Yes"] / sum(confusions.compas.any.age1[,"Yes"])
confusions.compas.vio.age1 = table(predicted = age1.dat$v_score_factor, actual = age1.dat$vio_recid_2yr)
dimnames(confusions.compas.vio.age1)[[1]] = c("No (Predicted)","Yes (Predicted)")
FP.compas.vio.age1 <- confusions.compas.vio.age1["Yes (Predicted)", "No"] / sum(confusions.compas.vio.age1[,"No"])
FN.compas.vio.age1 <-  confusions.compas.vio.age1["No (Predicted)", "Yes"] / sum(confusions.compas.vio.age1[,"Yes"])
#COMPAS for age2
confusions.compas.any.age2= table(predicted = age2.dat$score_factor, actual = age2.dat$any_recid_2yr)
dimnames(confusions.compas.any.age2)[[1]] = c("No (Predicted)","Yes (Predicted)")
FP.compas.any.age2 <- confusions.compas.any.age2["Yes (Predicted)", "No"] / sum(confusions.compas.any.age2[,"No"])
FN.compas.any.age2 <-  confusions.compas.any.age2["No (Predicted)", "Yes"] / sum(confusions.compas.any.age2[,"Yes"])
confusions.compas.vio.age2 = table(predicted = age2.dat$v_score_factor, actual = age2.dat$vio_recid_2yr)
dimnames(confusions.compas.vio.age2)[[1]] = c("No (Predicted)","Yes (Predicted)")
FP.compas.vio.age2 <- confusions.compas.vio.age2["Yes (Predicted)", "No"] / sum(confusions.compas.vio.age2[,"No"])
FN.compas.vio.age2 <-  confusions.compas.vio.age2["No (Predicted)", "Yes"] / sum(confusions.compas.vio.age2[,"Yes"])
#COMPAS for age3
confusions.compas.any.age3= table(predicted = age3.dat$score_factor, actual = age3.dat$any_recid_2yr)
dimnames(confusions.compas.any.age3)[[1]] = c("No (Predicted)","Yes (Predicted)")
FP.compas.any.age3 <- confusions.compas.any.age3["Yes (Predicted)", "No"] / sum(confusions.compas.any.age3[,"No"])
FN.compas.any.age3 <-  confusions.compas.any.age3["No (Predicted)", "Yes"] / sum(confusions.compas.any.age3[,"Yes"])
confusions.compas.vio.age3 = table(predicted = age3.dat$v_score_factor, actual = age3.dat$vio_recid_2yr)
dimnames(confusions.compas.vio.age3)[[1]] = c("No (Predicted)","Yes (Predicted)")
FP.compas.vio.age3 <- confusions.compas.vio.age3["Yes (Predicted)", "No"] / sum(confusions.compas.vio.age3[,"No"])
FN.compas.vio.age3 <-  confusions.compas.vio.age3["No (Predicted)", "Yes"] / sum(confusions.compas.vio.age3[,"Yes"])
```

| COMPAS: Any Recidivism                                          |           Under 25 | 25 - 45 |       45 and above|
|------------------------------------------------------------------|-----------------|------------------|--------------|
| Labeled Higher Risk, But Didn’t Re-Offend  (False positive)      |`r scales::percent(FP.compas.any.age1, 0.1)`  | `r scales::percent(FP.compas.any.age2, 0.1)` | `r scales::percent(FP.compas.any.age3, 0.1)`|
| Labeled Lower Risk, Yet Did Re-Offend      (False Negative)      |`r scales::percent(FN.compas.any.age1, 0.1)`  | `r scales::percent(FN.compas.any.age2, 0.1)` | `r scales::percent(FN.compas.any.age3, 0.1)`|


| COMPAS: Violent Recidivism                                          |           Under 25 | 25 - 45 |       45 and above|
|------------------------------------------------------------------|-----------------|------------------|--------------|
| Labeled Higher Risk, But Didn’t Re-Offend  (False positive)      |`r scales::percent(FP.compas.vio.age1, 0.1)`  | `r scales::percent(FP.compas.vio.age2, 0.1)` | `r scales::percent(FP.compas.vio.age3, 0.1)`|
| Labeled Lower Risk, Yet Did Re-Offend      (False Negative)      |`r scales::percent(FN.compas.vio.age1, 0.1)`  | `r scales::percent(FN.compas.vio.age2, 0.1)` | `r scales::percent(FN.compas.vio.age3, 0.1)`|

**Our tool**  

Based on our analysis focusing on age, we find that our model also seems to assume that if you are in the age group of "Greater than 45," then you are at a lower risk of recidivating than the actual data shows. Furthermore, the younger an individual is, the more likely our model is to give that individual a higher risk rating. While this is in parallel with what we found in our analysis of the data during our model building process (see the PCA plot), our model is biased against younger individuals.  

``` {r}
age1.test.dat <- test.dat[test.dat$age_factor == "Less than 25",]
age2.test.dat <- test.dat[test.dat$age_factor == "25 - 45",]
age3.test.dat <- test.dat[test.dat$age_factor == "Greater than 45",]
##age1 PEOPLE VERSION OF FINAL TEST - any 
final.predict.test.any.age1 = predict(glm.logit, age1.test.dat, type="response") 
final.predict.test.any.age1 = ifelse(final.predict.test.any.age1 > 0.6, "1", "0") #threshold of 0.6
confusions.pred.test.any.age1 = table(final.predict.test.any.age1, age1.test.dat$any_recid_2yr)
dimnames(confusions.pred.test.any.age1)[[1]] = c("No (Predicted)","Yes (Predicted)")
FP.any.age1 <- confusions.pred.test.any.age1["Yes (Predicted)", "No"] / sum(confusions.pred.test.any.age1[,"No"])
FN.any.age1 <-  confusions.pred.test.any.age1["No (Predicted)", "Yes"] / sum(confusions.pred.test.any.age1[,"Yes"])
  
##age2 PEOPLE VERSION OF FINAL TEST - any 
final.predict.test.any.age2 = predict(glm.logit, age2.test.dat, type="response") 
final.predict.test.any.age2 = ifelse(final.predict.test.any.age2 > 0.6, "1", "0") #threshold of 0.6
confusions.pred.test.any.age2 = table(final.predict.test.any.age2, age2.test.dat$any_recid_2yr)
dimnames(confusions.pred.test.any.age2)[[1]] = c("No (Predicted)","Yes (Predicted)")
FP.any.age2 <- confusions.pred.test.any.age2["Yes (Predicted)", "No"] / sum(confusions.pred.test.any.age2[,"No"])
FN.any.age2 <-  confusions.pred.test.any.age2["No (Predicted)", "Yes"] / sum(confusions.pred.test.any.age2[,"Yes"])
##age3 PEOPLE VERSION OF FINAL TEST - any 
final.predict.test.any.age3 = predict(glm.logit, age3.test.dat, type="response") 
final.predict.test.any.age3 = ifelse(final.predict.test.any.age3 > 0.6, "1", "0") #threshold of 0.6
confusions.pred.test.any.age3 = table(final.predict.test.any.age3, age3.test.dat$any_recid_2yr)
dimnames(confusions.pred.test.any.age3)[[1]] = c("No (Predicted)","Yes (Predicted)")
FP.any.age3 <- confusions.pred.test.any.age3["Yes (Predicted)", "No"] / sum(confusions.pred.test.any.age3[,"No"])
FN.any.age3 <-  confusions.pred.test.any.age3["No (Predicted)", "Yes"] / sum(confusions.pred.test.any.age3[,"Yes"])
```

| Our RAI: Any Recidivism                                          |           Under 25 | 25 - 45 |       45 and above|
|------------------------------------------------------------------|-----------------|------------------|--------------|
| Labeled Higher Risk, But Didn’t Re-Offend  (False positive)      |`r scales::percent(FP.any.age1, 0.1)`  | `r scales::percent(FP.any.age2, 0.1)` | `r scales::percent(FP.any.age3, 0.1)`|
| Labeled Lower Risk, Yet Did Re-Offend      (False Negative)      |`r scales::percent(FN.any.age1, 0.1)`  | `r scales::percent(FN.any.age2, 0.1)` | `r scales::percent(FN.any.age3, 0.1)`|

``` {r}
age1.test.dat <- test.dat[test.dat$age_factor == "Less than 25",]
age2.test.dat <- test.dat[test.dat$age_factor == "25 - 45",]
age3.test.dat <- test.dat[test.dat$age_factor == "Greater than 45",]
##age1 PEOPLE VERSION OF FINAL TEST - vio 
final.predict.test.vio.age1 = predict(glm.logit, age1.test.dat, type="response") 
final.predict.test.vio.age1 = ifelse(final.predict.test.vio.age1 > 0.6, "1", "0") #threshold of 0.6
confusions.pred.test.vio.age1 = table(final.predict.test.vio.age1, age1.test.dat$vio_recid_2yr)
dimnames(confusions.pred.test.vio.age1)[[1]] = c("No (Predicted)","Yes (Predicted)")
FP.vio.age1 <- confusions.pred.test.vio.age1["Yes (Predicted)", "No"] / sum(confusions.pred.test.vio.age1[,"No"])
FN.vio.age1 <-  confusions.pred.test.vio.age1["No (Predicted)", "Yes"] / sum(confusions.pred.test.vio.age1[,"Yes"])
  
##age2 PEOPLE VERSION OF FINAL TEST - vio 
final.predict.test.vio.age2 = predict(glm.logit, age2.test.dat, type="response") 
final.predict.test.vio.age2 = ifelse(final.predict.test.vio.age2 > 0.6, "1", "0") #threshold of 0.6
confusions.pred.test.vio.age2 = table(final.predict.test.vio.age2, age2.test.dat$vio_recid_2yr)
dimnames(confusions.pred.test.vio.age2)[[1]] = c("No (Predicted)","Yes (Predicted)")
FP.vio.age2 <- confusions.pred.test.vio.age2["Yes (Predicted)", "No"] / sum(confusions.pred.test.vio.age2[,"No"])
FN.vio.age2 <-  confusions.pred.test.vio.age2["No (Predicted)", "Yes"] / sum(confusions.pred.test.vio.age2[,"Yes"])
##age3 PEOPLE VERSION OF FINAL TEST - vio 
final.predict.test.vio.age3 = predict(glm.logit, age3.test.dat, type="response") 
final.predict.test.vio.age3 = ifelse(final.predict.test.vio.age3 > 0.6, "1", "0") #threshold of 0.6
confusions.pred.test.vio.age3 = table(final.predict.test.vio.age3, age3.test.dat$vio_recid_2yr)
dimnames(confusions.pred.test.vio.age3)[[1]] = c("No (Predicted)","Yes (Predicted)")
FP.vio.age3 <- confusions.pred.test.vio.age3["Yes (Predicted)", "No"] / sum(confusions.pred.test.vio.age3[,"No"])
FN.vio.age3 <-  confusions.pred.test.vio.age3["No (Predicted)", "Yes"] / sum(confusions.pred.test.vio.age3[,"Yes"])
```
  

| Our RAI: Violent Recidivism                                          |           Under 25 | 25 - 45 |       45 and above|
|------------------------------------------------------------------|-----------------|------------------|--------------|
| Labeled Higher Risk, But Didn’t Re-Offend  (False positive)      |`r scales::percent(FP.vio.age1, 0.1)`  | `r scales::percent(FP.vio.age2, 0.1)` | `r scales::percent(FP.vio.age3, 0.1)`|
| Labeled Lower Risk, Yet Did Re-Offend      (False Negative)      |`r scales::percent(FN.vio.age1, 0.1)`  | `r scales::percent(FN.vio.age2, 0.1)` | `r scales::percent(FN.vio.age3, 0.1)`|


``` {r}
####RE-RUN WITH FULL, INSTEAD OF recid.new
age1_full.recid.new <- recid.new[recid.new$age_factor == "Less than 25",]
age2_full.recid.new <- recid.new[recid.new$age_factor == "25 - 45",]
age3_full.recid.new <- recid.new[recid.new$age_factor == "Greater than 45",]
##age1_full PEOPLE VERSION OF FINAL TEST - vio 
final.predict.test.vio.age1_full = predict(glm.logit, age1_full.recid.new, type="response") 
final.predict.test.vio.age1_full = ifelse(final.predict.test.vio.age1_full > 0.6, "1", "0") #threshold of 0.6
confusions.pred.test.vio.age1_full = table(final.predict.test.vio.age1_full, age1_full.recid.new$vio_recid_2yr)
dimnames(confusions.pred.test.vio.age1_full)[[1]] = c("No (Predicted)","Yes (Predicted)")
FP.vio.age1_full <- confusions.pred.test.vio.age1_full["Yes (Predicted)", "No"] / sum(confusions.pred.test.vio.age1_full[,"No"])
FN.vio.age1_full <-  confusions.pred.test.vio.age1_full["No (Predicted)", "Yes"] / sum(confusions.pred.test.vio.age1_full[,"Yes"])
  
##age2_full PEOPLE VERSION OF FINAL TEST - vio 
final.predict.test.vio.age2_full = predict(glm.logit, age2_full.recid.new, type="response") 
final.predict.test.vio.age2_full = ifelse(final.predict.test.vio.age2_full > 0.6, "1", "0") #threshold of 0.6
confusions.pred.test.vio.age2_full = table(final.predict.test.vio.age2_full, age2_full.recid.new$vio_recid_2yr)
dimnames(confusions.pred.test.vio.age2_full)[[1]] = c("No (Predicted)","Yes (Predicted)")
FP.vio.age2_full <- confusions.pred.test.vio.age2_full["Yes (Predicted)", "No"] / sum(confusions.pred.test.vio.age2_full[,"No"])
FN.vio.age2_full <-  confusions.pred.test.vio.age2_full["No (Predicted)", "Yes"] / sum(confusions.pred.test.vio.age2_full[,"Yes"])
##age3_full PEOPLE VERSION OF FINAL TEST - vio 
final.predict.test.vio.age3_full = predict(glm.logit, age3_full.recid.new, type="response") 
final.predict.test.vio.age3_full = ifelse(final.predict.test.vio.age3_full > 0.6, "1", "0") #threshold of 0.6
confusions.pred.test.vio.age3_full = table(final.predict.test.vio.age3_full, age3_full.recid.new$vio_recid_2yr)
dimnames(confusions.pred.test.vio.age3_full)[[1]] = c("No (Predicted)","Yes (Predicted)")
FP.vio.age3_full <- confusions.pred.test.vio.age3_full["Yes (Predicted)", "No"] / sum(confusions.pred.test.vio.age3_full[,"No"])
FN.vio.age3_full <-  confusions.pred.test.vio.age3_full["No (Predicted)", "Yes"] / sum(confusions.pred.test.vio.age3_full[,"Yes"])
```

The ``r scales::percent(FN.vio.age3)`` false negative rate for the `45 and above` category confused us originally, but was a product of the small sample size with the test dataset. Only ``r sum(confusions.pred.test.vio.age3[,"Yes"])`` older individuals in our test data (``r scales::percent(sum(confusions.pred.test.vio.age3[,"Yes"]) / sum(confusions.pred.test.vio.age3), 0.1)`` of older individuals) were violent recidivators. While it does not reflect well on our model that it missed all of them, the rate shown only reflects ``r sum(confusions.pred.test.vio.age3[,"Yes"])`` errors. The table below uses the full dataset to show what the numbers might look like with a larger sample size, with the caveat that this is not "fresh" data (as the model was trained on some of it).  

| Our RAI: Violent Recidivism                                          |           Under 25 | 25 - 45 |       45 and above|
|------------------------------------------------------------------|-----------------|------------------|--------------|
| Labeled Higher Risk, But Didn’t Re-Offend  (False positive)      |`r scales::percent(FP.vio.age1_full, 0.1)`  | `r scales::percent(FP.vio.age2_full, 0.1)` | `r scales::percent(FP.vio.age3_full, 0.1)`|
| Labeled Lower Risk, Yet Did Re-Offend      (False Negative)      |`r scales::percent(FN.vio.age1_full, 0.1)`  | `r scales::percent(FN.vio.age2_full, 0.1)` | `r scales::percent(FN.vio.age3_full, 0.1)`|


# **Section 3: Key Findings and Takeaways**  

### **3a: Comparing our tool to the COMPAS tool**  

#### **3a.1: Do our RAIs perform better or worse than COMPAS?**  
   
Re-visiting the charts above, our accuracy rate is higher than the COMPAS for any recidivism though lower for violent recidivism. For any recidivism, our specificity is higher, so we are preventing more innocent people from being considered high-risk. For violent recidivism, our accuracy is substantially lower. That said, for violent crime, some might argue that high sensitivity (out-performing the COMPAS) makes it a better model. That said, we believe that the analysis of racial bias is just as important as the overall analysis. Our models show racial bias in the same direction as the COMPAS. This is unacceptable, and therefore we believe that neither our model nor the COMPAS should be used.  


| Model for any recidivism                   |  Accuracy      | Sensitivity      | Specificity |  
|--------------------------------------------|----------------|------------------|-------------|  
| **COMPAS Model **                          |**`r scales::percent(compas.acc.any, 0.1)`** |**`r scales::percent(compas.sens.any, 0.1)`** |**`r scales::percent(compas.spec.any, 0.1)`** |
| <span style ="color: #4D9DE0;"> *Logistic regression* |<span style ="color: #4D9DE0;">*`r scales::percent(logit.acc, 0.1)`*</span> |<span style ="color: #4D9DE0;"> *`r scales::percent(logit.sens, 0.1)`*</span>   |<span style ="color: #4D9DE0;">*`r scales::percent(logit.spec)`*</span> |  

| Model for violent recidivism               |  Accuracy      | Sensitivity      | Specificity |  
|--------------------------------------------|----------------|------------------|-------------|  
| **COMPAS Model **                          |**`r scales::percent(compas.acc.vio, 0.1)`** |**`r scales::percent(compas.sens.vio, 0.1)`** |**`r scales::percent(compas.spec.vio, 0.1)`** |
|<span style ="color: #4D9DE0;"> *Logistic regression*</span>|<span style ="color: #4D9DE0;">*`r scales::percent(logit.acc.vio, 0.1)`*</span> |<span style ="color: #4D9DE0;">*`r scales::percent(logit.sens.vio, 0.1)`*</span> |<span style ="color: #4D9DE0;">*`r scales::percent(logit.spec.vio, 0.1)`*</span>|  

```{r, warning = FALSE, message = FALSE}
# #Create table
# confusions.compas.any = table(predicted = recid.new$score_factor, actual = recid.new$any_recid_2yr)
# dimnames(confusions.compas.any)[[1]] = c("No (Predicted)","Yes (Predicted)")
# kable(confusions.compas.any, caption = "**Confusion matrix for the COMPAS: Any Recidivism**")
# compas.acc <- sum(diag(confusions.compas.any)) / sum(confusions.compas.any)
# compas.sens <- confusions.compas.any["Yes (Predicted)", "Yes"] / sum(confusions.compas.any[,"Yes"])
# compas.spec <- confusions.compas.any["No (Predicted)", "No"] / sum(confusions.compas.any[,"No"])
# kable(tibble(Accuracy = scales::percent(compas.acc, accuracy = 0.1),
#              Sensitivity = scales::percent(compas.sens, accuracy = 0.1),
#              Specificity = scales::percent(compas.spec, accuracy = 0.1)),
#       caption = "**Success of the COMPAS instrument**")
# #Same for violent recidivism
# confusions.compas.vio = table(predicted = recid.new$v_score_factor, actual = recid.new$vio_recid_2yr)
# dimnames(confusions.compas.vio)[[1]] = c("No (Predicted)","Yes (Predicted)")
# kable(confusions.compas.vio, caption = "**Confusion matrix for the COMPAS: Violent Recidivism**")
# compas.acc <- sum(diag(confusions.compas.vio)) / sum(confusions.compas.vio)
# compas.sens <- confusions.compas.vio["Yes (Predicted)", "Yes"] / sum(confusions.compas.vio[,"Yes"])
# compas.spec <- confusions.compas.vio["No (Predicted)", "No"] / sum(confusions.compas.vio[,"No"])
# kable(tibble(Accuracy = scales::percent(compas.acc, accuracy = 0.1),
#              Sensitivity = scales::percent(compas.sens, accuracy = 0.1),
#              Specificity = scales::percent(compas.spec, accuracy = 0.1)),
#       caption = "**Success of the COMPAS instrument**")
```

#### **3a.2: Do our RAIs produce similar classifications to COMPAS?**  

As shown above, our accuracy rates are similar for any recidivism and lower for violent recidivism. In the "any recidivism" model, we have adjusted the thresholds so that our classifications are less likely to result in false positives.   

It is more than an unfortunate coincidence that both the COMPAS model and our model have a strong racial bias. Notably, this is not even taking into account racial bias in how often people who committed crimes were caught. Even if we were to perfectly predict arrests, any model based on American crime data, no matter how internally sound, will reflect the biased data from which it was built. Additional referenes about bias in policing are available in the links at the bottom of this document. 

#### **3a.3: Are there systematic differences between our classifications and those of COMPAS?**  

As stated earlier, there are similarities between our classifications and those of COMPAS in regards to the biases of how our model classifies individuals. However, since we also favored Specificity in our model for Any Recidivism data, our logistic model for Any Recidivism does have a higher Specificity rate. As shown in section 2d, our models have lower false positive rates than the COMPAS model.

One major systematic difference between our classifications and those of COMPAS is that while both models are racist and ageist, ours has the unfortunate distinction of also being sexist.

### **3b: Reflections on risk assessment tools**   

We show in this report that with fewer variables, data mining tools available to the average student, and a limited amount of time, we were able to replicate the accuracy rate of a model already in use by the government, along with providing adjustments. While it is important for models to be validated, is it also important that until they are proven ready to be fair to the people they are affecting, they should not play a role in any decision making. If policy-makers are determined to use data mining for risk assessment, then tools such as the COMPAS should be tested not just for overall accuracy but for discrimination before they are used.  

As students at Carnegie Mellon, we are concerned about the use of RAIs like the COMPAS in our community of Pittsburgh, Pennsylvania. The PittCyber task force has announced via [community meetings](https://www.wesa.fm/post/residents-raise-concerns-about-bias-countys-automated-decision-making-tools#stream/0) that a recidivism RAI is being used here, but there is very little public information available. [Predictive policing](https://www.nbcnews.com/mach/science/how-science-helping-stop-crime-it-occurs-ncna805176) is also being used in Pittsburgh, and presenters at the [Pittsburgh Racial Justice Summit](http://pittsburghracialjusticesummit.org/the-2020-summit/) shared that they are working to increase public transparency about this process, as very little information is shared publicly. 

## **Additional References and Links**  

[Our github repo](https://github.com/kaylareiman1/RecidivismPrediction)  
[ProPublica main article]( https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)  
[ProPublica methodology]( https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm)  
[ProPublica github]( https://github.com/propublica/compas-analysis)  
[Raw version of COMPAS survey]( https://www.documentcloud.org/documents/2702103-Sample-Risk-Assessment-COMPAS-CORE.html)  
[Columbia article on Risk as proxy for race (2010)]( https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1677654)  
[Article on Risk, Race, and Recidivism]( https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2687339) 
[Original publication by COMPAS creator]( http://www.northpointeinc.com/files/publications/Criminal-Justice-Behavior-COMPAS.pdf)  
[Assignment instructions on Canvas]( https://canvas.cmu.edu/courses/14656/modules/items/4432299)  
[Project description on Canvas]( https://canvas.cmu.edu/courses/14656/modules/items/4443203)  
[Information about Predictive Policing](https://rss.onlinelibrary.wiley.com/doi/epdf/10.1111/j.1740-9713.2016.00960.x)  
[Nature article on limitations of machine learning](https://www.nature.com/articles/s41562-017-0141.epdf?author_access_token=zp6NdnflZBcbq4u8QE13i9RgN0jAjWel9jnR3ZoTv0OrWCV-_5fNRpT89FpEf7bpDfmwoV6oPboJp9g43OZUEZ3Jvuivgqlqr1rjq9C3M62_Ady8s_dGfhXTCFeIUphULML1P_InKq12GOvN0UKaRw%3D%3D)  
Some conceptual understanding of racial bias in arrest data was informed by an [October 2019 lecture](https://heinz.campusgroups.com/DTW/rsvp_boot?id=598928) at Heinz College by Patrick Ball of the [Human Right Data Analysis Group](https://hrdag.org/usa/). 

## **Appendix: Data Cleaning**  

In order to establish racial bias in how COMPAS scores predicted future recidivism, ProPublica analysts ran the model below:  
`glm(formula = score_factor ~ gender_factor + age_factor + race_factor + priors_count + crime_factor + two_year_recid, family = "binomial", data = df)`  

The approximate definitions of the ProPublica variables are below:  
   - two_year_recid: this is the variable that ProPublica used to indicate recidivism in each dataset.   
   - score_factor: the binary variable indicating low risk vs. medium/high risk.  
   - Other covariates:   
      + gender_factor: male vs. female  
      + age_factor: a 3-level categorical variable for age  
      + race_factor: from the original race variable. As will be shown below, most categories have very few observations.  
      + crime_factor: felonies vs. misdimeanors  
      + priors_count: prior infractions  
      
However, their published datasets included many more variables than were included in the model, as shown below. The variables prefaced with r_ indicate recidivism data and vr_ (or v_) indicate violent recidivism data. They cannot be part of our models because they would be unknown at the time of risk assessment. Other variables relate to screening and are also unhelpful. That said, ProPublica dropped the juvenile variables from their glm model, and we are not dropping those variables from our predictions. All original columns in the ProPublica dataset (compas-scores-two-years.csv) are below.

```{r, message = FALSE, echo = TRUE}
colnames.all <- colnames(recid.all)
colnames.vio <- colnames(recid.vio)
#Print full list of columns in ProPublica's dataset (listed on their github as compas-scores-two-years)
colnames.all
#Print columns that differ
colnames.vio[(colnames.vio != colnames.all)]
colnames.all[(colnames.vio != colnames.all)]
```

The tables below show the checks that were run to determine that ProPublica's "violent recidivsm" dataset was a subset of the "all recidivism" dataset. Additionally, these checks helped us learn that we should use the two-year recidivism varible instead of the is_recid variable. 

```{r, message = FALSE, echo = FALSE}
#Check how the recidivism variables relate
kable(recid.all %>%
  group_by (two_year_recid, is_recid, is_violent_recid, violent_recid) %>%
  summarize(count = n()), caption = "**How variables relate in recid.all dataset**")
kable(recid.vio %>%
  group_by (two_year_recid, two_year_recid.1, is_recid, is_violent_recid, violent_recid) %>%
  summarize(count = n()), caption = "**How variables relate in recid.vio dataset**")
```


After substantial discussion, we decided to use the full dataset for all analyses and create new outcome variables that showed whether the new offense was violent or not. This way, we are not dropping the people who re-offended non-violently. The table below shows factor variables that we created for our analyses, including the two outcomes variables (`any_recid_2yr` and `vio_recid_2yr`) during the data cleaning stage. We also created Y/N variables for juvenile inputs. 

**New Variables**  
```{r, message = FALSE}
kable(recid.new%>%
  count(any_recid_2yr, two_year_recid), caption = "**Outcome variable 1: any recidivism within 2 years**")
kable(recid.new%>%
  count(vio_recid_2yr, two_year_recid, is_violent_recid), caption = "**Outcome variable 2: violent recidivism within 2 years**")
kable(recid.new%>%
  count(juv_fel_bi, juv_fel_count), caption = "**Yes/No version of Juvenile Felonies**")
kable(recid.new%>%
  count(juv_misd_bi, juv_misd_count), caption = "**Yes/No version of Juvenile Misd.**")
kable(recid.new%>%
  count(juv_other_bi, juv_other_count), caption = "**Yes/No version of Juvenile Other Events**")
```

The table below shows variables that we dropped because they were redundant: 

**Redundant Variables**  
```{r}
kable(recid.all %>%
        count(sex, gender_factor))
kable(recid.all %>%
        count(age_factor, age_cat))
kable(recid.all %>%
        count(priors_count, priors_count.1))
kable(recid.all %>%
        count(crime_factor, c_charge_degree))
kable(recid.all %>%
        count(race_factor, race))
```
