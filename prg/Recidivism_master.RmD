
### **Running models for vio recidivism  
***2a.1V: Logistic regression:*** 

**Variable Selection**  
```{r}
#Model that includes all of our potential variables:
fullmodelvio <- vio_recid_2yr ~ crime_factor +
                            age +
                            age_factor +
                            race_factor +
                            gender_factor +
                            priors_log +
                            priors_count +
                            juv_fel_count +
                            juv_fel_bi +
                            juv_misd_count +
                            juv_misd_bi +
                            juv_other_count +
                            juv_other_bi +
                            length_of_stay
#Choosing variables - use fullmodelvio as the input
recid.vio.subsets <- regsubsets(fullmodelvio,
                            data = train.dat,
                            nbest = 1,
                            nvmax = 15,
                            method = "exhaustive")
par(mfrow = c(1, 2))
#AIC Chart
plot(x = summary(recid.vio.subsets)$cp, xlab = "Number of Variables", ylab = "Mallow's CP")
mincp <- which.min(summary(recid.vio.subsets)$cp)
points(mincp, summary(recid.vio.subsets)$cp[mincp], col = "green3", cex = 1, pch = 20)
#BIC Chart
plot(x = summary(recid.vio.subsets)$bic, xlab = "Number of Variables", ylab = "BIC")
minbic <- which.min(summary(recid.vio.subsets)$bic)
points(minbic, summary(recid.vio.subsets)$bic[minbic], col = "green3", cex = 1, pch = 20)
```
  
Depending on the metric used, the best-performing model uses somewhere between ``r minbic`` (according to BIC) and ``r mincp`` (according to Cp) variables. As the increase in BIC is small between 4 and 7 variables, it makes sense to choose we chose to use the 7 variables so clearly specified in the Mallow's CP model as ideal. After examining the models in this range, we went with the following 7 variables, presented with their coefficients below. Interestingly, unlike the full recidivism dataset, race was included int his model - though only for hispanic people. We left the full race variable in the dataset for the original logistic regression below. Notably, our choice to leave race in the model is informed by the understanding that we are predicting whether people will be caught recidivating, rather than true recidivism, given racial bias in the policing system.

```{r}
kable(coef(recid.vio.subsets, 7), format = "markdown", digits = 3, col.names = c("Value"))
```

***Model outcome***  
```{r, echo = FALSE}
#Copied manually from above
glm.viologit <- glm(vio_recid_2yr ~
                   crime_factor +
                   age +
                   race_factor + 
                   gender_factor +
                   priors_log +
                   juv_other_bi +
                   length_of_stay,
                 data=train.dat,
                 family = binomial)
#summary(glm.viologit)
kable(coef(summary(glm.viologit)), digits= 4)
train.pred.vio <- predict(glm.viologit, type = "response")

ggplot(data = as.data.frame(train.pred.vio),
       aes(x = train.pred.vio)) +
  geom_histogram(aes(y = (..count..)/sum(..count..)),
                 binwidth = 0.25/8, boundary = 0) +
  labs(x = "Predicted Probability of Violent Recidivism",
       y = "Proportion of Individuals",
       title = "Distribution of Violent Recidivism Predictions (Logistic Regression)")
```

The misclassification rate and confusion matrix are below, using a cut point of 0.5. Notably, almost nobody was predicted to commit violent crime in our model. Therefore, this misclassification rate is somwhat meaningless, given that only 11% of our population was caught for violent recidivism total. 
``` {r}
#Predicted values
train.pred.vio <- predict(glm.viologit, type = "response")
#Confusion matrix
confusion.glm = ifelse(train.pred.vio > 0.5, "1", "0")
confusions.pred.train = table(confusion.glm, train.dat$vio_recid_2yr)
dimnames(confusions.pred.train)[[1]] = c("No (Predicted)","Yes (Predicted)")
kable(confusions.pred.train, caption = "Logistic regression: confusion matrix using training data")

kable(round(1- sum(diag(confusions.pred.train)) / sum(confusions.pred.train), 2), col.names = "Misclassification Rate (with train.dat):")
```

Surprisingly, the ROC curve below using training data DOES show some value in the model. This likely means that our default cut-off of 0.5 was not optimal. In fact, the ROC curve shows that the optimal cut point (still using training data) is 0.129.  
```{r, message = FALSE}

#Predicted values
glm.viologit.predict.train  = predict(glm.viologit, train.dat, type="response")

## ROC plot
roccurve.vio <- roc(response = train.dat$vio_recid_2yr, predictor = glm.viologit.predict.train)
plot.roc(roccurve.vio, print.thres = TRUE)
```
**Update needed: somebody please check if I got the words right**
Re-running the confusion matrix above with this new cut point gives a much higher misclassification rate. Essentially, the original model categorizing (almost) everyone as unlikely to reoffend violently within two years gives the best accuracy, but it prioritizes specificity $too$ highly at the risk of sensitivity That said, thinking about the humans involved, this more "optimal" sensitivity of 75% means that 25% of people are being treated as if they are going to commit a violent crime, when in reality they are not going to commit this crime. This makes us doubt that risk assessment tools can be useful at all.   

However, this model may be over-fitting, so we must move on to using validation data to truly understand this model.

```{r, message = FALSE}
logit.threshold <- 0.129
```

**Model Validation**
An AUC curve using validation data is shown below. The final misclassification rate for our logistic model is 
```{r, message = FALSE}

#Predicted values
glm.viologit.predict.val  = predict(glm.viologit, val.dat, type="response")

## ROC plot
roccurve.vio <- roc(response = val.dat$vio_recid_2yr, predictor = glm.viologit.predict.val)
plot.roc(roccurve.vio, print.auc = TRUE, print.thres = TRUE)
```

``` {r}
#Predicted values
glm.viologit.predict.val  = predict(glm.viologit, val.dat, type="response")

## ROC plot
roccurve.vio <- roc(response = val.dat$vio_recid_2yr, predictor = glm.viologit.predict.val)
plot.roc(roccurve.vio, print.auc = TRUE, print.thres = TRUE)
```

***2a.2V Linear Discriminant Analysis (LDA)***  

**Model Outcome**
Running an LDA on our training data, we achieve the following results:
```{r}
recid.vio.lda <- lda(vio_recid_2yr ~
                   crime_factor +
                   age +
                   race_factor + 
                   gender_factor +
                   priors_log +
                   juv_other_bi +
                   length_of_stay,
                 data = train.dat)
#Results are currently for the training data
recid.vio.lda.pred.train <- predict(recid.vio.lda,
                          type = "response")
lda.tab = table(predicted = recid.vio.lda.pred.train$class,
                actual = train.dat$vio_recid_2yr)
lda.tab
lda.acc <- (lda.tab[1,1] + lda.tab[2,2]) / sum(lda.tab)
lda.sens <- lda.tab[2,2] / sum(lda.tab[,2])
lda.spec <- lda.tab[1,1] / sum(lda.tab[,1])
kable(tibble(Accuracy = scales::percent(lda.acc, accuracy = 0.1),
             Sensitivity = scales::percent(lda.sens, accuracy = 0.1),
             Specificity = scales::percent(lda.spec, accuracy = 0.1)),
      format = "markdown")
```
  
**Model Validation**  
Just like for logistic regression, we then re-ran the model using our validation data. The results from this analysis are below.  
```{r}
#Validation Time!!
lda.pred.val <- predict(object = recid.vio.lda, val.dat, type = "response")

lda.cv.tab = table(predicted = lda.pred.val$class,
                   actual = val.dat$vio_recid_2yr)
lda.cv.tab
lda.cv.acc <- (lda.cv.tab[1,1] + lda.cv.tab[2,2]) / sum(lda.cv.tab)
lda.cv.sens <- lda.cv.tab[2,2] / sum(lda.cv.tab[,2])
lda.cv.spec <- lda.cv.tab[1,1] / sum(lda.cv.tab[,1])
tibble(Accuracy = scales::percent(lda.cv.acc, accuracy = 0.1),
             Sensitivity = scales::percent(lda.cv.sens, accuracy = 0.1),
             Specificity = scales::percent(lda.cv.spec, accuracy = 0.1))
```

***2a.3V Quadratic Discriminant Analysis (QDA)***  
**Variable Selection and Model Outcomes**  
We then checked to see if QDA could outperform LDA -- which might be the case if covariation between input variables were different across classes.  
```{r}
recid.vio.qda <- qda(vio_recid_2yr ~
                   crime_factor +
                   age +
                   race_factor + 
                   gender_factor +
                   priors_log +
                   juv_other_bi +
                   length_of_stay,
                 data = train.dat)
recid.vio.qda.pred.train <- predict(recid.vio.qda,
                          type = "response")
qda.tab = table(predicted = recid.vio.qda.pred.train$class,
                actual = train.dat$two_year_recid)
qda.tab
qda.acc <- (qda.tab[1,1] + qda.tab[2,2]) / sum(qda.tab)
qda.sens <- qda.tab[2,2] / sum(qda.tab[,2])
qda.spec <- qda.tab[1,1] / sum(qda.tab[,1])
kable(tibble(Accuracy = scales::percent(qda.acc, accuracy = 0.1),
             Sensitivity = scales::percent(qda.sens, accuracy = 0.1),
             Specificity = scales::percent(qda.spec, accuracy = 0.1)),
      format = "markdown")
```

**Validation of QDA**
```{r}
qda.cv.pred <- predict(object = recid.vio.qda,
                       newdata = val.dat,
                       type = "response")
qda.cv.tab = table(predicted = qda.cv.pred$class,
                actual = val.dat$two_year_recid)
qda.cv.tab
qda.cv.acc <- (qda.cv.tab[1,1] + qda.cv.tab[2,2]) / sum(qda.cv.tab)
qda.cv.sens <- qda.cv.tab[2,2] / sum(qda.cv.tab[,2])
qda.cv.spec <- qda.cv.tab[1,1] / sum(qda.cv.tab[,1])
tibble(Accuracy = scales::percent(qda.cv.acc, accuracy = 0.1),
             Sensitivity = scales::percent(qda.cv.sens, accuracy = 0.1),
             Specificity = scales::percent(qda.cv.spec, accuracy = 0.1))
```


***2a.4V Classification And Regression Tree***  

**Classification tree: Variable Selection**  
The classification process will automatically choose which variables to use. Therefore, we will feed it the full model with all possible input variables. Acknowledgement: this analysis relies heavily on lab 8 from [ISLR](http://faculty.marshall.usc.edu/gareth-james/ISL/code.html). That said, the textbook was focusing on regression trees, where as we are working on classification trees. That said, we also tried regression trees since that model allowed us to create ROC curves.

```{r}
#Run original CLASSIFICATION tree
tree.recid <- tree(formula = update(fullmodelvio, ~ . - priors_log), data = train.dat)
#Create summary object
summary.tree.recid <- summary(tree.recid)
```

Just like with the full recidivism dataseet, we also ran a regression tree. This single tree makes it clear how the two relate.
```{r}
#Run REGRESSION tree
reg.tree.recid <- tree(formula = (as.numeric(vio_recid_2yr) - 1) ~
                         crime_factor +
                         age +
                         age_factor +
                         race_factor +
                         gender_factor +
                         #priors_log +
                         priors_count +
                         juv_fel_count +
                         juv_misd_count +
                         juv_other_count +
                         length_of_stay,
                       data = train.dat)
summary.reg.tree.recid <- summary(reg.tree.recid)

#INSERT TEXT HERE
plot(reg.tree.recid)
text(reg.tree.recid, pretty = 0)

```

Using the training dataset (with `r nrow(train.dat)` rows), we find the that there is a misclassification rate of `r round(summary.tree.recid$misclass[1]/summary.tree.recid$misclass[2], 2)`. The initial tree predicting whether a person will recidivate within two years is shown below.  

```{r}
kable(summary.tree.recid$used, col.names = "Variables used in first tree")
plot(tree.recid)
text(tree.recid, pretty=0)
```

However, this is only the first step in the process. We must next re-run the code using our validation data with (with `r nrow(val.dat)` rows.

```{r}
tree.pred=predict(tree.recid, val.dat, type="class")
table(tree.pred,val.dat$vio_recid_2yr)
tree.pred.errors = table(tree.pred,val.dat$vio_recid_2yr)[2,1] + table(tree.pred,val.dat$vio_recid_2yr)[1,2]
```

Based on the confusion matrix produced above with validation data, we see `r tree.pred.errors` errors out of `r nrow(val.dat)` observations, for a test error rate of `r round(tree.pred.errors/nrow(val.dat), 2)`. 

Next we see if there are improvements that can be made by pruning it using cross-validation (with the function `cv.tree`). This process will help us avoid over-fitting.

```{r}
#Create CV version of the model, pruning by misclassification rate.
cv.recid=cv.tree(tree.recid, FUN=prune.misclass)
#plot the error rate as a function of both size and k.
par(mfrow=c(1,2))
plot(cv.recid$size,cv.recid$dev,type="b")
plot(cv.recid$k,cv.recid$dev,type="b")
cv.recid.vio.df <- data.frame(cv.recid$size, cv.recid$dev)
#Figure out smallest deviance
cv.recid.vio.min.dev <- min(cv.recid.vio.df$cv.recid.vio.dev)
#Select the best model based on this deviance
cv.recid.vio.best <- min(cv.recid.vio.df$cv.recid.vio.size[cv.recid.vio.df$cv.recid.vio.dev== cv.recid.vio.min.dev])
#Create the pruned version - based on figuring out the best size above. Note that the book did this manually but we are automating so that we can re-use this code for both all and violent recidivism.
prune.recid=prune.misclass(tree.recid,best=cv.recid.vio.best)
plot(prune.recid)
text(prune.recid,pretty=0)
#Again, predict and use the validation data
tree.pred.prune=predict(prune.recid, val.dat, type="class")

table(tree.pred.prune,val.dat$vio_recid_2yr)
tree.pred.prune.errors = table(tree.pred.prune,val.dat$vio_recid_2yr)[2,1] + table(tree.pred.prune,val.dat$vio_recid_2yr)[1,2]


## ROC plot -- cannot create this b/c all yes/no variables -- nothing quant.
# roccurve.tree <- roc(response = val.dat$vio_recid_2yr, predictor = tree.pred.prune)
# plot.roc(roccurve.vio, print.auc = TRUE, print.thres = TRUE)

```
This final confusion matrix produced after pruning shows `r tree.pred.prune.errors` errors out of `r nrow(val.dat)` observations, for a test error rate of `r round(tree.pred.prune.errors/nrow(val.dat), 2)`.  

confusions.tree <- table(tree.pred.prune,val.dat$vio_recid_2yr)
confusions.tree
tree.pred.prune.errors = confusions.tree[2,1] + confusions.tree[1,2]
```

This final confusion matrix produced after pruning shows `r tree.pred.prune.errors` errors out of `r nrow(val.dat)` observations, for a test error rate of `r round(tree.pred.prune.errors/nrow(val.dat), 2)`.

```{r}
#paste("Accuracy:")
tree.acc <- sum(diag(confusions.tree)) / sum(confusions.tree)
#logit.acc
#paste("Sensitivity Rate:")
tree.sens <- sensitivity(confusions.tree)
#logit.sens
#paste("Specificity Rate:")
tree.spec <- specificity(confusions.tree)
#logit.spec
kable(tibble(Accuracy = scales::percent(tree.acc, accuracy = 0.1),
             Sensitivity = scales::percent(tree.sens, accuracy = 0.1),
             Specificity = scales::percent(tree.spec, accuracy = 0.1)),
      format = "markdown")

```

**Regression trees**
```{r}
#Create model using rpart functiona and training dataset
tree.recid.vio.vio.rpart.train <- rpart(formula = fullmodelvio, data = train.dat)
#Output pretty picture
tree.recid.vio.vio.rpart.train.pretty <- as.party(tree.recid.vio.vio.rpart.train)
plot(tree.recid.vio.vio.rpart.train.pretty, pretty = 1)
print(tree.recid.vio.vio.rpart.train.pretty)
#Predict and use the validation data
tree.pred.vio.rpart.val   <- predict(tree.recid.vio.vio.rpart.train, val.dat)

#Create ROC curve
roccurve.vio.rpart<- roc(response = val.dat$two_year_recid, predictor = tree.pred.vio.rpart.val[,2], levels = c(0, 1), direction = "<")
plot.roc(roccurve.vio.rpart, print.auc = TRUE, print.thres = TRUE)



```

***2a.5V Random Forest***  
```  {r}
recid.vio.rf =randomForest(formula = fullmodelvio,
                       data    = trainval.dat,
                       mtry    = 4 #this shouild be sqrt(p) since we're doing classification. In our case, p = 14.
                       ,importance=TRUE #keeping this from book
                       )
``` 

The confusion matrix for our random forrest created using training data is below:
```{r}
recid.vio.rf$confusion
rf.vio.error = (recid.vio.rf$confusion[2,1] + recid.vio.rf$confusion[1,2]) /nrow(trainval.dat)
```

This matrix shows that the overall error rate for this random forest is `r round(rf.vio.error, 2)`.

To increase transparency, the plot below shows the importance of each variable in the random forest mode. Looking at the plot on the right, a small gini coefficient indicates that nodes are more pure. We see that age decreases the gini coefficient the most, followed by length of stay, priors count, and the log of priors count.  The least meaningful variables are the juvenile recods. Race is our 5th-most important predictor here. Notably, we have left this variable in because evidence shows that models that exclude race as a variable are not necessarily less racist. We will assess discrimination later int his report. [**update needed: this will need to be changed when we assess violent recidivism**] 

```{r}
varImpPlot(recid.vio.rf, main="Variable Importance by Factor")
```


We also plotted ROC curves for the random forest, as shown below.
```{r, warnings = FALSE}
recid.vio.rf =randomForest(formula = fullmodelvio,
                       data    = train.dat,
                       mtry    = 4 #this shouild be sqrt(p) since we're doing classification. In our case, p = 14.
                       ,importance=TRUE #keeping this from book
                       )
rf.vio.predict <- as.data.frame(predict(recid.vio.rf, val.dat, type = "prob"))


#Adapting code from other parts of the program
##Update this: two_year_recid news a violent equivalent
roccurve.vio.rpart<- roc(response = val.dat$two_year_recid, predictor = rf.vio.predict$Yes)
plot.roc(roccurve.vio.rpart, print.auc = TRUE, print.thres = TRUE)

```
We then decided to pursue [which models should we use?]
