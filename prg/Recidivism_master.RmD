---
title: "Recidivism Analysis"
subtitle: "Course: Data Mining with Dr. Diane Igoche"
author: "Group 12: Moses Hetfield, Sormeh Yazdi, and Kayla Reiman"
date: "May 2020"
output: html_document
---

# **Team Notes**
****Delete this notes section eventually****  

#### **Outstanding conceptual questions** 
- How do we decide which classification model(s) to try? Just try as many as we have time for and then compare misclassificaiton rates?
- How does the video about the importance of splitting the data before starting relate to K-fold CV, which we learned about next as the better alternative?
- How do we deal with the 59 variables, given that it's bad data mining practice to drop anything?
- What is a cox validation study?
- How did ProPublica decide on a logistic regression using low/high as the outcome?
`glm(formula = score_factor ~ gender_factor + age_factor + race_factor + priors_count + crime_factor + two_year_recid, family = "binomial", data = df)`  
- Why are the datasets of violent crimes and regular crimes separate? Why are there different sample sizes? Why can't we just merge them?  
- Confirm the row driver of each dataset

#### **Items for discussion** 
- Logistics: What's everyone's comfort level with github?  
   + Basic use (my level) = keep most up-to-date version on here. Text each other to make sure we're not working on the same thing at the same time.  
   + Advanced use (beyond me) = branches, pull requests, etc.
   + Preferred method of tracking communal to-do list, if not in RmD?
- Outline:
   + Format: following [Assignment instructions on Canvas]( https://canvas.cmu.edu/courses/14656/modules/items/4432299)
   + How would you like to adjust this overall outline?
- Which data mining methods do you think make sense (2a.1)? 
   + Assuming using more is good if we have time, like the GROOMS thing we read
   + But actually...should we be deciding in advance, and if so how?
- Division of to-do list
   + Anyone planning on working before Tuesday? 
   + Which methods do you understand? (ex. KR just did LDA/QDA HW, but not RF)

#### **To do list**       
* **High level of effort**      
   + Validation: Figure out how we should be splitting our data into training/validation from the beginning so we don't make [this mistake](https://canvas.cmu.edu/courses/14656/files/4676325?module_item_id=4438423). Then document it at the start of section 2.  
   + Figure out which models to use
   + Improve KR's descriptions of possible models in 2a.1: Model Selection and validation. You can use the book's section called "4.5 A Comparison of Classification Methods" as a resource.
* **Medium level of effort**        
* **Low level of effort**        
   + Switch to relative path referencing while importing CSVs  


#### **Tasks for later**   
* **Medium level of effort**
   + Descriptive analysis (1a-d): start creating pretty plots!
   + Model Selection and validation (2a.1): run our first model!
* **High level of effort**
   + Figure out the best version of a given model (ex. logistic regression) using test/training error
   + Compare models (ex. logistic regression vs. random forrest)
   + Make pretty pictures about how our models performed (lift charts, ROC curves, sensitivity vs. specificity)
   
#### **List of resources**
[Our github repo](https://github.com/kaylareiman1/RecidivismPrediction)  
[ProPublica main article]( https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)  
[ProPublica methodology]( https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm)  
[ProPublica github]( https://github.com/propublica/compas-analysis)  
[Raw version of COMPAS survey]( https://www.documentcloud.org/documents/2702103-Sample-Risk-Assessment-COMPAS-CORE.html)  
[Columbia article on Risk as proxy for race (2010)]( https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1677654)
[Article on Risk, Race, and Recidivism]( https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2687339)  
[Original publication by COMPAS creator]( http://www.northpointeinc.com/files/publications/Criminal-Justice-Behavior-COMPAS.pdf)  
[Assignment instructions on Canvas]( https://canvas.cmu.edu/courses/14656/modules/items/4432299)  
[Project description on Canvas]( https://canvas.cmu.edu/courses/14656/modules/items/4443203)  


****End of team notes**** 

```{r global_options, include=FALSE}
#This code chunk sets up the HTML output to not show code by default.  
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE)
```

```{r, warning = FALSE, message = FALSE}
#Suppressing warnings about libraries
#Import  libraries (currently overdoing it)
library(tidyverse)    
library(ggplot2)      # graphics library
library(gridExtra)    # For displaying graphs side-by-side
library(knitr)        # contains knitting control
library(tree)         # For the tree-fitting 'tree' function
library(randomForest) # For random forests
library(rpart)        # For nicer tree fitting
library(partykit)     # For nicer tree plotting
library(boot)         # For cv.glm
library(leaps)        # needed for regsubsets (though maybe not relevant b/c our outcome vars are binary)


#Format numbers so that they are not in scientific notation.
options(scipen = 4)
```

# **Introduction**  
#### **Background**    
In 2016, ProPublica published a [story](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) on how a commonly-used pre-trial risk assessment tool called the COMPAS is racially biased. The journalists showed that in spite of a 2009 validation study showing similar accuracy rates for black and white men (67 percent versus 69 percent), the inacccuracies were in opposite directions for the two groups. This racial bias of the tool is refleected in their their published table replicated below:      
  
| Type of error                              |           White | African American | 
|--------------------------------------------|-----------------|------------------|
| Labeled Higher Risk, But Didn’t Re-Offend  |            23.5%|            44.9% |
| Labeled Lower Risk, Yet Did Re-Offend      |            47.7%|            28.0% |

#### **Additional information on the COMPAS**    
The COMPAS is widely used across states [add more here from ProPublica article]. It gives people a risk score ranging from 1 to 10, where risk scores of 1 to 4 are  “Low”, 5 to 7 are labeled “Medium”, and 8 to 10 are labeled “High.” Although race is not included in its [137 questions]( https://www.documentcloud.org/documents/2702103-Sample-Risk-Assessment-COMPAS-CORE.html), some of the  questions such as how often people moved can be linked to poverty...[add more here from]  

#### **Prompt**    
* Using the available data, construct a Risk Assessment Instrument (RAI) for predicting two-year recidivism.   
   - Evaluate the predictive performance of your model. 
   - What are the most important predictors of recidivism?
   
* Create an RAI for predicting violent recidivism.
   - Evaluate the predictive performance of your model. 
   - What are the most important predictors of violent recidivism? 
   - How do they compare to the important predictors of general recidivism?
  
* Assess whether the RAIs from (A) and (B) are equally predictive across race/ethnicity groups? How about across age and gender_factor groups?

* Compare your RAIs to the COMPAS RAI. 
   - Do your RAIs perform better or worse than COMPAS?   
   - Do your RAIs produce similar classifications to COMPAS? 
   - Can you identify any systematic differences between your classifications and those of COMPAS? 
   
#### **Goal** 
Our goal is to investigate whether it is possible to create a Risk Assessment Instrument (RAI) that is more accurate and less racist than the COMPAS.

#### **Data**
This analysis is run using ProPublica's data. In order to have clean data, it is necessary to remove certain observations. Fortunately, ProPublica staff published their [Jupyter notebook](https://github.com/propublica/compas-analysis/blob/master/Compas%20Analysis.ipynb) with data cleaning steps in R, so our data is cleaned in the same way (ex. dropping people whose charge date was not w/in 30 days ). However, we have adapted their code so that it does not drop any attributes because this would be bad practice for data mining.  

```{r, message = FALSE}
###################################
# Read in all ProPublica datasets #
###################################
compas.scores.raw <- read_csv("C:\\Users\\uukay\\OneDrive\\Documents\\mini4\\Data Mining\\Final project\\RecidivismPrediction\\datasets\\compas-scores-raw.csv")

compas.scores <- read_csv("C:\\Users\\uukay\\OneDrive\\Documents\\mini4\\Data Mining\\Final project\\RecidivismPrediction\\datasets\\compas-scores.csv")

compas.scores.two.years <- read_csv("C:\\Users\\uukay\\OneDrive\\Documents\\mini4\\Data Mining\\Final project\\RecidivismPrediction\\datasets\\compas-scores-two-years.csv")

compas.scores.two.years.violent <- read_csv("C:\\Users\\uukay\\OneDrive\\Documents\\mini4\\Data Mining\\Final project\\RecidivismPrediction\\datasets\\compas-scores-two-years-violent.csv")

cox.parsed <- read_csv("C:\\Users\\uukay\\OneDrive\\Documents\\mini4\\Data Mining\\Final project\\RecidivismPrediction\\datasets\\cox-parsed.csv")

cox.violent.parsed <- read_csv("C:\\Users\\uukay\\OneDrive\\Documents\\mini4\\Data Mining\\Final project\\RecidivismPrediction\\datasets\\cox-violent-parsed.csv")

#Next step 1 (low LOE): Somebody fix so the file paths aren't just for my local directories. Here are three options in order of how ideal they would be:
#   The propublica github files
#   My forked version of the propublica github files:  https://github.com/kaylareiman1/compas-analysis
#   Our copied and pasted version of the propublica github files: https://github.com/kaylareiman1/RecidivismPrediction/tree/master/datasets

####################################
# Adapt ProPublica's Data Cleaning #
####################################
#Note: This code is copied directly from ProPublica. Here is why they dropped data:
      # - dropped if charge date not w/in 30 days  
      # - Coded the recidivist flag -- is_recid -- to be -1 if could not find a compas case at all.  
      # - Ordinary traffic offenses removed  
      # - Filtered the underlying data from Broward county to include:  
      #   + people who had either recidivated in two years  
      #   + had at least two years outside of a correctional facility.  

#Unlike ProPublica, we won't be dropping any attributes because that may bias our model.
### All recidivism ###

#Dropping bad data
recid.all <-compas.scores.two.years %>% 
        filter(days_b_screening_arrest <= 30) %>%
        filter(days_b_screening_arrest >= -30) %>%
        filter(is_recid != -1) %>%
        filter(c_charge_degree != "O") %>%
        filter(score_text != 'N/A')

#Recoding variables
recid.all$length_of_stay <- as.numeric(as.Date(recid.all$c_jail_out) - as.Date(recid.all$c_jail_in))
recid.all <- mutate(recid.all, crime_factor = factor(c_charge_degree)) %>%
      mutate(age_factor = as.factor(age_cat)) %>%
      within(age_factor <- relevel(age_factor, ref = 1)) %>%
      mutate(race_factor = factor(race)) %>%
      within(race_factor <- relevel(race_factor, ref = 3)) %>%
      mutate(gender_factor = factor(sex, labels= c("Female","Male"))) %>%
      within(gender_factor <- relevel(gender_factor, ref = 2)) %>%
      mutate(score_factor = factor(score_text != "Low", labels = c("LowScore","HighScore")))


### Violent recidivism ###
#Dropping bad data
recid.vio <- compas.scores.two.years.violent %>% 
        filter(days_b_screening_arrest <= 30) %>%
        filter(days_b_screening_arrest >= -30) %>% 
        filter(is_recid != -1) %>%
        filter(c_charge_degree != "O") %>%
        filter(v_score_text != 'N/A')

#Recoding variables
#KR note: Why is race coded differently? Is there a mistake?
recid.vio <- mutate(recid.vio, crime_factor = factor(c_charge_degree)) %>%
      mutate(age_factor = as.factor(age_cat)) %>%
      within(age_factor <- relevel(age_factor, ref = 1)) %>%
      mutate(race_factor = factor(race,
                                  labels = c("African-American", 
                                             "Asian",
                                             "Caucasian", 
                                             "Hispanic", 
                                             "Native American",
                                             "Other"))) %>%
      within(race_factor <- relevel(race_factor, ref = 3)) %>%
      mutate(gender_factor = factor(sex, labels= c("Female","Male"))) %>%
      within(gender_factor <- relevel(gender_factor, ref = 2)) %>%
      mutate(score_factor = factor(v_score_text != "Low", labels = c("LowScore","HighScore")))
```

# **Section 1: Exploratory Data Analysis**


#### **1a: Overview of datasets**  
[KR note: must check whether 1 person per row or different row driver]

This project uses 2 datasets from [ProPublica's github repository]( https://github.com/propublica/compas-analysis), which are described in the table below:  
  
| Dataset name | original obs | obs after cleaning | # varibles |
|--------------|--------------|--------------------|------------|   
| recid.all    | 7214         | 6172               | 59         |    
| recid.vio    | 4743         | 4020               | 59         |    

An overview of relevant variables is below:  
   - two_year_recid: this is the variable that ProPublica used to indicate recidivism in each dataset. 
   - score_factor:
   - Demographic variables in the prompt: gender_factor, age_factor, race_factor    
   - Other variables that ProPublica used as covariates: 
      -crime_factor: felonies vs. misdimeanors [KR: check original vs. within 2 years]
      -priors_count: prior infractions [KR: learn more about this]

#### **1b: Demographic Characteristics**
**Note: These graphs use the full dataset (not limited to violent crime)**
The univariate graphs below show that the most categories in the dataset were men, black people, and those between the age of of 20 and 45.
```{r, message = FALSE}

gender_factor.uni.all <- ggplot(recid.all, aes(x=gender_factor))
gender_factor.uni.all + 
  geom_bar(stat = "count", na.rm = F) + 
  ggtitle("Univariate Gender Breakdown") + 
  guides(fill = FALSE) + 
  xlab("Gender") + 
  ylab("Number of People")

race_factor.uni.all <- ggplot(recid.all, aes(x=race_factor))
race_factor.uni.all + 
  geom_bar(stat = "count", na.rm = F) + 
  ggtitle("Univariate Race Breakdown") + 
  guides(fill = FALSE) + 
  xlab("Race") + 
  ylab("Number of People")

age_factor.uni.all <- ggplot(recid.all, aes(x=age_factor))
age_factor.uni.all + 
  geom_bar(stat = "count", na.rm = F) + 
  ggtitle("Univariate Age Breakdown") + 
  guides(fill = FALSE) + 
  xlab("Age") + 
  ylab("Number of People")

```

#### **1c: Recidivism Rates**
The table below shows how common two-year recidivism of both types was in our dataset.
```{r, message = FALSE}
rate.all <- recid.all %>%
  summarize(100*round(mean(two_year_recid), 2))
rate.vio <- recid.vio %>%
  summarize(100*round(mean(two_year_recid), 2))
```

| Dataset                                |Two-year Rate |
|----------------------------------------|--------------|
| All recidivism (recid.all)             | `r rate.all`%|
| Violent recidivism (recid.vio)         | `r rate.vio`%|

#### **1d: Compas Predictions**
The graphs below show participants' 10-point scores, where a score of 10 indicates the highest recidivism potential. Judges were often shown categories of low, medium, and high risk. The ProPublica analysis combined the categories of medium and high in order to create a binary variable (low, not-low).  

```{r, warning = FALSE, message = FALSE}
#Propublica relevant vars: `glm(formula = score_factor ~ gender_factor + age_factor + race_factor + priors_count + crime_factor + two_year_recid, family = "binomial", data = df)`  

ranking.all <- ggplot(recid.all, aes(x=decile_score, fill = score_text))+
  geom_bar(stat = "count", bin = 1) +
  ggtitle("Prediction Metrics\nAll data") + 
  xlab("Prediction") + 
  guides(fill = FALSE) + 
  ylab("Number of people") 

ranking.vio <- ggplot(recid.vio, aes(x=v_decile_score, fill = v_score_text))+
  geom_bar(stat = "count", bin = 1) +
  ggtitle("Prediction Metrics\nViolence data") + 
  xlab("Prediction")+ 
  ylab(NULL)

grid.arrange(ranking.all, ranking.vio, ncol = 2)

```

#### **1e: Accuracy of Predictions by Demographic Group**
[KR note to team: I actually expected these wouldn't look alike at all. But by the time I finished this, I'd spent so much time adapting code from our good ol' NLSY project that now I want to keep them.]
Initial data exploration showed that the trends in recidivism were similar to predictions. However, a few limitations should be noted:
   - The higher rate of recidivism among African-American respondents cannot be separated from bias in policing [cite Michelle Alexander].
   - This includes no interactions between terms demographic characteristics (ex. race and gender)
   - As ProPublica pointed out, these charts do not differentiate between false positives and false negatives. 

```{r, warning = FALSE, message = FALSE}
#Note -- just copying and pasting different characteristics. Could create a function instead.

### gender_factor ###
# Bivariate w/ decile score 
gender_factor.bi.conf.all <- recid.all %>%
  group_by(gender_factor) %>%
  summarize(Avgdecile_score = mean(decile_score),
            upper = t.test(decile_score)$conf.int[1],
            lower = t.test(decile_score)$conf.int[2])

gender_factor.bi.all.dec <- ggplot(data = gender_factor.bi.conf.all, aes(x = gender_factor, y = Avgdecile_score)) +  geom_bar(stat = "identity") +
  xlab("gender_factor") + 
  ylab("Average prediction") +
  ggtitle("Prediction from COMPAS") + 
  guides(fill = FALSE) +
  geom_errorbar(aes(ymax = upper, ymin = lower), width = 0.1, size = 1) +
  theme(text = element_text(size=12))

# Bivariate w/ recid 
gender_factor.bi.conf.all <- recid.all %>%
  group_by(gender_factor) %>%
  summarize(Avgtwo_year_recid = mean(two_year_recid),
            upper = t.test(two_year_recid)$conf.int[1],
            lower = t.test(two_year_recid)$conf.int[2])

gender_factor.bi.all.recid <- ggplot(data = gender_factor.bi.conf.all, aes(x = gender_factor, y = Avgtwo_year_recid)) +
  geom_bar(stat = "identity") +
  xlab("gender_factor") + 
  ylab("Average Rate") +
  ggtitle("Actual 2-year Recidivism") + 
  guides(fill = FALSE) +
  geom_errorbar(aes(ymax = upper, ymin = lower), width = 0.1, size = 1) +
  theme(text = element_text(size=12))


### RACE ###
# Bivariate w/ decile score 
race_factor.bi.conf.all <- recid.all %>%
  group_by(race_factor) %>%
  summarize(Avgdecile_score = mean(decile_score),
            upper = t.test(decile_score)$conf.int[1],
            lower = t.test(decile_score)$conf.int[2])

race_factor.bi.all.dec <- ggplot(data = race_factor.bi.conf.all, aes(x = race_factor, y = Avgdecile_score)) +  geom_bar(stat = "identity") +
  xlab("Race") + 
  ylab("Average prediction") +
  ggtitle("Prediction from COMPAS") + 
  guides(fill = FALSE) +
  geom_errorbar(aes(ymax = upper, ymin = lower), width = 0.1, size = 1)  +
  theme(text = element_text(size=12), axis.text.x = element_text(angle = 60, vjust = 1, hjust = 1))
# Bivariate w/ recid 
race_factor.bi.conf.all <- recid.all %>%
  group_by(race_factor) %>%
  summarize(Avgtwo_year_recid = mean(two_year_recid),
            upper = t.test(two_year_recid)$conf.int[1],
            lower = t.test(two_year_recid)$conf.int[2])

race_factor.bi.all.recid <- ggplot(data = race_factor.bi.conf.all, aes(x = race_factor, y = Avgtwo_year_recid)) +
  geom_bar(stat = "identity") +
  xlab("Race") + 
  ylab("Average Rate") +
  ggtitle("Actual 2-year Recidivism") + 
  guides(fill = FALSE) +
  geom_errorbar(aes(ymax = upper, ymin = lower), width = 0.1, size = 1)  +
  theme(text = element_text(size=12), axis.text.x = element_text(angle = 60, vjust = 1, hjust = 1))

### AGE ###
# Bivariate w/ decile score 
age_factor.bi.conf.all <- recid.all %>%
  group_by(age_factor) %>%
  summarize(Avgdecile_score = mean(decile_score),
            upper = t.test(decile_score)$conf.int[1],
            lower = t.test(decile_score)$conf.int[2])

age_factor.bi.all.dec <- ggplot(data = age_factor.bi.conf.all, aes(x = age_factor, y = Avgdecile_score)) +  geom_bar(stat = "identity") +
  xlab("Age") + 
  ylab("Average prediction") +
  ggtitle("Prediction from COMPAS") + 
  guides(fill = FALSE) +
  geom_errorbar(aes(ymax = upper, ymin = lower), width = 0.1, size = 1) +
  theme(text = element_text(size=12))

# Bivariate w/ recid 
age_factor.bi.conf.all <- recid.all %>%
  group_by(age_factor) %>%
  summarize(Avgtwo_year_recid = mean(two_year_recid),
            upper = t.test(two_year_recid)$conf.int[1],
            lower = t.test(two_year_recid)$conf.int[2])

age_factor.bi.all.recid <- ggplot(data = age_factor.bi.conf.all, aes(x = age_factor, y = Avgtwo_year_recid)) +
  geom_bar(stat = "identity") +
  xlab("age") + 
  ylab("Average Rate") +
  ggtitle("Actual 2-year Recidivism") + 
  guides(fill = FALSE) +
  geom_errorbar(aes(ymax = upper, ymin = lower), width = 0.1, size = 1) +
  theme(text = element_text(size=12))

grid.arrange(gender_factor.bi.all.dec, gender_factor.bi.all.recid, ncol = 2)
grid.arrange(race_factor.bi.all.dec, race_factor.bi.all.recid, ncol = 2)
grid.arrange(age_factor.bi.all.dec, age_factor.bi.all.recid, ncol = 2)



```

# **Section 2: Methodology** 
Prior to starting the data mining process, we randomly split the data into training and validation sets.  
```{r, message = FALSE}
#Somebody figure out how we're supposed to do this. I thought I understood it until we learned about cross-validation, which is better than regular validation. 
```

#### **2a: Data Mining Methods and Performance**
**2a.1: Model Selection and validation**   

**Possible models**  
[KR note to team: we may cut some of these if we a) don't have time or b) think they're the wrong models]  
Given our task of predicting binary outcomes, we considered the following classification methods:  
   - Logistic regression:   
   - Naive Bayes (NB): This works when there is no interaction between predictors.  
   - Linear Discriminant Analysis (LDA): This works when the interaction between predictors is the same across classes.  
   - Quadratic Discriminant Analysis (QDA): This works when there are class-based interactions among the predictors  
   - Random Forrest (RF): 
   - K-Nearest Neighbors (KNN):  
   
```{r, message = FALSE}
#Run these models on training data
#Run cross-validation on these models
#Select the best version of each model based on a low CV
#Output line graphs showing how training and CV error levels work
```

To decide between models, we looked at three measures of error:
   -Accuracy: This is the percent of predictions that were correct. It is a limited measure of the tool's usefulness because it counts false positives and false negatives in the same way, when their human impact is much different.
    -Specifity: This shows the true negative rate. A higher number shows that we are not mistakenly classifying people as high-risk.  general, we believe this is the most important measure because it keeps innocent people from receiving harsh punishments.
   -Sensitivity: This shows the true positive rate. A higher number shows that we successfully predicted people would commit crimes. For violent crime, a high sensitivity rate is important.

  
**Statistics for the best version of each data mining method**    
[fill this in once we've run the models]  

| Model for any recidivism                   |  Accuracy      | Sensitivity      | Specificity |
|--------------------------------------------|----------------|------------------|-------------|
| Logistic regression                        |                |                  |             |  
| Linear Discriminant Analysis (LDA)         |                |                  |             |
| Quadratic Discriminant Analysis (QDA)      |                |                  |             | 
| Naive Bayes (NB)                           |                |                  |             |
| Random Forrest (RF)                        |                |                  |             |

| Model for violent recidivism               |  Accuracy      | Sensitivity      | Specificity |
|--------------------------------------------|----------------|------------------|-------------|
| Logistic regression                        |                |                  |             |  
| Linear Discriminant Analysis (LDA)         |                |                  |             |
| Quadratic Discriminant Analysis (QDA)      |                |                  |             | 
| Naive Bayes (NB)                           |                |                  |             |
| Random Forrest (RF)                        |                |                  |             |


[KR note: let's add some graphs here that show tradoffs between different measures that we care about (like sensitivity vs. specificity. Caulkins would be proud)]

**2c: Demographic analysis of the best version of each model:**  

**2c1: Race**  

**2c2: Gender**  

**2c3: Age**  

#### **2b: Final model(s)**
Our final model is....
This has an accuracy rate of [], specifity of [].

```{r, message = FALSE}
#Add pretty figures about our final model(s)
```

# **Section 3: Key Findings and Takeaways**

#### **3a: Comparing our tool to the COMPAS tool** 

**3a.1: Does our RAIs perform better or worse than COMPAS?**    
**3a.2: Do our RAIs produce similar classifications to COMPAS?** 
**3a.3: Are there systematic differences between our classifications and those of COMPAS?** 

#### **3b: Reflections on risk assessment tools** 
We know that arrest data is biased based on where police patrol, so even a model that's interally sound is still based on biased data.[maybe cite Patrick Ball]

Perhaps cite some articles here, like the two that ProPublica cited, or something more recent
[Columbia article on Risk as proxy for race (2010)]( https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1677654)
[Article on Risk, Race, and Recidivism](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2687339)

[May also reflect on Pittsburgh's use of a pre-trial risk assessment tool]

# **References**  
[ProPublica main article](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)  
[ProPublica methodology](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm)  
[ProPublica github](https://github.com/propublica/compas-analysis)   
[Raw version of COMPAS survey](https://www.documentcloud.org/documents/2702103-Sample-Risk-Assessment-COMPAS-CORE.html)


